{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KK C2 Spark WordCount",
      "provenance": [],
      "collapsed_sections": [
        "W2QOKe1NRHn4",
        "sRvq6-Cd5cwS",
        "-tVgOWHgpc0W",
        "_2HO3H2WkkDC",
        "G9QnYzQBmqC6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praxis-QR/BDSN/blob/main/KK_C2_Spark_WordCount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj93_nN2wPV1"
      },
      "source": [
        "![alt text](https://github.com/Praxis-QR/RDWH/raw/main/images/YantraJaalBanner.png)<br>\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "[Prithwis Mukerjee](http://www.linkedin.com/in/prithwis)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark WordCount Demo\n",
        "An alternative approach is available at https://github.com/Praxis-QR/BDSN/blob/main/KK_C2A_Spark_WordCount_Alternative.ipynb"
      ],
      "metadata": {
        "id": "Y-gYEr36F06t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "print('Tested on ',datetime.now(pytz.timezone('Asia/Calcutta')))\n",
        "\n",
        "!python --version\n",
        "!lsb_release -a\n",
        "\n",
        "#check which version of MongoDB  is available\n",
        "#!apt-cache policy mongodb\n",
        "\n",
        "#check which versions of software are available\n",
        "!pip3 index versions pyspark\n",
        "!pip3 index versions pyngrok"
      ],
      "metadata": {
        "id": "dHceCHdUqREm",
        "outputId": "fb71e111-7be7-4fbc-ef34-a26a6b040479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tested on  2024-04-24 10:17:35.781023+05:30\n",
            "Python 3.10.12\n",
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 22.04.3 LTS\n",
            "Release:\t22.04\n",
            "Codename:\tjammy\n",
            "\u001b[33mWARNING: pip index is currently an experimental command. It may be removed/changed in a future release without prior warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mpyspark (3.5.1)\n",
            "Available versions: 3.5.1, 3.5.0, 3.4.3, 3.4.2, 3.4.1, 3.4.0, 3.3.4, 3.3.3, 3.3.2, 3.3.1, 3.3.0, 3.2.4, 3.2.3, 3.2.2, 3.2.1, 3.2.0, 3.1.3, 3.1.2, 3.1.1, 3.0.3, 3.0.2, 3.0.1, 3.0.0, 2.4.8, 2.4.7, 2.4.6, 2.4.5, 2.4.4, 2.4.3, 2.4.2, 2.4.1, 2.4.0, 2.3.4, 2.3.3, 2.3.2, 2.3.1, 2.3.0, 2.2.3, 2.2.2, 2.2.1, 2.2.0.post0, 2.1.3, 2.1.2\n",
            "\u001b[33mWARNING: pip index is currently an experimental command. It may be removed/changed in a future release without prior warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mpyngrok (7.1.6)\n",
            "Available versions: 7.1.6, 7.1.5, 7.1.4, 7.1.3, 7.1.2, 7.1.1, 7.1.0, 7.0.5, 7.0.4, 7.0.3, 7.0.2, 7.0.1, 7.0.0, 6.1.2, 6.1.1, 6.1.0, 6.0.0, 5.2.3, 5.2.2, 5.2.1, 5.2.0, 5.1.0, 5.0.6, 5.0.5, 5.0.4, 5.0.3, 5.0.2, 5.0.1, 5.0.0, 4.2.2, 4.2.1, 4.1.16, 4.1.15, 4.1.13, 4.1.12, 4.1.11, 4.1.10, 4.1.9, 4.1.8, 4.1.7, 4.1.6, 4.1.5, 4.1.4, 4.1.3, 4.1.2, 4.1.1, 4.1.0, 4.0.3, 4.0.2, 4.0.1, 4.0.0, 3.1.1, 3.1.0, 3.0.0, 2.1.7, 2.1.6, 2.1.5, 2.1.4, 2.1.3, 2.1.2, 2.1.1, 2.1.0, 2.0.3, 2.0.2, 2.0.1, 2.0.0, 1.4.4, 1.4.3, 1.4.2, 1.4.0, 1.3.8, 1.3.7, 1.3.6, 1.3.5, 1.3.4, 1.3.3, 1.3.2, 1.3.1, 1.3.0, 1.2.2, 1.2.0, 1.1.2, 1.1.1, 1.0.0, 0.3.2, 0.3.1, 0.3.0, 0.2.2, 0.2.1, 0.2.0, 0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSOI8k9qicmu"
      },
      "source": [
        "# Spark Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Installation - Not Required, SKIP"
      ],
      "metadata": {
        "id": "GguG29b062na"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIjVxBSKIDsW"
      },
      "source": [
        "#!apt update > /dev/null\n",
        "#!apt install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAgQ-0fGhxWI"
      },
      "source": [
        "# Get latest and correct version of Spark\n",
        "#\n",
        "# if the current version of Spark is not used, there may be errors\n",
        "# check here for current versions http://apache.osuosl.org/spark\n",
        "#\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.2.2/spark-2.2.2-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "#!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "#!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "#!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "#!pip install -q findspark\n",
        "#!pip install -q pyspark\n",
        "\n",
        "#import os\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ClRT_Ari2Ys"
      },
      "source": [
        "# findspark route is no more required\n",
        "#import findspark\n",
        "#findspark.init()\n",
        "#\n",
        "#findspark.init('/content/spark-2.2.2-bin-hadoop2.7')\n",
        "#\n",
        "# https://stackoverflow.com/questions/42223498/findspark-init-indexerror-list-index-out-of-range-error\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simpler Install. Works"
      ],
      "metadata": {
        "id": "vI0evwwr7CpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install pyspark\n",
        "#from pyspark.sql import SparkSession\n",
        "#spark = SparkSession.builder.appName('Praxis').getOrCreate()"
      ],
      "metadata": {
        "id": "JA26cSUf7Maq",
        "outputId": "b57cc7e8-58eb-4694-bf8b-a017331131b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm3XlHanNEq7"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# note UI port switched from default 4040 to 4050 to avoid clash with ngrok\n",
        "spark = SparkSession.builder.master(\"local[*]\").config('spark.ui.port', '4050').getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "931FrA2jfeXf"
      },
      "source": [
        "sc = spark.sparkContext"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsSE4GKBiwf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "8a2cc36c-63e9-4b71-b374-947d79b3eab3"
      },
      "source": [
        "# The spark Console UI is available in the link that will be displayed in this cell.\n",
        "# If you do not wish to use the Console, you may skip the Tunnel part\n",
        "sc"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cd2757e85d9d:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2QOKe1NRHn4"
      },
      "source": [
        "##Tunnel with ngrok <br>\n",
        "This part required ONLY if you need to access Spark Console<br>\n",
        "To get you ngrok token please visit https://dashboard.ngrok.com/login <br>\n",
        "and create a free account"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG1ZwJ8yehyw"
      },
      "source": [
        "###Loading ngrok token from GDrive<br>\n",
        "this part may be skipped if ngrok can be placed directly in one of the two next steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7uFjFW9RU9L",
        "outputId": "c38265ff-e08e-474b-99b5-b8cbe96b2d75"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsCxPFhjRqSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944473f0-2c48-4bb2-e6ec-4764e2b71e67"
      },
      "source": [
        "#!ls /content/drive/'My Drive'/Praxis/'Course - DevOps'/'Advanced Colab'/\n",
        "!ls /content/drive/'My Drive'/Praxis/WebCredentials/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASTRA_DataStaxCredentials_Jun22.py  Parashar21_Atlas_18Sep22.py\n",
            "AtlasCredentials_Jun22.py\t    QuandlApiRegistrations_praxis.txt\n",
            "awsrdscreds28dec2023.py\t\t    QuandlApiRegistrations.txt\n",
            "CCPraxisMongoDB16Oct22.py\t    rdsPGcredJan2024.py\n",
            "cleverCloudMongo51Sep22.py\t    secure-connect-praxisdb.zip\n",
            "cleverCloud.py\t\t\t    zzclevercloudMongoDB_jun22.py\n",
            "db4freeCredentials.py\t\t    zzclevercloudMongoDB.py\n",
            "neonCredentials250124.py\t    zzparashar21URI.py\n",
            "ngrokToken.py\t\t\t    zzsqlCredentials_020221.py-deprecated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a48D9kHyR9gI"
      },
      "source": [
        "#!cp /content/drive/'My Drive'/Praxis/'Course - DevOps'/'Advanced Colab'/ngrokToken.py .\n",
        "!cp /content/drive/'My Drive'/Praxis/WebCredentials/ngrokToken.py ."
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXb94HvLSSBb"
      },
      "source": [
        "# credential file for Prithwis Mukerjee\n",
        "# this file needs to be uploaded into the VM\n",
        "\n",
        "from ngrokToken import ngrokToken\n",
        "\n",
        "#for the sake of privacy\n",
        "#the following credentials need to be stored in a text file called ngrokToken.py\n",
        "#in the format given below\n",
        "#in the Colab VM\n",
        "\n",
        "#otherwise, the values can be directly placed here\n",
        "\n",
        "#ngrokToken = 'xxxxxxxxxxxxxxx'   # uncomment this line and place your own credentials here\n",
        "#token is available at https://dashboard.ngrok.com/get-started/setup\n",
        "\n",
        "\n",
        "#print(ngrokToken)\n",
        "ngrokTokenCmd = 'ngrok authtoken '+ngrokToken"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twDJK8Wzb6yd"
      },
      "source": [
        "###ngrok - native <br>\n",
        "has been having problems <br>\n",
        "this code adapted from https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/?unapproved=164287&moderation-hash=399998d65a0f1425c41aa2979e873387#comment-164287 <br>\n",
        "DEPRECATED - DO NOT USE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaCo9yVIcRej"
      },
      "source": [
        "#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "#!unzip ngrok-stable-linux-amd64.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO04Lp9YcZEL"
      },
      "source": [
        "#!./ngrok authtoken xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
        "#get_ipython().system_raw(ngrokTokenCmd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia0D9fHtRs1f"
      },
      "source": [
        "#!apt-get install jq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt_zKdINeuI3"
      },
      "source": [
        "#!./ngrok http 4050 &                       # -- causes problems"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFM4kdSljCo_"
      },
      "source": [
        "#get_ipython().system_raw('./ngrok http 4050 &')\n",
        "#!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk3jZ8lfSddJ"
      },
      "source": [
        "#!curl -s http://localhost:4040/api/tunnels | jq .tunnels[0].public_url\n",
        "#!curl -s http://localhost:4040/api/tunnels | jq .tunnels[0].config.addr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuCKEn_dsfkm"
      },
      "source": [
        "###ngrok - python wrapper <br>\n",
        "this code adapted from https://towardsdatascience.com/quickly-share-ml-webapps-from-google-colab-using-ngrok-for-free-ae899ca2661a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sWPM5QvtS4B",
        "outputId": "930ffd8c-e53d-4a7a-f68a-77dbf67e86e0"
      },
      "source": [
        "!pip3 install pyngrok"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx0ssMRt36k4"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "# you may place the token directly here\n",
        "#!ngrok authtoken xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
        "get_ipython().system_raw(ngrokTokenCmd)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU_KntEetijL"
      },
      "source": [
        "# Open a HTTP tunnel on the default port 80\n",
        "#public_url = ngrok.connect(port = '4050')\n",
        "public_url = ngrok.connect(4050)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6ysgjHbtozr",
        "outputId": "11dcc26f-3256-4e51-af3e-e0e9065c5e19"
      },
      "source": [
        "#This is where the Spark Console UI will be visible\n",
        "public_url"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://e735-34-73-173-102.ngrok-free.app\" -> \"http://localhost:4050\">"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL_D5j-oOn2V"
      },
      "source": [
        "The next four cells are not required. Mainly used to check on the status of the Tunnel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Ne_XhzuDGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3226a6f6-969b-4356-b824-0816d04035c5"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"tunnels\":[{\"name\":\"http-4050-07606024-3b0c-4c56-ba95-4dc25e5c106c\",\"uri\":\"/api/tunnels/http-4050-07606024-3b0c-4c56-ba95-4dc25e5c106c\",\"public_url\":\"https://e2fd5c9fb145.ngrok.io\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}},{\"name\":\"http-4050-07606024-3b0c-4c56-ba95-4dc25e5c106c (http)\",\"uri\":\"/api/tunnels/http-4050-07606024-3b0c-4c56-ba95-4dc25e5c106c%20%28http%29\",\"public_url\":\"http://e2fd5c9fb145.ngrok.io\",\"proto\":\"http\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO66r_yEftBC"
      },
      "source": [
        "#!ngrok help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7tQPpQ8q09V",
        "outputId": "f0e9a5df-2c68-4352-af7b-49b454f4ecdc"
      },
      "source": [
        "!ps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:01 node\n",
            "     17 ?        00:00:00 tail\n",
            "     55 ?        00:00:01 jupyter-noteboo\n",
            "     56 ?        00:00:00 dap_multiplexer\n",
            "     66 ?        00:00:03 python3\n",
            "     86 ?        00:00:00 python3\n",
            "   1074 ?        00:00:09 java\n",
            "   1171 ?        00:00:00 bash\n",
            "   1172 ?        00:00:00 drive\n",
            "   1173 ?        00:00:00 grep\n",
            "   1220 ?        00:00:00 fusermount <defunct>\n",
            "   1246 ?        00:00:00 drive\n",
            "   1299 ?        00:00:00 bash\n",
            "   1300 ?        00:00:00 tail\n",
            "   1301 ?        00:00:00 python3\n",
            "   1345 ?        00:00:00 ngrok\n",
            "   1361 ?        00:00:00 ps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvkMV2GxrI3L"
      },
      "source": [
        "!kill -9 1219"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKbXwFgI8WW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkBb0xKLi9BW"
      },
      "source": [
        "# Data Input - Three options, use only 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRvq6-Cd5cwS"
      },
      "source": [
        "##Local Machine\n",
        "Please upload a .txt file from your desktop to the VM using the upload feature of Colab. Without this, this set of commands will not work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atmB_erdoijn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856674a6-fa6f-44c2-ee12-c330bf60f7f5"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 223496\n",
            "drwx------  6 root root      4096 Aug 21 00:59 drive\n",
            "-rw-------  1 root root        64 Aug 21 00:59 ngrokToken.py\n",
            "drwxr-xr-x  2 root root      4096 Aug 21 00:59 __pycache__\n",
            "drwxr-xr-x  1 root root      4096 Aug 13 13:35 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 May 24 04:45 spark-3.1.2-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228834641 May 24 05:01 spark-3.1.2-bin-hadoop3.2.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cetfRq2hToE"
      },
      "source": [
        "textRDD = sc.textFile(\"CY-033-28May2020-TextFile.txt\")\n",
        "#textRDD = sc.textFile(\"hobbit.txt\")\n",
        "#textRDD = spark.read.text('hobbit.txt').rdd\n",
        "# for an explanation of the alternate way to read text files\n",
        "# see this https://stackoverflow.com/questions/52665353/reading-a-text-file-in-spark/52669632#52669632"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiDK866KhToY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb7d6d6-6908-499e-907b-53e926a5c5a7"
      },
      "source": [
        "print (textRDD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CY-033-28May2020-TextFile.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylf23XX_o3m2"
      },
      "source": [
        "#textRDD.take(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tVgOWHgpc0W"
      },
      "source": [
        "##Google Drive - Mount\n",
        "In this case, you can mount or connect Google drive and pull out a txt file from there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLUlMVwGpc0M"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2QgW3PpfesD"
      },
      "source": [
        "#!ls /content/drive/\"My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4XHQchlpc0E"
      },
      "source": [
        "!ls /content/drive/\"My Drive\"/Praxis/\"Course - Big Data Spark\"/WordCount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slfM2Xgcpcz0"
      },
      "source": [
        "!cat /content/drive/\"My Drive\"/Praxis/\"Course - Big Data Spark\"/WordCount/hobbit2.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luNYr9g-pczi"
      },
      "source": [
        "#textRDD = spark.read.text(\"/content/drive/My Drive/Praxis/Course - Big Data Spark/WordCount/hobbit2.txt\").rdd\n",
        "textRDD = sc.textFile(\"/content/drive/My Drive/Praxis/Course - Big Data Spark/WordCount/hobbit2.txt\")\n",
        "# for an explanation of the alternate way to read text files\n",
        "# see this https://stackoverflow.com/questions/52665353/reading-a-text-file-in-spark/52669632#52669632"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN6ZJ9v3pczT"
      },
      "source": [
        "textRDD.take(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0spX8yVNMyZw"
      },
      "source": [
        "##Google Drive - GDown / Git repository\n",
        "In this case, a file that has universal read access privileges is pulled from Google Drive without the need to login in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvA6zv4fM1fc"
      },
      "source": [
        "#!gdown https://drive.google.com/uc?id=1RELUMtExjMTSfoWF765Hr8JwNCSL7AgH\n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/Chronotantra.txt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO61epWsOBUP"
      },
      "source": [
        "textRDD = sc.textFile(\"Chronotantra.txt\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zykb54rCOLe3",
        "outputId": "be43e8d7-8e61-4be6-b3a3-7e1675998fd5"
      },
      "source": [
        "textRDD.take(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', 'Chronotantra ', '', 'Prithwis Mukerjee', '']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2HO3H2WkkDC"
      },
      "source": [
        "# Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAvPRM1-kqJo"
      },
      "source": [
        "from operator import add"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Np0nEJhTpK"
      },
      "source": [
        "#def tokenize1(textz):\n",
        "#    return textz.value.split()\n",
        "def tokenize2(textz):\n",
        "    return textz.split()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7fP_I0OhTpa"
      },
      "source": [
        "wordsRDD = textRDD.flatMap(tokenize2)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl9ogplR7EGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6736f941-de35-42d7-eb44-283648d27d2e"
      },
      "source": [
        "print(wordsRDD)\n",
        "wordsRDD.take(10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PythonRDD[3] at RDD at PythonRDD.scala:53\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chronotantra',\n",
              " 'Prithwis',\n",
              " 'Mukerjee',\n",
              " 'CHRONOTANTRA',\n",
              " 'A',\n",
              " 'science',\n",
              " 'fiction',\n",
              " 'novel',\n",
              " 'Published',\n",
              " 'by']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac8Rb_Uiooqs"
      },
      "source": [
        "#wordsRDD.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMCcwZrxhTp8"
      },
      "source": [
        "wc = wordsRDD.map(lambda x: (x,1))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3qbQGoyhTqQ"
      },
      "source": [
        "#print (wc.toDebugString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcIoomB-hTqi"
      },
      "source": [
        "counts = wc.reduceByKey(add)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9ODw1hXhTq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8d8877-974a-40ac-da21-5ef464ba89ff"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chronotantra.txt  drive  ngrokToken.py\t__pycache__  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5683u2uKhTrU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb3da86-e29a-4dd4-8679-86918674b906"
      },
      "source": [
        "# If running a program for the second time, the output directory needs to be deleted\n",
        "!rm -r CT-Out"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'CT-Out': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wydu9UDDhTro"
      },
      "source": [
        "#counts.saveAsTextFile(\"hobbit-Out\")\n",
        "counts.saveAsTextFile(\"CT-Out\")\n",
        "#counts.collect()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9QnYzQBmqC6"
      },
      "source": [
        "# Data Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ac3dQ2VmYda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b971eb-6f7c-4133-f5b3-589f7d744c0e"
      },
      "source": [
        "# Output is visble here\n",
        "#!ls hobbit-Out\n",
        "!ls CT-Out"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000  part-00001\t_SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMkcg01rnBR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fb9427-6958-4f9c-90c7-bcefb09fa1e1"
      },
      "source": [
        "#!cat hobbit-Out/part-00001\n",
        "#!cat hobbit-Out/part-00001\n",
        "!head CT-Out/part-00001"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Chronotantra', 2)\n",
            "('Prithwis', 5)\n",
            "('CHRONOTANTRA', 1)\n",
            "('A', 46)\n",
            "('fiction', 4)\n",
            "('Published', 1)\n",
            "('by', 190)\n",
            "('the', 3399)\n",
            "('Author', 2)\n",
            "('available', 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smdrOXSTiSPp"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nZcQeS1hTuy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8e0001b9-dd82-4019-f3d2-bc45753bdc48"
      },
      "source": [
        "files.download(\"CT-Out/part-00000\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0753b4cb-a78a-4da3-a7c9-ca5bf38cd561\", \"part-00000\", 86891)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "print('signed off at  ',datetime.now(pytz.timezone('Asia/Kolkata')))"
      ],
      "metadata": {
        "id": "yOGlEzx78E2M",
        "outputId": "8d5729a9-e4cd-43af-bf66-1a8959c456eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "signed off at   2024-04-24 10:24:43.646011+05:30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKTZezaqMK_0"
      },
      "source": [
        "#Chronobooks <br>\n",
        "![alt text](https://1.bp.blogspot.com/-lTiYBkU2qbU/X1er__fvnkI/AAAAAAAAjtE/GhDR3OEGJr4NG43fZPodrQD5kbxtnKebgCLcBGAsYHQ/s600/Footer2020-600x200.png)<hr>\n",
        "Chronotantra and Chronoyantra are two science fiction novels that explore the collapse of human civilisation on Earth and then its rebirth and reincarnation both on Earth as well as on the distant worlds of Mars, Titan and Enceladus. But is it the human civilisation that is being reborn? Or is it some other sentience that is revealing itself.\n",
        "If you have an interest in AI and found this material useful, you may consider buying these novels, in paperback or kindle, from [http://bit.ly/chronobooks](http://bit.ly/chronobooks)"
      ]
    }
  ]
}