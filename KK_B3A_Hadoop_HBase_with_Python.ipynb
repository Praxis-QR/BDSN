{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KK B3A Hadoop HBase with Python",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praxis-QR/BDSN/blob/main/KK_B3A_Hadoop_HBase_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XafE9O3pAmkC"
      },
      "source": [
        "![alt text](https://github.com/Praxis-QR/RDWH/raw/main/images/YantraJaalBanner.png)<br>\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "[Prithwis Mukerjee](http://www.linkedin.com/in/prithwis)<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "print('Tested',datetime.now(pytz.timezone('Asia/Calcutta')))"
      ],
      "metadata": {
        "id": "jDPvI3lXiwCO",
        "outputId": "3af6bc8c-42b7-4ce0-e2e5-7e05522fdb48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tested 2024-06-14 18:52:33.372977+05:30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyYTQEpb1htS"
      },
      "source": [
        "#Hadoop\n",
        "Install and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r0IOmlc0-Dt"
      },
      "source": [
        "# The default JVM available at /usr/lib/jvm/java-11-openjdk-amd64/  works for Hadoop\n",
        "# But gives errors with Hive https://stackoverflow.com/questions/54037773/hive-exception-class-jdk-internal-loader-classloadersappclassloader-cannot\n",
        "# Hence this JVM needs to be installed\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWspfH6A1a4P"
      },
      "source": [
        "# Download the latest version of Hadoop\n",
        "#!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.3/hadoop-3.3.3.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n",
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.4.0/hadoop-3.4.0.tar.gz\n",
        "# Unzip it\n",
        "# the tar command with the -x flag to extract, -z to uncompress, -v for verbose output, and -f to specify that we’re extracting from a file\n",
        "#!tar -xzf hadoop-3.3.0.tar.gz\n",
        "#!tar -xzf hadoop-3.3.3.tar.gz\n",
        "#!tar -xzf hadoop-3.3.5.tar.gz\n",
        "!tar -xzf hadoop-3.4.0.tar.gz\n",
        "#copy  hadoop file to user/local\n",
        "#!mv  hadoop-3.3.0/ /usr/local/\n",
        "#!mv  hadoop-3.3.3/ /usr/local/\n",
        "#!mv  hadoop-3.3.5/ /usr/local/\n",
        "!mv  hadoop-3.4.0/ /usr/local/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24QjQ22V1swJ"
      },
      "source": [
        "#To find the default Java path\n",
        "#!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "#!ls /usr/lib/jvm/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugibq3m1CbgC"
      },
      "source": [
        "## Set Hadoop Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j7WFh5k18E1"
      },
      "source": [
        "#To set java path, go to /usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh then\n",
        "#. . . export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ . . .\n",
        "#we have used a simpler alternative route using os.environ - it works\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"   # default is changed\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\"\n",
        "#os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.0/\"\n",
        "#os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.3/\"\n",
        "#os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.5/\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.4.0/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaaMpfVP2p3m"
      },
      "source": [
        "# Add Hadoop BIN to PATH\n",
        "# current_path taken from last command\n",
        "#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'\n",
        "current_path = os.getenv('PATH')\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.0/bin/'\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.3/bin/'\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.5/bin/'\n",
        "new_path = current_path+':/usr/local/hadoop-3.4.0/bin/'\n",
        "os.environ[\"PATH\"] = new_path"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD34gT3y2u-D"
      },
      "source": [
        "# Testing Hadoop with PI generating sample program, should calculate value of pi = 3.14157500000000000000\n",
        "# pi example\n",
        "#Uncomment the following line if  you want to test Hadoop with pi example\n",
        "#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar pi 16 100000\n",
        "#!hadoop jar /usr/local/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.3.jar pi 16 100000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHwLU0Uo3tLo"
      },
      "source": [
        "#Install HBase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bODP1xaM3vEB"
      },
      "source": [
        "# Get the latest HBase download site from here https://www.apache.org/dyn/closer.lua/hbase/\n",
        "#!wget https://mirrors.estointernet.in/apache/hbase/2.4.5/hbase-2.4.5-bin.tar.gz\n",
        "#!wget https://mirrors.estointernet.in/apache/hbase/2.4.8/hbase-2.4.8-bin.tar.gz\n",
        "#!wget https://downloads.apache.org/hbase/2.4.9/hbase-2.4.9-bin.tar.gz\n",
        "#!wget https://downloads.apache.org/hbase/2.4.12/hbase-2.4.12-bin.tar.gz\n",
        "#!wget https://downloads.apache.org/hbase/2.5.0/hbase-2.5.0-bin.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hbase/2.5.2/hbase-2.5.2-bin.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hbase/2.5.3/hbase-2.5.3-bin.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hbase/2.5.5/hbase-2.5.5-bin.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hbase/2.5.7/hbase-2.5.7-bin.tar.gz\n",
        "!wget -q https://downloads.apache.org/hbase/2.6.0/hbase-2.6.0-bin.tar.gz\n",
        "\n",
        "\n",
        "#!tar xzf hbase-2.4.5-bin.tar.gz\n",
        "#!tar xzf hbase-2.4.8-bin.tar.gz\n",
        "#!tar xzf hbase-2.4.9-bin.tar.gz\n",
        "#!tar xzf hbase-2.4.12-bin.tar.gz\n",
        "#!tar xzf hbase-2.5.0-bin.tar.gz\n",
        "#!tar xzf hbase-2.5.2-bin.tar.gz\n",
        "#!tar xzf hbase-2.5.3-bin.tar.gz\n",
        "#!tar xzf hbase-2.5.5-bin.tar.gz\n",
        "#!tar xzf hbase-2.5.7-bin.tar.gz\n",
        "!tar xzf hbase-2.6.0-bin.tar.gz"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaMtyNOVCiMS"
      },
      "source": [
        "## Set HBase Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4dc0K7h4f-y",
        "outputId": "405efc9f-326a-4891-aba2-35daaec250f6"
      },
      "source": [
        "#os.environ[\"HIVE_HOME\"] = \"/content/apache-hive-3.1.2-bin\"\n",
        "#!echo $HIVE_HOME\n",
        "#os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.4.5/\"\n",
        "#os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.4.8/\"\n",
        "#os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.4.9/\"\n",
        "#os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.4.12/\"\n",
        "#os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.5.0/\"\n",
        "#os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.5.2/\"\n",
        "#os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.5.3/\"\n",
        "#os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.5.5/\"\n",
        "#os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.5.7/\"\n",
        "os.environ[\"HBASE_HOME\"] = \"/content/hbase-2.6.0/\"\n",
        "!echo $HBASE_HOME\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hbase-2.6.0/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqXZ_kF65VXK",
        "outputId": "55ec194d-e478-4f63-da07-b645be687180"
      },
      "source": [
        "# current_path taken from command in previous cell\n",
        "#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/usr/local/hadoop-3.3.0/bin/'\n",
        "#new_path = current_path+':/content/hbase-2.4.5/bin'\n",
        "#\n",
        "current_path = os.getenv('PATH')\n",
        "#\n",
        "#new_path = current_path+':/content/hbase-2.4.8/bin'\n",
        "#new_path = current_path+':/content/hbase-2.4.9/bin'\n",
        "#new_path = current_path+':/content/hbase-2.4.12/bin'\n",
        "#new_path = current_path+':/content/hbase-2.5.0/bin'\n",
        "#new_path = current_path+':/content/hbase-2.5.2/bin'\n",
        "#new_path = current_path+':/content/hbase-2.5.3/bin'\n",
        "#new_path = current_path+':/content/hbase-2.5.5/bin'\n",
        "#new_path = current_path+':/content/hbase-2.5.7/bin'\n",
        "new_path = current_path+':/content/hbase-2.6.0/bin'\n",
        "#\n",
        "os.environ[\"PATH\"] = new_path\n",
        "!echo $PATH"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/usr/local/hadoop-3.4.0/bin/:/content/hbase-2.6.0/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrS6BL5k51Bl",
        "outputId": "a2285b02-39f0-42b1-811e-d0813aff3a72"
      },
      "source": [
        "!echo $JAVA_HOME\n",
        "!echo $HADOOP_HOME\n",
        "!echo $HBASE_HOME"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64\n",
            "/usr/local/hadoop-3.4.0/\n",
            "/content/hbase-2.6.0/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kDmgFfh6a12"
      },
      "source": [
        "# the file hbase-site.xml may need to be updated ...\n",
        "# however the default file works well in the simple stand-alone HBase mode\n",
        "# so no need to touch it\n",
        "#!cat $HBASE_HOME/conf/hbase-site.xml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWn-DJ1PDbXd"
      },
      "source": [
        "## Some Clean UP\n",
        "Otherwise we get ugly warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!find /content/hbase-2.5.2/lib -name slf4j*\n",
        "#!ls /content/hbase-2.5.2/lib/client-facing-thirdparty/slf4j-api-1.7.33.jar\n",
        "\n",
        "#!find /content/hbase-2.5.3/lib -name slf4j*\n",
        "#!ls /content/hbase-2.5.3/lib/client-facing-thirdparty/slf4j-api-1.7.33.jar\n",
        "\n",
        "#!find /content/hbase-2.5.5/lib -name slf4j*\n",
        "#!ls /content/hbase-2.5.5/lib/client-facing-thirdparty/slf4j-api-1.7.33.jar\n",
        "\n",
        "#!find /content/hbase-2.5.7/lib -name slf4j*\n",
        "#!ls /content/hbase-2.5.7/lib/client-facing-thirdparty/slf4j-api-1.7.33.jar\n",
        "\n",
        "!find /content/hbase-2.6.0/lib -name slf4j*\n",
        "!ls /content/hbase-2.6.0/lib/client-facing-thirdparty/slf4j-api-1.7.33.jar"
      ],
      "metadata": {
        "id": "vRILZFk5VHcG",
        "outputId": "e9897410-6649-453d-e9e3-1ffa9d7eb3f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hbase-2.6.0/lib/client-facing-thirdparty/slf4j-api-1.7.33.jar\n",
            "/content/hbase-2.6.0/lib/client-facing-thirdparty/slf4j-api-1.7.33.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $HADOOP_HOME/share/hadoop/common/lib/*slf4j*"
      ],
      "metadata": {
        "id": "FrqdOuubGZia",
        "outputId": "6bb694b3-bf88-49b1-cae9-b2b0206fd457",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar\n",
            "/usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-api-1.7.36.jar\n",
            "/usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OaOkjD5EOr0"
      },
      "source": [
        "# remove the Hadoop logger out of the path\n",
        "# without this you will get ugly warnings every time\n",
        "#\n",
        "#!mv /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar .\n",
        "#!mv /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-api-1.7.36.jar .\n",
        "#!mv /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar .\n",
        "#!mv /content/hbase-2.4.12/lib/client-facing-thirdparty/slf4j-reload4j-1.7.33.jar .\n",
        "#!mv /content/hbase-2.5.0/lib/client-facing-thirdparty/slf4j-api-1.7.33.jar .\n",
        "#!mv /content/hbase-2.5.2/lib/client-facing-thirdparty/slf4j-api-1.7.33.jar .\n",
        "#!mv /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar .\n",
        "#\n",
        "#!mv /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-api-1.7.36.jar /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-api-1.7.36.jar.disable\n",
        "#!mv /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar.disable\n",
        "#\n",
        "#!mv /usr/local/hadoop-3.3.5//share/hadoop/common/lib/slf4j-api-1.7.36.jar /usr/local/hadoop-3.3.5//share/hadoop/common/lib/slf4j-api-1.7.36.jar.disable\n",
        "#!mv /usr/local/hadoop-3.3.5//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar /usr/local/hadoop-3.3.5//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar.disable\n",
        "#\n",
        "!mv /usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-api-1.7.36.jar /usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-api-1.7.36.jar.disable\n",
        "!mv /usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar /usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar.disable"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA03iA4XFN_E"
      },
      "source": [
        "## Start HBase server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMQHzlCO7MDC",
        "outputId": "a6f6a213-a9c3-41dc-ef68-6f0ab977e41d"
      },
      "source": [
        "!start-hbase.sh\n",
        "!jps"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running master, logging to /content/hbase-2.6.0//logs/hbase--master-e5efd73160b7.out\n",
            "4549 HMaster\n",
            "4636 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd6mdgPSFSvB"
      },
      "source": [
        "# Run Hbase Shell\n",
        "do not skip these two shell commands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42DJQfpY7oqe"
      },
      "source": [
        "# you can enter hbase shell commands at the prompt. Double click on the prompt to open up a input box\n",
        "#!hbase shell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhuHJF4lH6f6"
      },
      "source": [
        "shell commands  <br>\n",
        "https://www.tutorialspoint.com/hbase/hbase_shell.htm <br>\n",
        "https://www.guru99.com/hbase-shell-general-commands.html <br>\n",
        "better way of passing shell commands are given here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQdbisL2GQtE",
        "outputId": "cc8eee91-947e-4c17-f41a-5be61f70945b"
      },
      "source": [
        "!echo 'status' | hbase shell -n\n",
        "#!echo \"status 'detail'\" | hbase shell -n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hbase:001:0> status\n",
            "1 active master, 0 backup masters, 1 servers, 0 dead, 2.0000 average load\n",
            "Took 1.1957 seconds                                                             \n",
            "hbase:002:0> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYhR2DpgIbnU",
        "outputId": "31140b00-1a34-4778-eb88-40a2a4bb3d16"
      },
      "source": [
        "!echo \"whoami\" | hbase shell -n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hbase:001:0> whoami\n",
            "root (auth:SIMPLE)\n",
            "    groups: root\n",
            "Took 0.2211 seconds                                                             \n",
            "hbase:002:0> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RMwSSDZd6T9"
      },
      "source": [
        "#Install HappyBase / Thrift <br>\n",
        "https://happybase.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qOwqJlQfQLc",
        "outputId": "31683df3-7097-4454-9c72-ad5762546737"
      },
      "source": [
        "!stop-hbase.sh"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopping hbase..............\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLfCN8jpZsZH",
        "outputId": "2f3f84af-c2fe-4bd1-c314-099d05f48e53"
      },
      "source": [
        "!pip -qq install happybase"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.0/779.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for happybase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for thriftpy2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-Wt2asZZb3m"
      },
      "source": [
        "import happybase\n",
        "#https://happybase.readthedocs.io/en/latest/"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-OVMxNEaWOq",
        "outputId": "274f3102-5523-4559-d15f-fe492344cd67"
      },
      "source": [
        "#!ls hbase-2.4.8/bin\n",
        "#!hbase-2.4.8/bin/hbase-daemon.sh start thrift\n",
        "!hbase-daemon.sh start thrift"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running thrift, logging to /content/hbase-2.6.0//logs/hbase--thrift-e5efd73160b7.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UHyIyGFf0yl",
        "outputId": "3a536d6e-bafa-44ae-bb17-5209d528c7eb"
      },
      "source": [
        "!start-hbase.sh\n",
        "!jps"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running master, logging to /content/hbase-2.6.0//logs/hbase--master-e5efd73160b7.out\n",
            "7077 Jps\n",
            "6590 ThriftServer\n",
            "6943 HMaster\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uL64b823-CB"
      },
      "source": [
        "#connection = happybase.Connection('localhost',9090, transport='framed',protocol='compact')\n",
        "#if this connection suddenly closes, you may get errors. in that case, re-run this cell again\n",
        "kxn = happybase.Connection('localhost',9090,autoconnect=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4yg79ueMhYZ"
      },
      "source": [
        "#Create Tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFjctMtoeCcW",
        "outputId": "9e810ced-8094-42ed-cdde-6813d99b28eb"
      },
      "source": [
        "kxn.open()\n",
        "kxn.tables()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zvAuebzbXj6"
      },
      "source": [
        "##Create Dept Table, Insert Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiUqgnVCMcl3"
      },
      "source": [
        "kxn.open()\n",
        "#Drop Table\n",
        "#kxn.disable_table('Dept')\n",
        "#kxn.delete_table('Dept')\n",
        "#Create Table\n",
        "kxn.create_table(\n",
        "    'Dept',\n",
        "    {'DeptID': dict(max_versions=10),\n",
        "     'DeptName': dict(max_versions=1, block_cache_enabled=False),\n",
        "     'ManagerID': dict(),  # use defaults\n",
        "     'Location': dict(),  # use defaults\n",
        "    }\n",
        ")\n",
        "kxn.close()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsULtvF6Nfcx"
      },
      "source": [
        "kxn.open()\n",
        "tDept = kxn.table('Dept')\n",
        "kxn.close()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1jju96MNoCf"
      },
      "source": [
        "#Insert one row\n",
        "kxn.open()\n",
        "tDept.put('10',{'DeptName:': 'Corporate', 'ManagerID:': '123456','Location:':'Calcutta'})\n",
        "kxn.close()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP5nd0nIOTxf",
        "outputId": "d3a18f91-b507-4d63-8ba3-9352514fb8b5"
      },
      "source": [
        "kxn.open()\n",
        "for key, data in tDept.scan():\n",
        "    print(key, data)\n",
        "kxn.close()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'10' {b'DeptName:': b'Corporate', b'Location:': b'Calcutta', b'ManagerID:': b'123456'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0Y9zgCJOywZ"
      },
      "source": [
        "#Insert 3 more rows in Batch\n",
        "kxn.open()\n",
        "b = tDept.batch()\n",
        "b.put(b'20', { b'DeptName:': b'Sales', b'ManagerID:': b'234567', b'Location:': b'Calcutta'})\n",
        "b.put(b'30', { b'DeptName:': b'Accounts', b'ManagerID:': b'567234', b'Location:': b'Calcutta'})\n",
        "b.put(b'40', { b'DeptName:': b'Production', b'ManagerID:': b'345876', b'Location:': b'Bombay'})\n",
        "b.send()\n",
        "kxn.close()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn9puFvNSQ0I",
        "outputId": "52cdc59a-6cec-4c98-8f9b-ddf269b1557f"
      },
      "source": [
        "#show all four ROWS\n",
        "kxn.open()\n",
        "#tDept = kxn.table('Dept')\n",
        "for key, value in tDept.scan():\n",
        "    print(key, value)\n",
        "kxn.close()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'10' {b'DeptName:': b'Corporate', b'Location:': b'Calcutta', b'ManagerID:': b'123456'}\n",
            "b'20' {b'DeptName:': b'Sales', b'Location:': b'Calcutta', b'ManagerID:': b'234567'}\n",
            "b'30' {b'DeptName:': b'Accounts', b'Location:': b'Calcutta', b'ManagerID:': b'567234'}\n",
            "b'40' {b'DeptName:': b'Production', b'Location:': b'Bombay', b'ManagerID:': b'345876'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sjTzUYObi7W"
      },
      "source": [
        "##Create Emp Table, LOAD data <br>\n",
        "https://people.apache.org/~stack/site/bulk-loads.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8GbQWvRcTwu"
      },
      "source": [
        "kxn.open()\n",
        "#Drop Table\n",
        "#kxn.disable_table('Emp')\n",
        "#kxn.delete_table('Emp')\n",
        "#Create Table\n",
        "kxn.create_table(\n",
        "    'Emp',\n",
        "    {'EmpID': dict(max_versions=10),\n",
        "     'LastName': dict(),  # use defaults\n",
        "     'FirstName': dict(),  # use defaults\n",
        "     'Role': dict(),  # use defaults\n",
        "     'DoJ': dict(),  # use defaults\n",
        "     'Salary': dict(),  # use defaults\n",
        "     'Comm': dict(),  # use defaults\n",
        "     'DeptID': dict(),  # use defaults\n",
        "    }\n",
        ")\n",
        "kxn.close()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9XDyx1HQxBL",
        "outputId": "7fa4de8f-c2b1-41db-ca6f-cad527c1620a"
      },
      "source": [
        "kxn.open()\n",
        "kxn.tables()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'Dept', b'Emp']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYH_g_J6423R",
        "outputId": "7c168ed2-8c3b-43a1-db21-666635538dd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Download txt file with Employee Data\n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/Documents/Employee.csv\n",
        "!cat Employee.csv"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "742866,Bacchan,Amitabh,Executive,2003-03-10,50000,0.1,10\n",
            "349870,Mukherjee,Rani,Manager,2005-05-04,25000,0.06,40\n",
            "865477,Dikshit,Madhuri,Clerk,2002-04-04,10000,0.02,20\n",
            "239456,Khan,Shahrukh,Manager,2004-01-03,30000,0.07,20\n",
            "897889,Sehwag,Virender,Cus_Rep,2005-01-02,15000,0.05,20\n",
            "123980,Dhoni,Mahender,Clerk,2004-10-09,9000,0.02,40\n",
            "822134,Dravid,Rahul,Sr Manager,2000-06-04,40000,0.08,30\n",
            "997445,Dalmia,Jagmohan,Clerk,2001-07-01,12000,0.02,30\n",
            "989007,Ganguly,Sourav,Cus_Rep,2002-01-01,20000,0.03,40\n",
            "299034,Ganesan,Rekha,Director,2002-10-10,60000,0.11,10\n",
            "546223,Karthikeyan,Narayan,Secretary,2005-12-04,40000,0.09,10\n",
            "223112,Mirza,Sania,Cus_Rep,2001-11-19,25000,0.04,30"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!mv /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-api-1.7.36.jar.disable /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-api-1.7.36.jar\n",
        "#!mv /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar.disable /usr/local/hadoop-3.3.3//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar\n",
        "#\n",
        "!mv $HADOOP_HOME//share/hadoop/common/lib/slf4j-api-1.7.36.jar.disable $HADOOP_HOME/share/hadoop/common/lib/slf4j-api-1.7.36.jar\n",
        "!mv $HADOOP_HOME//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar.disable $HADOOP_HOME//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar"
      ],
      "metadata": {
        "id": "5pTVOECVHurl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZbsNgyBd-aN"
      },
      "source": [
        "# input file has to be moved from regular file system to HDFS file system\n",
        "!hdfs dfs -copyFromLocal Employee.csv /tmp"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFRp3yateTCS",
        "outputId": "80b438ce-f3a3-4e04-952c-b85813a147ee"
      },
      "source": [
        "# https://community.cloudera.com/t5/Community-Articles/Import-CSV-data-into-HBase-using-importtsv/ta-p/244842\n",
        "#hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=,  -Dimporttsv.columns=\"HBASE_ROW_KEY,id,temp:in,temp:out,vibration,pressure:in,pressure:out\" sensor hdfs://sandbox.hortonworks.com:/tmp/hbase.csv\n",
        "!hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=','  -Dimporttsv.columns=\"HBASE_ROW_KEY,LastName:,FirstName:,Role:,DoJ:,Salary:,Comm:,DeptID:\" Emp /tmp/Employee.csv"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/content/hbase-2.6.0/lib/client-facing-thirdparty/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
            "2024-06-14 13:40:48,230 INFO  [main] Configuration.deprecation (Configuration.java:logDeprecation(1294)) - hbase.client.pause.cqtbe is deprecated. Instead, use hbase.client.pause.server.overloaded\n",
            "2024-06-14 13:40:48,358 INFO  [main] common.X509Util (X509Util.java:<clinit>(78)) - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\n",
            "2024-06-14 13:40:48,367 WARN  [main] client.ZKConnectionRegistry (ZKConnectionRegistry.java:<init>(83)) - ZKConnectionRegistry is deprecated. See https://hbase.apache.org/book.html#client.rpcconnectionregistry\n",
            "2024-06-14 13:40:48,390 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:zookeeper.version=3.8.4-9316c2a7a97e1666d8f4593f34dd6fc36ecc436c, built on 2024-02-12 22:16 UTC\n",
            "2024-06-14 13:40:48,390 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:host.name=e5efd73160b7\n",
            "2024-06-14 13:40:48,390 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.version=1.8.0_412\n",
            "2024-06-14 13:40:48,390 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.vendor=Private Build\n",
            "2024-06-14 13:40:48,391 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre\n",
            "2024-06-14 13:40:48,391 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.class.path=/content/hbase-2.6.0//conf:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/content/hbase-2.6.0/:/content/hbase-2.6.0//lib/agrona-1.12.0.jar:/content/hbase-2.6.0//lib/aircompressor-0.24.jar:/content/hbase-2.6.0//lib/aopalliance-1.0.jar:/content/hbase-2.6.0//lib/apacheds-i18n-2.0.0-M15.jar:/content/hbase-2.6.0//lib/apacheds-kerberos-codec-2.0.0-M15.jar:/content/hbase-2.6.0//lib/api-asn1-api-1.0.0-M20.jar:/content/hbase-2.6.0//lib/api-i18n-1.0.0-M20.jar:/content/hbase-2.6.0//lib/api-util-1.0.0-M20.jar:/content/hbase-2.6.0//lib/asm-3.1.jar:/content/hbase-2.6.0//lib/avro-1.11.3.jar:/content/hbase-2.6.0//lib/brotli4j-1.11.0.jar:/content/hbase-2.6.0//lib/byte-buddy-1.12.19.jar:/content/hbase-2.6.0//lib/byte-buddy-1.14.9.jar:/content/hbase-2.6.0//lib/byte-buddy-agent-1.12.19.jar:/content/hbase-2.6.0//lib/caffeine-2.8.1.jar:/content/hbase-2.6.0//lib/checker-qual-3.1.0.jar:/content/hbase-2.6.0//lib/commons-cli-1.5.0.jar:/content/hbase-2.6.0//lib/commons-codec-1.15.jar:/content/hbase-2.6.0//lib/commons-collections-3.2.2.jar:/content/hbase-2.6.0//lib/commons-compress-1.21.jar:/content/hbase-2.6.0//lib/commons-configuration-1.6.jar:/content/hbase-2.6.0//lib/commons-crypto-1.1.0.jar:/content/hbase-2.6.0//lib/commons-csv-1.0.jar:/content/hbase-2.6.0//lib/commons-daemon-1.0.13.jar:/content/hbase-2.6.0//lib/commons-digester-1.8.jar:/content/hbase-2.6.0//lib/commons-io-2.11.0.jar:/content/hbase-2.6.0//lib/commons-lang-2.6.jar:/content/hbase-2.6.0//lib/commons-lang3-3.9.jar:/content/hbase-2.6.0//lib/commons-math3-3.6.1.jar:/content/hbase-2.6.0//lib/commons-net-3.1.jar:/content/hbase-2.6.0//lib/curator-client-4.2.0.jar:/content/hbase-2.6.0//lib/curator-framework-4.2.0.jar:/content/hbase-2.6.0//lib/curator-recipes-4.2.0.jar:/content/hbase-2.6.0//lib/disruptor-3.4.4.jar:/content/hbase-2.6.0//lib/ehcache-3.3.1.jar:/content/hbase-2.6.0//lib/error_prone_annotations-2.26.1.jar:/content/hbase-2.6.0//lib/fst-2.50.jar:/content/hbase-2.6.0//lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/content/hbase-2.6.0//lib/gson-2.2.4.jar:/content/hbase-2.6.0//lib/guava-11.0.2.jar:/content/hbase-2.6.0//lib/guice-3.0.jar:/content/hbase-2.6.0//lib/guice-servlet-3.0.jar:/content/hbase-2.6.0//lib/hadoop-annotations-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-auth-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-client-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-common-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-distcp-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-hdfs-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-hdfs-2.10.2-tests.jar:/content/hbase-2.6.0//lib/hadoop-hdfs-client-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-app-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-common-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-core-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-hs-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-jobclient-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-shuffle-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-minicluster-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-api-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-client-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-common-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-registry-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-applicationhistoryservice-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-common-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-nodemanager-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-resourcemanager-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-tests-2.10.2-tests.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-timelineservice-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-web-proxy-2.10.2.jar:/content/hbase-2.6.0//lib/hamcrest-core-1.3.jar:/content/hbase-2.6.0//lib/hbase-annotations-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-annotations-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-asyncfs-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-asyncfs-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-backup-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-client-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-common-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-common-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-compression-aircompressor-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-compression-brotli-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-compression-lz4-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-compression-snappy-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-compression-zstd-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-endpoint-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-external-blockcache-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-hadoop2-compat-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-hadoop2-compat-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-hadoop-compat-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-hadoop-compat-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-hbtop-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-http-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-it-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-it-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-logging-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-mapreduce-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-mapreduce-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-metrics-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-metrics-api-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-openssl-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-procedure-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-procedure-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-protocol-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-protocol-shaded-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-replication-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-resource-bundle-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-rest-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-rsgroup-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-rsgroup-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-server-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-server-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-shaded-gson-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-jackson-jaxrs-json-provider-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-jersey-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-jetty-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-miscellaneous-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-netty-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-netty-tcnative-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-protobuf-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shell-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-testing-util-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-thrift-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-unsafe-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-zookeeper-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-zookeeper-2.6.0-tests.jar:/content/hbase-2.6.0//lib/HikariCP-java7-2.4.12.jar:/content/hbase-2.6.0//lib/httpclient-4.5.13.jar:/content/hbase-2.6.0//lib/httpcore-4.4.13.jar:/content/hbase-2.6.0//lib/jackson-annotations-2.17.0.jar:/content/hbase-2.6.0//lib/jackson-core-2.17.0.jar:/content/hbase-2.6.0//lib/jackson-core-asl-1.9.13.jar:/content/hbase-2.6.0//lib/jackson-databind-2.17.0.jar:/content/hbase-2.6.0//lib/jackson-mapper-asl-1.9.13.jar:/content/hbase-2.6.0//lib/jackson-module-jaxb-annotations-2.17.0.jar:/content/hbase-2.6.0//lib/jakarta.inject-2.6.1.jar:/content/hbase-2.6.0//lib/jakarta.validation-api-2.0.2.jar:/content/hbase-2.6.0//lib/jamon-runtime-2.4.1.jar:/content/hbase-2.6.0//lib/javassist-3.30.2-GA.jar:/content/hbase-2.6.0//lib/java-util-1.9.0.jar:/content/hbase-2.6.0//lib/javax.activation-api-1.2.0.jar:/content/hbase-2.6.0//lib/javax.annotation-api-1.2.jar:/content/hbase-2.6.0//lib/javax.el-3.0.1-b08.jar:/content/hbase-2.6.0//lib/java-xmlbuilder-0.4.jar:/content/hbase-2.6.0//lib/javax.servlet-api-3.1.0.jar:/content/hbase-2.6.0//lib/javax.servlet.jsp-2.3.2.jar:/content/hbase-2.6.0//lib/javax.servlet.jsp-api-2.3.1.jar:/content/hbase-2.6.0//lib/jaxb-api-2.3.1.jar:/content/hbase-2.6.0//lib/jaxb-impl-2.2.3-1.jar:/content/hbase-2.6.0//lib/jcip-annotations-1.0-1.jar:/content/hbase-2.6.0//lib/jcodings-1.0.58.jar:/content/hbase-2.6.0//lib/jets3t-0.9.0.jar:/content/hbase-2.6.0//lib/jettison-1.5.4.jar:/content/hbase-2.6.0//lib/jetty-6.1.26.jar:/content/hbase-2.6.0//lib/jetty-sslengine-6.1.26.jar:/content/hbase-2.6.0//lib/jetty-util-6.1.26.jar:/content/hbase-2.6.0//lib/joni-2.2.1.jar:/content/hbase-2.6.0//lib/jsch-0.1.55.jar:/content/hbase-2.6.0//lib/json-io-2.5.1.jar:/content/hbase-2.6.0//lib/jsr311-api-1.1.1.jar:/content/hbase-2.6.0//lib/junit-4.13.2.jar:/content/hbase-2.6.0//lib/leveldbjni-all-1.8.jar:/content/hbase-2.6.0//lib/libthrift-0.14.1.jar:/content/hbase-2.6.0//lib/lz4-java-1.8.0.jar:/content/hbase-2.6.0//lib/metrics-core-3.2.6.jar:/content/hbase-2.6.0//lib/mockito-core-4.11.0.jar:/content/hbase-2.6.0//lib/mssql-jdbc-6.2.1.jre7.jar:/content/hbase-2.6.0//lib/native-linux-aarch64-1.11.0.jar:/content/hbase-2.6.0//lib/netty-3.10.6.Final.jar:/content/hbase-2.6.0//lib/netty-all-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-buffer-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-dns-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-haproxy-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-http2-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-http-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-memcache-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-mqtt-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-redis-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-smtp-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-socks-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-stomp-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-xml-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-common-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-handler-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-handler-proxy-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-handler-ssl-ocsp-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-resolver-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-resolver-dns-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-resolver-dns-classes-macos-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-resolver-dns-native-macos-4.1.108.Final-osx-aarch_64.jar:/content/hbase-2.6.0//lib/netty-resolver-dns-native-macos-4.1.108.Final-osx-x86_64.jar:/content/hbase-2.6.0//lib/netty-transport-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-classes-epoll-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-classes-kqueue-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-native-epoll-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-native-epoll-4.1.108.Final-linux-aarch_64.jar:/content/hbase-2.6.0//lib/netty-transport-native-epoll-4.1.108.Final-linux-riscv64.jar:/content/hbase-2.6.0//lib/netty-transport-native-epoll-4.1.108.Final-linux-x86_64.jar:/content/hbase-2.6.0//lib/netty-transport-native-kqueue-4.1.108.Final-osx-aarch_64.jar:/content/hbase-2.6.0//lib/netty-transport-native-kqueue-4.1.108.Final-osx-x86_64.jar:/content/hbase-2.6.0//lib/netty-transport-native-unix-common-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-rxtx-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-sctp-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-udt-4.1.108.Final.jar:/content/hbase-2.6.0//lib/nimbus-jose-jwt-7.9.jar:/content/hbase-2.6.0//lib/objenesis-3.3.jar:/content/hbase-2.6.0//lib/okhttp-2.7.5.jar:/content/hbase-2.6.0//lib/okio-1.6.0.jar:/content/hbase-2.6.0//lib/opentelemetry-api-1.15.0.jar:/content/hbase-2.6.0//lib/opentelemetry-context-1.15.0.jar:/content/hbase-2.6.0//lib/opentelemetry-semconv-1.15.0-alpha.jar:/content/hbase-2.6.0//lib/protobuf-java-2.5.0.jar:/content/hbase-2.6.0//lib/service-1.11.0.jar:/content/hbase-2.6.0//lib/snappy-java-1.1.10.4.jar:/content/hbase-2.6.0//lib/spymemcached-2.12.2.jar:/content/hbase-2.6.0//lib/stax2-api-4.2.1.jar:/content/hbase-2.6.0//lib/woodstox-core-5.3.0.jar:/content/hbase-2.6.0//lib/xmlenc-0.52.jar:/content/hbase-2.6.0//lib/zookeeper-3.8.4.jar:/content/hbase-2.6.0//lib/zookeeper-jute-3.8.4.jar:/content/hbase-2.6.0//lib/zstd-jni-1.5.5-2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/audience-annotations-0.13.0.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/commons-logging-1.2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/htrace-core4-4.1.0-incubating.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/jcl-over-slf4j-1.7.33.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/jul-to-slf4j-1.7.33.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/opentelemetry-api-1.15.0.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/opentelemetry-context-1.15.0.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/opentelemetry-semconv-1.15.0-alpha.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/slf4j-api-1.7.33.jar:/usr/local/hadoop-3.4.0//etc/hadoop:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-haproxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-http-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-math3-3.6.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/hadoop-auth-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-xml-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-handler-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-util-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-simplekdc-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-redis-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-cli-1.5.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-sctp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-handler-proxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-core-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/hadoop-annotations-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-admin-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-xml-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-stomp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/dnsjava-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-compress-1.24.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/zookeeper-3.8.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-server-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-xdr-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-http-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/zookeeper-jute-3.8.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-all-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-udt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-util-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-server-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-io-2.14.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-socks-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-mqtt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-webapp-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-logging-1.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/nimbus-jose-jwt-9.31.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/bcprov-jdk15on-1.70.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/token-provider-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-config-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jline-3.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-http2-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/hadoop-shaded-guava-1.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-identity-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-security-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-buffer-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-io-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-util-ajax-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-util-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-client-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/avro-1.9.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/audience-annotations-0.12.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-common-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-smtp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-rxtx-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-memcache-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-servlet-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-common-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-common-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-registry-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-nfs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-kms-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-http-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/hadoop-auth-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-xml-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-handler-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-simplekdc-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-cli-1.5.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-handler-proxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/hadoop-annotations-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-admin-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-xml-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/dnsjava-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-compress-1.24.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/zookeeper-3.8.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-server-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-xdr-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-http-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/zookeeper-jute-3.8.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-all-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-util-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-server-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-io-2.14.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-webapp-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-logging-1.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/nimbus-jose-jwt-9.31.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/token-provider-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jline-3.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/hadoop-shaded-guava-1.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-identity-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-security-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-buffer-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-io-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-util-ajax-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-client-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/avro-1.9.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-common-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-servlet-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-client-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-client-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jetty-jndi-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-api-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/codemodel-2.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/asm-tree-9.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/bcutil-jdk15on-1.70.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-common-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/asm-commons-9.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jetty-plus-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jetty-client-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-server-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/guice-4.2.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jetty-annotations-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-servlet-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/bcpkix-jdk15on-1.70.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-client-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-api-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-services-api-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-services-core-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-common-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-tests-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-registry-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-client-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-router-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-common-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.0.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/log4j-1.2-api-2.17.2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/log4j-api-2.17.2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/log4j-core-2.17.2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/log4j-slf4j-impl-2.17.2.jar\n",
            "2024-06-14 13:40:48,563 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.library.path=/usr/local/hadoop-3.4.0//lib/native\n",
            "2024-06-14 13:40:48,564 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.io.tmpdir=/tmp\n",
            "2024-06-14 13:40:48,564 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.compiler=<NA>\n",
            "2024-06-14 13:40:48,564 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.name=Linux\n",
            "2024-06-14 13:40:48,564 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.arch=amd64\n",
            "2024-06-14 13:40:48,564 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.version=6.1.85+\n",
            "2024-06-14 13:40:48,564 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:user.name=root\n",
            "2024-06-14 13:40:48,564 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:user.home=/root\n",
            "2024-06-14 13:40:48,564 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:user.dir=/content\n",
            "2024-06-14 13:40:48,565 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.memory.free=158MB\n",
            "2024-06-14 13:40:48,565 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.memory.max=3229MB\n",
            "2024-06-14 13:40:48,565 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.memory.total=197MB\n",
            "2024-06-14 13:40:48,571 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (ZooKeeper.java:<init>(637)) - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$20/1992884247@29db10ab\n",
            "2024-06-14 13:40:48,578 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ClientCnxnSocket (ClientCnxnSocket.java:initProperties(239)) - jute.maxbuffer value is 1048575 Bytes\n",
            "2024-06-14 13:40:48,592 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ClientCnxn (ClientCnxn.java:initRequestTimeout(1747)) - zookeeper.request.timeout value is 0. feature enabled=false\n",
            "2024-06-14 13:40:48,614 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1177)) - Opening socket connection to server localhost/127.0.0.1:2181.\n",
            "2024-06-14 13:40:48,614 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1179)) - SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
            "2024-06-14 13:40:48,623 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(1013)) - Socket connection established, initiating session, client: /127.0.0.1:35066, server: localhost/127.0.0.1:2181\n",
            "2024-06-14 13:40:48,635 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1453)) - Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x100001115770003, negotiated timeout = 40000\n",
            "2024-06-14 13:40:53,080 INFO  [main] client.ConnectionImplementation (ConnectionImplementation.java:closeMasterService(2127)) - Closing master protocol: MasterService\n",
            "2024-06-14 13:40:53,135 INFO  [main] Configuration.deprecation (Configuration.java:logDeprecation(1294)) - session.id is deprecated. Instead, use dfs.metrics.session-id\n",
            "2024-06-14 13:40:53,140 INFO  [main] jvm.JvmMetrics (JvmMetrics.java:init(79)) - Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
            "2024-06-14 13:40:53,191 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (ZooKeeper.java:close(1232)) - Session: 0x100001115770003 closed\n",
            "2024-06-14 13:40:53,192 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(569)) - EventThread shut down for session: 0x100001115770003\n",
            "2024-06-14 13:40:53,258 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x59fc684e] zookeeper.ZooKeeper (ZooKeeper.java:<init>(637)) - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$20/1992884247@29db10ab\n",
            "2024-06-14 13:40:53,261 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x59fc684e] zookeeper.ClientCnxnSocket (ClientCnxnSocket.java:initProperties(239)) - jute.maxbuffer value is 1048575 Bytes\n",
            "2024-06-14 13:40:53,262 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x59fc684e] zookeeper.ClientCnxn (ClientCnxn.java:initRequestTimeout(1747)) - zookeeper.request.timeout value is 0. feature enabled=false\n",
            "2024-06-14 13:40:53,274 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x59fc684e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1177)) - Opening socket connection to server localhost/127.0.0.1:2181.\n",
            "2024-06-14 13:40:53,274 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x59fc684e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1179)) - SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
            "2024-06-14 13:40:53,275 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x59fc684e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(1013)) - Socket connection established, initiating session, client: /127.0.0.1:35068, server: localhost/127.0.0.1:2181\n",
            "2024-06-14 13:40:53,283 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x59fc684e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1453)) - Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x100001115770004, negotiated timeout = 40000\n",
            "2024-06-14 13:40:53,476 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x59fc684e] zookeeper.ZooKeeper (ZooKeeper.java:close(1232)) - Session: 0x100001115770004 closed\n",
            "2024-06-14 13:40:53,476 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x59fc684e-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(569)) - EventThread shut down for session: 0x100001115770004\n",
            "2024-06-14 13:40:53,734 INFO  [main] input.FileInputFormat (FileInputFormat.java:listStatus(289)) - Total input files to process : 1\n",
            "2024-06-14 13:40:53,803 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(204)) - number of splits:1\n",
            "2024-06-14 13:40:54,317 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(300)) - Submitting tokens for job: job_local559445773_0001\n",
            "2024-06-14 13:40:55,151 INFO  [main] mapred.LocalDistributedCacheManager (LocalDistributedCacheManager.java:symlink(200)) - Creating symlink: /tmp/hadoop-root/mapred/local/1718372454563/libjars <- /content/libjars/*\n",
            "2024-06-14 13:40:55,178 WARN  [main] fs.FileUtil (FileUtil.java:symLink(888)) - Command 'ln -s /tmp/hadoop-root/mapred/local/1718372454563/libjars /content/libjars/*' failed 1 with: ln: failed to create symbolic link '/content/libjars/*': No such file or directory\n",
            "\n",
            "2024-06-14 13:40:55,181 WARN  [main] mapred.LocalDistributedCacheManager (LocalDistributedCacheManager.java:symlink(202)) - Failed to create symlink: /tmp/hadoop-root/mapred/local/1718372454563/libjars <- /content/libjars/*\n",
            "2024-06-14 13:40:55,181 INFO  [main] mapred.LocalDistributedCacheManager (LocalDistributedCacheManager.java:setup(164)) - Localized file:/tmp/hadoop-root/mapred/staging/root559445773/.staging/job_local559445773_0001/libjars as file:/tmp/hadoop-root/mapred/local/1718372454563/libjars\n",
            "2024-06-14 13:40:55,415 INFO  [main] mapreduce.Job (Job.java:submit(1577)) - The url to track the job: http://localhost:8080/\n",
            "2024-06-14 13:40:55,416 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1622)) - Running job: job_local559445773_0001\n",
            "2024-06-14 13:40:55,421 INFO  [Thread-7] mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(500)) - OutputCommitter set in config null\n",
            "2024-06-14 13:40:55,490 INFO  [Thread-7] mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(518)) - OutputCommitter is org.apache.hadoop.hbase.mapreduce.TableOutputCommitter\n",
            "2024-06-14 13:40:55,575 INFO  [Thread-7] mapred.LocalJobRunner (LocalJobRunner.java:runTasks(477)) - Waiting for map tasks\n",
            "2024-06-14 13:40:55,576 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner (LocalJobRunner.java:run(251)) - Starting task: attempt_local559445773_0001_m_000000_0\n",
            "2024-06-14 13:40:55,802 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task (Task.java:initialize(620)) -  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-06-14 13:40:55,832 INFO  [LocalJobRunner Map Task Executor #0] mapred.MapTask (MapTask.java:runNewMapper(763)) - Processing split: file:/tmp/Employee.csv:0+661\n",
            "2024-06-14 13:40:55,888 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x0fc331be] zookeeper.ZooKeeper (ZooKeeper.java:<init>(637)) - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$20/1992884247@29db10ab\n",
            "2024-06-14 13:40:55,889 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x0fc331be] zookeeper.ClientCnxnSocket (ClientCnxnSocket.java:initProperties(239)) - jute.maxbuffer value is 1048575 Bytes\n",
            "2024-06-14 13:40:55,889 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x0fc331be] zookeeper.ClientCnxn (ClientCnxn.java:initRequestTimeout(1747)) - zookeeper.request.timeout value is 0. feature enabled=false\n",
            "2024-06-14 13:40:55,903 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x0fc331be-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1177)) - Opening socket connection to server localhost/127.0.0.1:2181.\n",
            "2024-06-14 13:40:55,903 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x0fc331be-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1179)) - SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
            "2024-06-14 13:40:55,904 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x0fc331be-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(1013)) - Socket connection established, initiating session, client: /127.0.0.1:35080, server: localhost/127.0.0.1:2181\n",
            "2024-06-14 13:40:55,910 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x0fc331be-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1453)) - Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x100001115770005, negotiated timeout = 40000\n",
            "2024-06-14 13:40:55,932 INFO  [LocalJobRunner Map Task Executor #0] mapreduce.TableOutputFormat (TableOutputFormat.java:<init>(100)) - Created table instance for Emp\n",
            "2024-06-14 13:40:55,983 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x1b312cd9] zookeeper.ZooKeeper (ZooKeeper.java:<init>(637)) - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$20/1992884247@29db10ab\n",
            "2024-06-14 13:40:55,984 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x1b312cd9] zookeeper.ClientCnxnSocket (ClientCnxnSocket.java:initProperties(239)) - jute.maxbuffer value is 1048575 Bytes\n",
            "2024-06-14 13:40:55,985 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x1b312cd9] zookeeper.ClientCnxn (ClientCnxn.java:initRequestTimeout(1747)) - zookeeper.request.timeout value is 0. feature enabled=false\n",
            "2024-06-14 13:40:55,999 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x1b312cd9-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1177)) - Opening socket connection to server localhost/127.0.0.1:2181.\n",
            "2024-06-14 13:40:55,999 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x1b312cd9-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1179)) - SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
            "2024-06-14 13:40:56,004 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x1b312cd9-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(1013)) - Socket connection established, initiating session, client: /127.0.0.1:35088, server: localhost/127.0.0.1:2181\n",
            "2024-06-14 13:40:56,010 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x1b312cd9-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1453)) - Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x100001115770006, negotiated timeout = 40000\n",
            "2024-06-14 13:40:56,220 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(620)) - \n",
            "2024-06-14 13:40:56,310 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x1b312cd9] zookeeper.ZooKeeper (ZooKeeper.java:close(1232)) - Session: 0x100001115770006 closed\n",
            "2024-06-14 13:40:56,314 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x1b312cd9-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(569)) - EventThread shut down for session: 0x100001115770006\n",
            "2024-06-14 13:40:56,394 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task (Task.java:done(1105)) - Task:attempt_local559445773_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-06-14 13:40:56,430 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1643)) - Job job_local559445773_0001 running in uber mode : false\n",
            "2024-06-14 13:40:56,431 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1650)) -  map 0% reduce 0%\n",
            "2024-06-14 13:40:56,442 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(620)) - map\n",
            "2024-06-14 13:40:56,442 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task (Task.java:sendDone(1243)) - Task 'attempt_local559445773_0001_m_000000_0' done.\n",
            "2024-06-14 13:40:56,449 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task (Task.java:done(1139)) - Final Counters for attempt_local559445773_0001_m_000000_0: Counters: 19\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=447434\n",
            "\t\tFILE: Number of bytes written=1031047\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=12\n",
            "\t\tMap output records=12\n",
            "\t\tInput split bytes=87\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tCPU time spent (ms)=760\n",
            "\t\tPhysical memory (bytes) snapshot=291135488\n",
            "\t\tVirtual memory (bytes) snapshot=5172146176\n",
            "\t\tTotal committed heap usage (bytes)=206831616\n",
            "\tImportTsv\n",
            "\t\tBad Lines=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=685\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=0\n",
            "2024-06-14 13:40:56,450 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner (LocalJobRunner.java:run(276)) - Finishing task: attempt_local559445773_0001_m_000000_0\n",
            "2024-06-14 13:40:56,450 INFO  [Thread-7] mapred.LocalJobRunner (LocalJobRunner.java:runTasks(485)) - map task executor complete.\n",
            "2024-06-14 13:40:56,498 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x0fc331be] zookeeper.ZooKeeper (ZooKeeper.java:close(1232)) - Session: 0x100001115770005 closed\n",
            "2024-06-14 13:40:56,499 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x0fc331be-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(569)) - EventThread shut down for session: 0x100001115770005\n",
            "2024-06-14 13:40:57,434 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1650)) -  map 100% reduce 0%\n",
            "2024-06-14 13:40:57,434 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1661)) - Job job_local559445773_0001 completed successfully\n",
            "2024-06-14 13:40:57,442 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1668)) - Counters: 19\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=447434\n",
            "\t\tFILE: Number of bytes written=1031047\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=12\n",
            "\t\tMap output records=12\n",
            "\t\tInput split bytes=87\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=13\n",
            "\t\tCPU time spent (ms)=760\n",
            "\t\tPhysical memory (bytes) snapshot=291135488\n",
            "\t\tVirtual memory (bytes) snapshot=5172146176\n",
            "\t\tTotal committed heap usage (bytes)=206831616\n",
            "\tImportTsv\n",
            "\t\tBad Lines=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=685\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxvMPuZ8gOW5"
      },
      "source": [
        "kxn.open()\n",
        "tEmp = kxn.table('Emp')\n",
        "kxn.close()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uonqOE2AgXeq",
        "outputId": "0d3394fb-35f1-4a50-b435-eca6553dcb61"
      },
      "source": [
        "kxn.open()\n",
        "#tDept = kxn.table('Dept')\n",
        "for key, data in tEmp.scan():\n",
        "    print(key, data)\n",
        "kxn.close()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'123980' {b'Comm:': b'0.02', b'DeptID:': b'40', b'DoJ:': b'2004-10-09', b'FirstName:': b'Mahender', b'LastName:': b'Dhoni', b'Role:': b'Clerk', b'Salary:': b'9000'}\n",
            "b'223112' {b'Comm:': b'0.04', b'DeptID:': b'30', b'DoJ:': b'2001-11-19', b'FirstName:': b'Sania', b'LastName:': b'Mirza', b'Role:': b'Cus_Rep', b'Salary:': b'25000'}\n",
            "b'239456' {b'Comm:': b'0.07', b'DeptID:': b'20', b'DoJ:': b'2004-01-03', b'FirstName:': b'Shahrukh', b'LastName:': b'Khan', b'Role:': b'Manager', b'Salary:': b'30000'}\n",
            "b'299034' {b'Comm:': b'0.11', b'DeptID:': b'10', b'DoJ:': b'2002-10-10', b'FirstName:': b'Rekha', b'LastName:': b'Ganesan', b'Role:': b'Director', b'Salary:': b'60000'}\n",
            "b'349870' {b'Comm:': b'0.06', b'DeptID:': b'40', b'DoJ:': b'2005-05-04', b'FirstName:': b'Rani', b'LastName:': b'Mukherjee', b'Role:': b'Manager', b'Salary:': b'25000'}\n",
            "b'546223' {b'Comm:': b'0.09', b'DeptID:': b'10', b'DoJ:': b'2005-12-04', b'FirstName:': b'Narayan', b'LastName:': b'Karthikeyan', b'Role:': b'Secretary', b'Salary:': b'40000'}\n",
            "b'742866' {b'Comm:': b'0.1', b'DeptID:': b'10', b'DoJ:': b'2003-03-10', b'FirstName:': b'Amitabh', b'LastName:': b'Bacchan', b'Role:': b'Executive', b'Salary:': b'50000'}\n",
            "b'822134' {b'Comm:': b'0.08', b'DeptID:': b'30', b'DoJ:': b'2000-06-04', b'FirstName:': b'Rahul', b'LastName:': b'Dravid', b'Role:': b'Sr Manager', b'Salary:': b'40000'}\n",
            "b'865477' {b'Comm:': b'0.02', b'DeptID:': b'20', b'DoJ:': b'2002-04-04', b'FirstName:': b'Madhuri', b'LastName:': b'Dikshit', b'Role:': b'Clerk', b'Salary:': b'10000'}\n",
            "b'897889' {b'Comm:': b'0.05', b'DeptID:': b'20', b'DoJ:': b'2005-01-02', b'FirstName:': b'Virender', b'LastName:': b'Sehwag', b'Role:': b'Cus_Rep', b'Salary:': b'15000'}\n",
            "b'989007' {b'Comm:': b'0.03', b'DeptID:': b'40', b'DoJ:': b'2002-01-01', b'FirstName:': b'Sourav', b'LastName:': b'Ganguly', b'Role:': b'Cus_Rep', b'Salary:': b'20000'}\n",
            "b'997445' {b'Comm:': b'0.02', b'DeptID:': b'30', b'DoJ:': b'2001-07-01', b'FirstName:': b'Jagmohan', b'LastName:': b'Dalmia', b'Role:': b'Clerk', b'Salary:': b'12000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS__XdMYjLzd"
      },
      "source": [
        "## Creating, INSERTing Denormalised EmpDept Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQRpLsmfjbgp"
      },
      "source": [
        "kxn.open()\n",
        "#Drop Table\n",
        "#kxn.disable_table('EmpDept')\n",
        "#kxn.delete_table('EmpDept')\n",
        "#Create Table\n",
        "kxn.create_table(\n",
        "    'EmpDept',\n",
        "    {'EmpID': dict(max_versions=10),\n",
        "     'Emp': dict(),  # use defaults\n",
        "     'Dept': dict(),  # use defaults\n",
        "    }\n",
        ")\n",
        "kxn.close()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI6Zk4bKqMas",
        "outputId": "16e301eb-1845-4fe8-fd2d-1eb199285e5a"
      },
      "source": [
        "kxn.open()\n",
        "kxn.tables()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'Dept', b'Emp', b'EmpDept']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duGwwpF1n5IG"
      },
      "source": [
        "kxn.open()\n",
        "tEmpDept = kxn.table('EmpDept')\n",
        "b = tEmpDept.batch()\n",
        "b.put(b'742866', { b'Emp:FirstName': b'Amitabh', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta'})\n",
        "b.put(b'349870', { b'Emp:LastName': b'Mukheree', b'Dept:ManagerID': b'567234', b'Dept:Location:': b'Bombay'})\n",
        "b.send()\n",
        "kxn.close()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzxFRXp7rS87",
        "outputId": "a25b6d94-d2c6-4401-a4fd-5dd6b9fdc6a2"
      },
      "source": [
        "kxn.open()\n",
        "#tDept = kxn.table('Dept')\n",
        "for key, data in tEmpDept.scan():\n",
        "    print(key, data)\n",
        "kxn.close()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'349870' {b'Dept:Location:': b'Bombay', b'Dept:ManagerID': b'567234', b'Emp:LastName': b'Mukheree'}\n",
            "b'742866' {b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Emp:FirstName': b'Amitabh'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWZKpJ_LtMnQ"
      },
      "source": [
        "##Creating and LOADing denormalised tables EmpDept2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giAKNFnMtJ-b"
      },
      "source": [
        "kxn.open()\n",
        "#Drop Table\n",
        "#kxn.disable_table('EmpDept2')\n",
        "#kxn.delete_table('EmpDept2')\n",
        "#Create Table\n",
        "kxn.create_table(\n",
        "    'EmpDept2',\n",
        "    {'EmpID': dict(max_versions=10),\n",
        "     'Emp': dict(),  # use defaults\n",
        "     'Dept': dict(),  # use defaults\n",
        "    }\n",
        ")\n",
        "kxn.close()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2WwCA4euf-i",
        "outputId": "8d8d4229-b7e4-469f-85e1-058be9d828ff"
      },
      "source": [
        "kxn.open()\n",
        "kxn.tables()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'Dept', b'Emp', b'EmpDept', b'EmpDept2']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fijaJ_G5ktF",
        "outputId": "414fc8be-5e28-49d3-8386-4e9b762816ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/Documents/EmployeeDept2.txt\n",
        "!cat EmployeeDept2.txt"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "742866,Bacchan,Amitabh,Executive,2003-03-10,50000,0.1,10,Corporate,299034,Calcutta\n",
            "349870,Mukherjee,Rani,Manager,2005-05-04,25000,0.06,40,Production,349870,Bombay\n",
            "865477,Dikshit,Madhuri,Clerk,2002-04-04,10000,0.02,20,Sales,239456,Calcutta\n",
            "239456,Khan,Shahrukh,Manager,2004-01-03,30000,0.07,20,Sales,239456,Calcutta\n",
            "897889,Sehwag,Virender,Cus_Rep,2005-01-02,15000,0.05,20,Sales,239456,Calcutta\n",
            "123980,Dhoni,Mahender,Clerk,2004-10-09,9000,0.02,40,Production,349870,Bombay\n",
            "822134,Dravid,Rahul,Sr Manager,2000-06-04,40000,0.08,30,Accounts,822134,Calcutta\n",
            "997445,Dalmia,Jagmohan,Clerk,2001-07-01,12000,0.02,30,Accounts,822134,Calcutta\n",
            "989007,Ganguly,Sourav,Cus_Rep,2002-01-01,20000,0.03,40,Production,349870,Bombay\n",
            "299034,Ganesan,Rekha,Director,2002-10-10,60000,0.11,10,Corporate,299034,Calcutta\n",
            "546223,Karthikeyan,Narayan,Secretary,2005-12-04,40000,0.09,10,Corporate,299034,Calcutta\n",
            "223112,Mirza,Sania,Cus_Rep,2001-11-19,25000,0.04,30,Accounts,822134,Calcutta"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvItjBLuu7oM"
      },
      "source": [
        "# input file has to be moved from regular file system to HDFS file system\n",
        "!hdfs dfs -copyFromLocal EmployeeDept2.txt /tmp"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_VFWQZJvXqn",
        "outputId": "448b2ade-a226-4939-dee7-2817a3e748ab"
      },
      "source": [
        "# https://community.cloudera.com/t5/Community-Articles/Import-CSV-data-into-HBase-using-importtsv/ta-p/244842\n",
        "#hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=,  -Dimporttsv.columns=\"HBASE_ROW_KEY,id,temp:in,temp:out,vibration,pressure:in,pressure:out\" sensor hdfs://sandbox.hortonworks.com:/tmp/hbase.csv\n",
        "!hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=','  -Dimporttsv.columns=\"HBASE_ROW_KEY,Emp:LastName,Emp:FirstName,Emp:Role,Emp:DoJ,Emp:Salary,Emp:Comm,Dept:DeptID,Dept:DeptName,Dept:ManagerID,Dept:Location\" EmpDept2 /tmp/EmployeeDept2.txt"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/content/hbase-2.6.0/lib/client-facing-thirdparty/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
            "2024-06-14 13:42:02,801 INFO  [main] Configuration.deprecation (Configuration.java:logDeprecation(1294)) - hbase.client.pause.cqtbe is deprecated. Instead, use hbase.client.pause.server.overloaded\n",
            "2024-06-14 13:42:02,914 INFO  [main] common.X509Util (X509Util.java:<clinit>(78)) - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\n",
            "2024-06-14 13:42:02,927 WARN  [main] client.ZKConnectionRegistry (ZKConnectionRegistry.java:<init>(83)) - ZKConnectionRegistry is deprecated. See https://hbase.apache.org/book.html#client.rpcconnectionregistry\n",
            "2024-06-14 13:42:02,951 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:zookeeper.version=3.8.4-9316c2a7a97e1666d8f4593f34dd6fc36ecc436c, built on 2024-02-12 22:16 UTC\n",
            "2024-06-14 13:42:02,951 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:host.name=e5efd73160b7\n",
            "2024-06-14 13:42:02,951 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.version=1.8.0_412\n",
            "2024-06-14 13:42:02,951 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.vendor=Private Build\n",
            "2024-06-14 13:42:02,951 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre\n",
            "2024-06-14 13:42:02,951 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.class.path=/content/hbase-2.6.0//conf:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/content/hbase-2.6.0/:/content/hbase-2.6.0//lib/agrona-1.12.0.jar:/content/hbase-2.6.0//lib/aircompressor-0.24.jar:/content/hbase-2.6.0//lib/aopalliance-1.0.jar:/content/hbase-2.6.0//lib/apacheds-i18n-2.0.0-M15.jar:/content/hbase-2.6.0//lib/apacheds-kerberos-codec-2.0.0-M15.jar:/content/hbase-2.6.0//lib/api-asn1-api-1.0.0-M20.jar:/content/hbase-2.6.0//lib/api-i18n-1.0.0-M20.jar:/content/hbase-2.6.0//lib/api-util-1.0.0-M20.jar:/content/hbase-2.6.0//lib/asm-3.1.jar:/content/hbase-2.6.0//lib/avro-1.11.3.jar:/content/hbase-2.6.0//lib/brotli4j-1.11.0.jar:/content/hbase-2.6.0//lib/byte-buddy-1.12.19.jar:/content/hbase-2.6.0//lib/byte-buddy-1.14.9.jar:/content/hbase-2.6.0//lib/byte-buddy-agent-1.12.19.jar:/content/hbase-2.6.0//lib/caffeine-2.8.1.jar:/content/hbase-2.6.0//lib/checker-qual-3.1.0.jar:/content/hbase-2.6.0//lib/commons-cli-1.5.0.jar:/content/hbase-2.6.0//lib/commons-codec-1.15.jar:/content/hbase-2.6.0//lib/commons-collections-3.2.2.jar:/content/hbase-2.6.0//lib/commons-compress-1.21.jar:/content/hbase-2.6.0//lib/commons-configuration-1.6.jar:/content/hbase-2.6.0//lib/commons-crypto-1.1.0.jar:/content/hbase-2.6.0//lib/commons-csv-1.0.jar:/content/hbase-2.6.0//lib/commons-daemon-1.0.13.jar:/content/hbase-2.6.0//lib/commons-digester-1.8.jar:/content/hbase-2.6.0//lib/commons-io-2.11.0.jar:/content/hbase-2.6.0//lib/commons-lang-2.6.jar:/content/hbase-2.6.0//lib/commons-lang3-3.9.jar:/content/hbase-2.6.0//lib/commons-math3-3.6.1.jar:/content/hbase-2.6.0//lib/commons-net-3.1.jar:/content/hbase-2.6.0//lib/curator-client-4.2.0.jar:/content/hbase-2.6.0//lib/curator-framework-4.2.0.jar:/content/hbase-2.6.0//lib/curator-recipes-4.2.0.jar:/content/hbase-2.6.0//lib/disruptor-3.4.4.jar:/content/hbase-2.6.0//lib/ehcache-3.3.1.jar:/content/hbase-2.6.0//lib/error_prone_annotations-2.26.1.jar:/content/hbase-2.6.0//lib/fst-2.50.jar:/content/hbase-2.6.0//lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/content/hbase-2.6.0//lib/gson-2.2.4.jar:/content/hbase-2.6.0//lib/guava-11.0.2.jar:/content/hbase-2.6.0//lib/guice-3.0.jar:/content/hbase-2.6.0//lib/guice-servlet-3.0.jar:/content/hbase-2.6.0//lib/hadoop-annotations-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-auth-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-client-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-common-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-distcp-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-hdfs-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-hdfs-2.10.2-tests.jar:/content/hbase-2.6.0//lib/hadoop-hdfs-client-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-app-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-common-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-core-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-hs-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-jobclient-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-mapreduce-client-shuffle-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-minicluster-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-api-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-client-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-common-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-registry-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-applicationhistoryservice-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-common-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-nodemanager-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-resourcemanager-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-tests-2.10.2-tests.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-timelineservice-2.10.2.jar:/content/hbase-2.6.0//lib/hadoop-yarn-server-web-proxy-2.10.2.jar:/content/hbase-2.6.0//lib/hamcrest-core-1.3.jar:/content/hbase-2.6.0//lib/hbase-annotations-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-annotations-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-asyncfs-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-asyncfs-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-backup-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-client-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-common-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-common-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-compression-aircompressor-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-compression-brotli-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-compression-lz4-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-compression-snappy-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-compression-zstd-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-endpoint-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-external-blockcache-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-hadoop2-compat-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-hadoop2-compat-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-hadoop-compat-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-hadoop-compat-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-hbtop-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-http-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-it-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-it-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-logging-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-mapreduce-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-mapreduce-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-metrics-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-metrics-api-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-openssl-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-procedure-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-procedure-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-protocol-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-protocol-shaded-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-replication-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-resource-bundle-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-rest-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-rsgroup-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-rsgroup-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-server-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-server-2.6.0-tests.jar:/content/hbase-2.6.0//lib/hbase-shaded-gson-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-jackson-jaxrs-json-provider-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-jersey-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-jetty-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-miscellaneous-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-netty-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-netty-tcnative-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shaded-protobuf-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-shell-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-testing-util-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-thrift-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-unsafe-4.1.7.jar:/content/hbase-2.6.0//lib/hbase-zookeeper-2.6.0.jar:/content/hbase-2.6.0//lib/hbase-zookeeper-2.6.0-tests.jar:/content/hbase-2.6.0//lib/HikariCP-java7-2.4.12.jar:/content/hbase-2.6.0//lib/httpclient-4.5.13.jar:/content/hbase-2.6.0//lib/httpcore-4.4.13.jar:/content/hbase-2.6.0//lib/jackson-annotations-2.17.0.jar:/content/hbase-2.6.0//lib/jackson-core-2.17.0.jar:/content/hbase-2.6.0//lib/jackson-core-asl-1.9.13.jar:/content/hbase-2.6.0//lib/jackson-databind-2.17.0.jar:/content/hbase-2.6.0//lib/jackson-mapper-asl-1.9.13.jar:/content/hbase-2.6.0//lib/jackson-module-jaxb-annotations-2.17.0.jar:/content/hbase-2.6.0//lib/jakarta.inject-2.6.1.jar:/content/hbase-2.6.0//lib/jakarta.validation-api-2.0.2.jar:/content/hbase-2.6.0//lib/jamon-runtime-2.4.1.jar:/content/hbase-2.6.0//lib/javassist-3.30.2-GA.jar:/content/hbase-2.6.0//lib/java-util-1.9.0.jar:/content/hbase-2.6.0//lib/javax.activation-api-1.2.0.jar:/content/hbase-2.6.0//lib/javax.annotation-api-1.2.jar:/content/hbase-2.6.0//lib/javax.el-3.0.1-b08.jar:/content/hbase-2.6.0//lib/java-xmlbuilder-0.4.jar:/content/hbase-2.6.0//lib/javax.servlet-api-3.1.0.jar:/content/hbase-2.6.0//lib/javax.servlet.jsp-2.3.2.jar:/content/hbase-2.6.0//lib/javax.servlet.jsp-api-2.3.1.jar:/content/hbase-2.6.0//lib/jaxb-api-2.3.1.jar:/content/hbase-2.6.0//lib/jaxb-impl-2.2.3-1.jar:/content/hbase-2.6.0//lib/jcip-annotations-1.0-1.jar:/content/hbase-2.6.0//lib/jcodings-1.0.58.jar:/content/hbase-2.6.0//lib/jets3t-0.9.0.jar:/content/hbase-2.6.0//lib/jettison-1.5.4.jar:/content/hbase-2.6.0//lib/jetty-6.1.26.jar:/content/hbase-2.6.0//lib/jetty-sslengine-6.1.26.jar:/content/hbase-2.6.0//lib/jetty-util-6.1.26.jar:/content/hbase-2.6.0//lib/joni-2.2.1.jar:/content/hbase-2.6.0//lib/jsch-0.1.55.jar:/content/hbase-2.6.0//lib/json-io-2.5.1.jar:/content/hbase-2.6.0//lib/jsr311-api-1.1.1.jar:/content/hbase-2.6.0//lib/junit-4.13.2.jar:/content/hbase-2.6.0//lib/leveldbjni-all-1.8.jar:/content/hbase-2.6.0//lib/libthrift-0.14.1.jar:/content/hbase-2.6.0//lib/lz4-java-1.8.0.jar:/content/hbase-2.6.0//lib/metrics-core-3.2.6.jar:/content/hbase-2.6.0//lib/mockito-core-4.11.0.jar:/content/hbase-2.6.0//lib/mssql-jdbc-6.2.1.jre7.jar:/content/hbase-2.6.0//lib/native-linux-aarch64-1.11.0.jar:/content/hbase-2.6.0//lib/netty-3.10.6.Final.jar:/content/hbase-2.6.0//lib/netty-all-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-buffer-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-dns-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-haproxy-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-http2-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-http-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-memcache-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-mqtt-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-redis-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-smtp-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-socks-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-stomp-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-codec-xml-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-common-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-handler-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-handler-proxy-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-handler-ssl-ocsp-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-resolver-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-resolver-dns-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-resolver-dns-classes-macos-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-resolver-dns-native-macos-4.1.108.Final-osx-aarch_64.jar:/content/hbase-2.6.0//lib/netty-resolver-dns-native-macos-4.1.108.Final-osx-x86_64.jar:/content/hbase-2.6.0//lib/netty-transport-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-classes-epoll-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-classes-kqueue-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-native-epoll-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-native-epoll-4.1.108.Final-linux-aarch_64.jar:/content/hbase-2.6.0//lib/netty-transport-native-epoll-4.1.108.Final-linux-riscv64.jar:/content/hbase-2.6.0//lib/netty-transport-native-epoll-4.1.108.Final-linux-x86_64.jar:/content/hbase-2.6.0//lib/netty-transport-native-kqueue-4.1.108.Final-osx-aarch_64.jar:/content/hbase-2.6.0//lib/netty-transport-native-kqueue-4.1.108.Final-osx-x86_64.jar:/content/hbase-2.6.0//lib/netty-transport-native-unix-common-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-rxtx-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-sctp-4.1.108.Final.jar:/content/hbase-2.6.0//lib/netty-transport-udt-4.1.108.Final.jar:/content/hbase-2.6.0//lib/nimbus-jose-jwt-7.9.jar:/content/hbase-2.6.0//lib/objenesis-3.3.jar:/content/hbase-2.6.0//lib/okhttp-2.7.5.jar:/content/hbase-2.6.0//lib/okio-1.6.0.jar:/content/hbase-2.6.0//lib/opentelemetry-api-1.15.0.jar:/content/hbase-2.6.0//lib/opentelemetry-context-1.15.0.jar:/content/hbase-2.6.0//lib/opentelemetry-semconv-1.15.0-alpha.jar:/content/hbase-2.6.0//lib/protobuf-java-2.5.0.jar:/content/hbase-2.6.0//lib/service-1.11.0.jar:/content/hbase-2.6.0//lib/snappy-java-1.1.10.4.jar:/content/hbase-2.6.0//lib/spymemcached-2.12.2.jar:/content/hbase-2.6.0//lib/stax2-api-4.2.1.jar:/content/hbase-2.6.0//lib/woodstox-core-5.3.0.jar:/content/hbase-2.6.0//lib/xmlenc-0.52.jar:/content/hbase-2.6.0//lib/zookeeper-3.8.4.jar:/content/hbase-2.6.0//lib/zookeeper-jute-3.8.4.jar:/content/hbase-2.6.0//lib/zstd-jni-1.5.5-2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/audience-annotations-0.13.0.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/commons-logging-1.2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/htrace-core4-4.1.0-incubating.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/jcl-over-slf4j-1.7.33.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/jul-to-slf4j-1.7.33.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/opentelemetry-api-1.15.0.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/opentelemetry-context-1.15.0.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/opentelemetry-semconv-1.15.0-alpha.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/slf4j-api-1.7.33.jar:/usr/local/hadoop-3.4.0//etc/hadoop:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-haproxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-http-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-math3-3.6.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/hadoop-auth-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/curator-framework-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-text-1.10.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-xml-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jackson-core-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-handler-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-util-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-simplekdc-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-redis-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jersey-server-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/gson-2.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-cli-1.5.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jersey-json-1.20.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-sctp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-api-1.7.36.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-handler-proxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-core-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/hadoop-annotations-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-admin-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-xml-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-stomp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/dnsjava-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jersey-core-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-crypto-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-compress-1.24.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/zookeeper-3.8.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-server-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-xdr-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-http-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-net-3.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/zookeeper-jute-3.8.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-all-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-udt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-util-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-server-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-io-2.14.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/curator-client-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-socks-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-mqtt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-webapp-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jettison-1.5.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-logging-1.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/nimbus-jose-jwt-9.31.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/bcprov-jdk15on-1.70.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/token-provider-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-config-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jline-3.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-http2-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/hadoop-shaded-guava-1.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-identity-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-security-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-pkix-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-buffer-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-io-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-util-ajax-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-util-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-client-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/avro-1.9.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/audience-annotations-0.12.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerb-common-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-smtp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/snappy-java-1.1.10.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/kerby-asn1-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-resolver-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/reload4j-1.2.22.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-transport-rxtx-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/netty-codec-memcache-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/lib/jetty-servlet-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-common-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-common-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-registry-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-nfs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/common/hadoop-kms-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-http-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-math3-3.6.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/HikariCP-4.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/hadoop-auth-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-xml-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-handler-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-util-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_21-1.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-simplekdc-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-redis-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/gson-2.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-cli-1.5.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jersey-json-1.20.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-sctp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-handler-proxy-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-core-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/hadoop-annotations-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-admin-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-xml-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-stomp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/dnsjava-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-crypto-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-compress-1.24.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/zookeeper-3.8.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-server-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-xdr-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-http-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/zookeeper-jute-3.8.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-all-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-udt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-util-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-server-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-io-2.14.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-socks-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-webapp-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jettison-1.5.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-logging-1.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/nimbus-jose-jwt-9.31.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/token-provider-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-config-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jline-3.9.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-http2-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/hadoop-shaded-guava-1.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-identity-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-security-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-pkix-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-buffer-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-io-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-util-ajax-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-util-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-client-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/avro-1.9.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/audience-annotations-0.12.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerb-common-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-smtp-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/snappy-java-1.1.10.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/kerby-asn1-2.0.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-resolver-dns-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-common-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/netty-codec-memcache-4.1.100.Final.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/commons-codec-1.15.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/lib/jetty-servlet-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-httpfs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-client-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-nfs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-rbf-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-native-client-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/hdfs/hadoop-hdfs-client-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0-tests.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jsonschema2pojo-core-1.0.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jetty-jndi-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-api-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/snakeyaml-2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/codemodel-2.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/guice-servlet-4.2.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/asm-tree-9.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/bcutil-jdk15on-1.70.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-common-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/asm-commons-9.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jetty-plus-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jetty-client-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-server-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/guice-4.2.3.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/objenesis-2.6.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jetty-annotations-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-servlet-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/bcpkix-jdk15on-1.70.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jna-5.2.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/websocket-client-9.4.53.v20231009.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-api-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-services-api-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-services-core-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-globalpolicygenerator-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-common-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-tests-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-registry-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-client-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-router-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-common-3.4.0.jar:/usr/local/hadoop-3.4.0//share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.4.0.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/log4j-1.2-api-2.17.2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/log4j-api-2.17.2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/log4j-core-2.17.2.jar:/content/hbase-2.6.0//lib/client-facing-thirdparty/log4j-slf4j-impl-2.17.2.jar\n",
            "2024-06-14 13:42:03,142 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.library.path=/usr/local/hadoop-3.4.0//lib/native\n",
            "2024-06-14 13:42:03,143 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.io.tmpdir=/tmp\n",
            "2024-06-14 13:42:03,143 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:java.compiler=<NA>\n",
            "2024-06-14 13:42:03,144 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.name=Linux\n",
            "2024-06-14 13:42:03,144 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.arch=amd64\n",
            "2024-06-14 13:42:03,144 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.version=6.1.85+\n",
            "2024-06-14 13:42:03,144 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:user.name=root\n",
            "2024-06-14 13:42:03,144 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:user.home=/root\n",
            "2024-06-14 13:42:03,144 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:user.dir=/content\n",
            "2024-06-14 13:42:03,144 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.memory.free=157MB\n",
            "2024-06-14 13:42:03,144 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.memory.max=3229MB\n",
            "2024-06-14 13:42:03,144 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (Environment.java:logEnv(98)) - Client environment:os.memory.total=197MB\n",
            "2024-06-14 13:42:03,151 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (ZooKeeper.java:<init>(637)) - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$20/1022300713@521e3a03\n",
            "2024-06-14 13:42:03,157 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ClientCnxnSocket (ClientCnxnSocket.java:initProperties(239)) - jute.maxbuffer value is 1048575 Bytes\n",
            "2024-06-14 13:42:03,167 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ClientCnxn (ClientCnxn.java:initRequestTimeout(1747)) - zookeeper.request.timeout value is 0. feature enabled=false\n",
            "2024-06-14 13:42:03,186 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1177)) - Opening socket connection to server localhost/127.0.0.1:2181.\n",
            "2024-06-14 13:42:03,187 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1179)) - SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
            "2024-06-14 13:42:03,196 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(1013)) - Socket connection established, initiating session, client: /127.0.0.1:35348, server: localhost/127.0.0.1:2181\n",
            "2024-06-14 13:42:03,205 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1453)) - Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x100001115770007, negotiated timeout = 40000\n",
            "2024-06-14 13:42:05,686 INFO  [main] client.ConnectionImplementation (ConnectionImplementation.java:closeMasterService(2127)) - Closing master protocol: MasterService\n",
            "2024-06-14 13:42:05,720 INFO  [main] Configuration.deprecation (Configuration.java:logDeprecation(1294)) - session.id is deprecated. Instead, use dfs.metrics.session-id\n",
            "2024-06-14 13:42:05,722 INFO  [main] jvm.JvmMetrics (JvmMetrics.java:init(79)) - Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
            "2024-06-14 13:42:05,777 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x5e77f0f4] zookeeper.ZooKeeper (ZooKeeper.java:<init>(637)) - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$20/1022300713@521e3a03\n",
            "2024-06-14 13:42:05,779 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x5e77f0f4] zookeeper.ClientCnxnSocket (ClientCnxnSocket.java:initProperties(239)) - jute.maxbuffer value is 1048575 Bytes\n",
            "2024-06-14 13:42:05,780 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x5e77f0f4] zookeeper.ClientCnxn (ClientCnxn.java:initRequestTimeout(1747)) - zookeeper.request.timeout value is 0. feature enabled=false\n",
            "2024-06-14 13:42:05,786 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x5e77f0f4-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1177)) - Opening socket connection to server localhost/127.0.0.1:2181.\n",
            "2024-06-14 13:42:05,787 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x5e77f0f4-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1179)) - SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
            "2024-06-14 13:42:05,788 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x5e77f0f4-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(1013)) - Socket connection established, initiating session, client: /127.0.0.1:35352, server: localhost/127.0.0.1:2181\n",
            "2024-06-14 13:42:05,792 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x5e77f0f4-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1453)) - Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x100001115770008, negotiated timeout = 40000\n",
            "2024-06-14 13:42:05,797 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e] zookeeper.ZooKeeper (ZooKeeper.java:close(1232)) - Session: 0x100001115770007 closed\n",
            "2024-06-14 13:42:05,797 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x49c7b90e-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(569)) - EventThread shut down for session: 0x100001115770007\n",
            "2024-06-14 13:42:05,975 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x5e77f0f4] zookeeper.ZooKeeper (ZooKeeper.java:close(1232)) - Session: 0x100001115770008 closed\n",
            "2024-06-14 13:42:05,975 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x5e77f0f4-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(569)) - EventThread shut down for session: 0x100001115770008\n",
            "2024-06-14 13:42:06,151 INFO  [main] input.FileInputFormat (FileInputFormat.java:listStatus(289)) - Total input files to process : 1\n",
            "2024-06-14 13:42:06,206 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(204)) - number of splits:1\n",
            "2024-06-14 13:42:06,534 INFO  [main] mapreduce.JobSubmitter (JobSubmitter.java:printTokens(300)) - Submitting tokens for job: job_local2020907212_0001\n",
            "2024-06-14 13:42:07,356 INFO  [main] mapred.LocalDistributedCacheManager (LocalDistributedCacheManager.java:symlink(200)) - Creating symlink: /tmp/hadoop-root/mapred/local/1718372526819/libjars <- /content/libjars/*\n",
            "2024-06-14 13:42:07,379 WARN  [main] fs.FileUtil (FileUtil.java:symLink(888)) - Command 'ln -s /tmp/hadoop-root/mapred/local/1718372526819/libjars /content/libjars/*' failed 1 with: ln: failed to create symbolic link '/content/libjars/*': No such file or directory\n",
            "\n",
            "2024-06-14 13:42:07,379 WARN  [main] mapred.LocalDistributedCacheManager (LocalDistributedCacheManager.java:symlink(202)) - Failed to create symlink: /tmp/hadoop-root/mapred/local/1718372526819/libjars <- /content/libjars/*\n",
            "2024-06-14 13:42:07,379 INFO  [main] mapred.LocalDistributedCacheManager (LocalDistributedCacheManager.java:setup(164)) - Localized file:/tmp/hadoop-root/mapred/staging/root2020907212/.staging/job_local2020907212_0001/libjars as file:/tmp/hadoop-root/mapred/local/1718372526819/libjars\n",
            "2024-06-14 13:42:07,606 INFO  [main] mapreduce.Job (Job.java:submit(1577)) - The url to track the job: http://localhost:8080/\n",
            "2024-06-14 13:42:07,607 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1622)) - Running job: job_local2020907212_0001\n",
            "2024-06-14 13:42:07,614 INFO  [Thread-7] mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(500)) - OutputCommitter set in config null\n",
            "2024-06-14 13:42:07,695 INFO  [Thread-7] mapred.LocalJobRunner (LocalJobRunner.java:createOutputCommitter(518)) - OutputCommitter is org.apache.hadoop.hbase.mapreduce.TableOutputCommitter\n",
            "2024-06-14 13:42:07,733 INFO  [Thread-7] mapred.LocalJobRunner (LocalJobRunner.java:runTasks(477)) - Waiting for map tasks\n",
            "2024-06-14 13:42:07,736 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner (LocalJobRunner.java:run(251)) - Starting task: attempt_local2020907212_0001_m_000000_0\n",
            "2024-06-14 13:42:07,914 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task (Task.java:initialize(620)) -  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-06-14 13:42:07,944 INFO  [LocalJobRunner Map Task Executor #0] mapred.MapTask (MapTask.java:runNewMapper(763)) - Processing split: file:/tmp/EmployeeDept2.txt:0+955\n",
            "2024-06-14 13:42:07,975 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x68bfd596] zookeeper.ZooKeeper (ZooKeeper.java:<init>(637)) - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$20/1022300713@521e3a03\n",
            "2024-06-14 13:42:07,976 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x68bfd596] zookeeper.ClientCnxnSocket (ClientCnxnSocket.java:initProperties(239)) - jute.maxbuffer value is 1048575 Bytes\n",
            "2024-06-14 13:42:07,980 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x68bfd596] zookeeper.ClientCnxn (ClientCnxn.java:initRequestTimeout(1747)) - zookeeper.request.timeout value is 0. feature enabled=false\n",
            "2024-06-14 13:42:07,994 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x68bfd596-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1177)) - Opening socket connection to server localhost/127.0.0.1:2181.\n",
            "2024-06-14 13:42:07,995 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x68bfd596-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1179)) - SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
            "2024-06-14 13:42:07,997 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x68bfd596-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(1013)) - Socket connection established, initiating session, client: /127.0.0.1:52812, server: localhost/127.0.0.1:2181\n",
            "2024-06-14 13:42:08,002 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x68bfd596-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1453)) - Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x100001115770009, negotiated timeout = 40000\n",
            "2024-06-14 13:42:08,013 INFO  [LocalJobRunner Map Task Executor #0] mapreduce.TableOutputFormat (TableOutputFormat.java:<init>(100)) - Created table instance for EmpDept2\n",
            "2024-06-14 13:42:08,060 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x434b283c] zookeeper.ZooKeeper (ZooKeeper.java:<init>(637)) - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$20/1022300713@521e3a03\n",
            "2024-06-14 13:42:08,061 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x434b283c] zookeeper.ClientCnxnSocket (ClientCnxnSocket.java:initProperties(239)) - jute.maxbuffer value is 1048575 Bytes\n",
            "2024-06-14 13:42:08,069 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x434b283c] zookeeper.ClientCnxn (ClientCnxn.java:initRequestTimeout(1747)) - zookeeper.request.timeout value is 0. feature enabled=false\n",
            "2024-06-14 13:42:08,080 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x434b283c-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1177)) - Opening socket connection to server localhost/127.0.0.1:2181.\n",
            "2024-06-14 13:42:08,080 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x434b283c-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1179)) - SASL config status: Will not attempt to authenticate using SASL (unknown error)\n",
            "2024-06-14 13:42:08,084 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x434b283c-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(1013)) - Socket connection established, initiating session, client: /127.0.0.1:52824, server: localhost/127.0.0.1:2181\n",
            "2024-06-14 13:42:08,097 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x434b283c-SendThread(127.0.0.1:2181)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1453)) - Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x10000111577000a, negotiated timeout = 40000\n",
            "2024-06-14 13:42:08,301 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(620)) - \n",
            "2024-06-14 13:42:08,386 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x434b283c] zookeeper.ZooKeeper (ZooKeeper.java:close(1232)) - Session: 0x10000111577000a closed\n",
            "2024-06-14 13:42:08,387 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x434b283c-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(569)) - EventThread shut down for session: 0x10000111577000a\n",
            "2024-06-14 13:42:08,627 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1643)) - Job job_local2020907212_0001 running in uber mode : false\n",
            "2024-06-14 13:42:08,629 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1650)) -  map 0% reduce 0%\n",
            "2024-06-14 13:42:08,639 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task (Task.java:done(1105)) - Task:attempt_local2020907212_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-06-14 13:42:08,798 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner (LocalJobRunner.java:statusUpdate(620)) - map\n",
            "2024-06-14 13:42:08,799 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task (Task.java:sendDone(1243)) - Task 'attempt_local2020907212_0001_m_000000_0' done.\n",
            "2024-06-14 13:42:08,824 INFO  [LocalJobRunner Map Task Executor #0] mapred.Task (Task.java:done(1139)) - Final Counters for attempt_local2020907212_0001_m_000000_0: Counters: 19\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=447733\n",
            "\t\tFILE: Number of bytes written=1034040\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=12\n",
            "\t\tMap output records=12\n",
            "\t\tInput split bytes=92\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=65\n",
            "\t\tCPU time spent (ms)=760\n",
            "\t\tPhysical memory (bytes) snapshot=289226752\n",
            "\t\tVirtual memory (bytes) snapshot=5171093504\n",
            "\t\tTotal committed heap usage (bytes)=206831616\n",
            "\tImportTsv\n",
            "\t\tBad Lines=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=979\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=0\n",
            "2024-06-14 13:42:08,824 INFO  [LocalJobRunner Map Task Executor #0] mapred.LocalJobRunner (LocalJobRunner.java:run(276)) - Finishing task: attempt_local2020907212_0001_m_000000_0\n",
            "2024-06-14 13:42:08,825 INFO  [Thread-7] mapred.LocalJobRunner (LocalJobRunner.java:runTasks(485)) - map task executor complete.\n",
            "2024-06-14 13:42:08,838 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x68bfd596] zookeeper.ZooKeeper (ZooKeeper.java:close(1232)) - Session: 0x100001115770009 closed\n",
            "2024-06-14 13:42:08,838 INFO  [ReadOnlyZKClient-127.0.0.1:2181@0x68bfd596-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(569)) - EventThread shut down for session: 0x100001115770009\n",
            "2024-06-14 13:42:09,639 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1650)) -  map 100% reduce 0%\n",
            "2024-06-14 13:42:09,639 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1661)) - Job job_local2020907212_0001 completed successfully\n",
            "2024-06-14 13:42:09,646 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1668)) - Counters: 19\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=447733\n",
            "\t\tFILE: Number of bytes written=1034040\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=12\n",
            "\t\tMap output records=12\n",
            "\t\tInput split bytes=92\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=65\n",
            "\t\tCPU time spent (ms)=760\n",
            "\t\tPhysical memory (bytes) snapshot=289226752\n",
            "\t\tVirtual memory (bytes) snapshot=5171093504\n",
            "\t\tTotal committed heap usage (bytes)=206831616\n",
            "\tImportTsv\n",
            "\t\tBad Lines=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=979\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-H5Zh5UwRzd"
      },
      "source": [
        "tEmpDept2 = kxn.table('EmpDept2')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4tQppiqwMTk",
        "outputId": "09b08dce-6e41-40e0-af42-5dbffc7df151"
      },
      "source": [
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "for key, data in tEmpDept2.scan():\n",
        "    print(key, data)\n",
        "kxn.close()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'123980' {b'Dept:DeptID': b'40', b'Dept:DeptName': b'Production', b'Dept:Location': b'Bombay', b'Dept:ManagerID': b'349870', b'Emp:Comm': b'0.02', b'Emp:DoJ': b'2004-10-09', b'Emp:FirstName': b'Mahender', b'Emp:LastName': b'Dhoni', b'Emp:Role': b'Clerk', b'Emp:Salary': b'9000'}\n",
            "b'223112' {b'Dept:DeptID': b'30', b'Dept:DeptName': b'Accounts', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'822134', b'Emp:Comm': b'0.04', b'Emp:DoJ': b'2001-11-19', b'Emp:FirstName': b'Sania', b'Emp:LastName': b'Mirza', b'Emp:Role': b'Cus_Rep', b'Emp:Salary': b'25000'}\n",
            "b'239456' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.07', b'Emp:DoJ': b'2004-01-03', b'Emp:FirstName': b'Shahrukh', b'Emp:LastName': b'Khan', b'Emp:Role': b'Manager', b'Emp:Salary': b'30000'}\n",
            "b'299034' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.11', b'Emp:DoJ': b'2002-10-10', b'Emp:FirstName': b'Rekha', b'Emp:LastName': b'Ganesan', b'Emp:Role': b'Director', b'Emp:Salary': b'60000'}\n",
            "b'349870' {b'Dept:DeptID': b'40', b'Dept:DeptName': b'Production', b'Dept:Location': b'Bombay', b'Dept:ManagerID': b'349870', b'Emp:Comm': b'0.06', b'Emp:DoJ': b'2005-05-04', b'Emp:FirstName': b'Rani', b'Emp:LastName': b'Mukherjee', b'Emp:Role': b'Manager', b'Emp:Salary': b'25000'}\n",
            "b'546223' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.09', b'Emp:DoJ': b'2005-12-04', b'Emp:FirstName': b'Narayan', b'Emp:LastName': b'Karthikeyan', b'Emp:Role': b'Secretary', b'Emp:Salary': b'40000'}\n",
            "b'742866' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.1', b'Emp:DoJ': b'2003-03-10', b'Emp:FirstName': b'Amitabh', b'Emp:LastName': b'Bacchan', b'Emp:Role': b'Executive', b'Emp:Salary': b'50000'}\n",
            "b'822134' {b'Dept:DeptID': b'30', b'Dept:DeptName': b'Accounts', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'822134', b'Emp:Comm': b'0.08', b'Emp:DoJ': b'2000-06-04', b'Emp:FirstName': b'Rahul', b'Emp:LastName': b'Dravid', b'Emp:Role': b'Sr Manager', b'Emp:Salary': b'40000'}\n",
            "b'865477' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.02', b'Emp:DoJ': b'2002-04-04', b'Emp:FirstName': b'Madhuri', b'Emp:LastName': b'Dikshit', b'Emp:Role': b'Clerk', b'Emp:Salary': b'10000'}\n",
            "b'897889' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.05', b'Emp:DoJ': b'2005-01-02', b'Emp:FirstName': b'Virender', b'Emp:LastName': b'Sehwag', b'Emp:Role': b'Cus_Rep', b'Emp:Salary': b'15000'}\n",
            "b'989007' {b'Dept:DeptID': b'40', b'Dept:DeptName': b'Production', b'Dept:Location': b'Bombay', b'Dept:ManagerID': b'349870', b'Emp:Comm': b'0.03', b'Emp:DoJ': b'2002-01-01', b'Emp:FirstName': b'Sourav', b'Emp:LastName': b'Ganguly', b'Emp:Role': b'Cus_Rep', b'Emp:Salary': b'20000'}\n",
            "b'997445' {b'Dept:DeptID': b'30', b'Dept:DeptName': b'Accounts', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'822134', b'Emp:Comm': b'0.02', b'Emp:DoJ': b'2001-07-01', b'Emp:FirstName': b'Jagmohan', b'Emp:LastName': b'Dalmia', b'Emp:Role': b'Clerk', b'Emp:Salary': b'12000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGIY0y-OO1rV"
      },
      "source": [
        "#Data Retrieval <br>\n",
        "https://happybase.readthedocs.io/en/latest/user.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtmYXOCEO-EJ",
        "outputId": "97eac052-a9eb-43f4-d074-e0771239f6a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Retreive One Row\n",
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "prow = tEmpDept2.row(b'123980')\n",
        "print(prow)   # prints the value of cf1:col1"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{b'Dept:DeptID': b'40', b'Dept:DeptName': b'Production', b'Dept:Location': b'Bombay', b'Dept:ManagerID': b'349870', b'Emp:Comm': b'0.02', b'Emp:DoJ': b'2004-10-09', b'Emp:FirstName': b'Mahender', b'Emp:LastName': b'Dhoni', b'Emp:Role': b'Clerk', b'Emp:Salary': b'9000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azcfxg2YP9AH",
        "outputId": "7abbbcda-778d-496b-93f1-8fef94059040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Retreive More than one Row\n",
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "listOfrows = [b'742866', b'897889']\n",
        "rows = tEmpDept2.rows(listOfrows)\n",
        "for key, data in rows:\n",
        "    print(key, data)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'742866' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.1', b'Emp:DoJ': b'2003-03-10', b'Emp:FirstName': b'Amitabh', b'Emp:LastName': b'Bacchan', b'Emp:Role': b'Executive', b'Emp:Salary': b'50000'}\n",
            "b'897889' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.05', b'Emp:DoJ': b'2005-01-02', b'Emp:FirstName': b'Virender', b'Emp:LastName': b'Sehwag', b'Emp:Role': b'Cus_Rep', b'Emp:Salary': b'15000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69-NH_HcQ84G",
        "outputId": "b9980432-f714-4587-ee2f-1b9f8c5b96de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Retreive More than one Row\n",
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "rowsDict = dict(tEmpDept2.rows([b'742866', b'897889']))\n",
        "for k,v in rowsDict.items():\n",
        "    print(k,v)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'742866' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.1', b'Emp:DoJ': b'2003-03-10', b'Emp:FirstName': b'Amitabh', b'Emp:LastName': b'Bacchan', b'Emp:Role': b'Executive', b'Emp:Salary': b'50000'}\n",
            "b'897889' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.05', b'Emp:DoJ': b'2005-01-02', b'Emp:FirstName': b'Virender', b'Emp:LastName': b'Sehwag', b'Emp:Role': b'Cus_Rep', b'Emp:Salary': b'15000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25jvHL81TwcN",
        "outputId": "f35c50b2-8171-4731-a395-e6a51ca0af22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Retreive More than one Row, but only one column family\n",
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "rows = tEmpDept2.rows([b'742866', b'897889'],columns=[b'Emp'])\n",
        "for key, data in rows:\n",
        "    print(key, data)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'742866' {b'Emp:Comm': b'0.1', b'Emp:DoJ': b'2003-03-10', b'Emp:FirstName': b'Amitabh', b'Emp:LastName': b'Bacchan', b'Emp:Role': b'Executive', b'Emp:Salary': b'50000'}\n",
            "b'897889' {b'Emp:Comm': b'0.05', b'Emp:DoJ': b'2005-01-02', b'Emp:FirstName': b'Virender', b'Emp:LastName': b'Sehwag', b'Emp:Role': b'Cus_Rep', b'Emp:Salary': b'15000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xT0svpwPUOUo",
        "outputId": "c47304ec-1301-4102-9c3d-44e813ce5b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Retreive More than one Row, but only one column family\n",
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "rows = tEmpDept2.rows([b'742866', b'897889'],columns=[b'Dept'])\n",
        "for key, data in rows:\n",
        "    print(key, data)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'742866' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034'}\n",
            "b'897889' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxjJtKhrUWwM",
        "outputId": "ca4b9717-a4e6-4971-f590-cc640e8bc5fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Retreive More than one Row, but only one column family\n",
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "rows = tEmpDept2.rows([b'742866', b'897889'],columns=[b'Emp:LastName',b'Dept:Location'])\n",
        "for key, data in rows:\n",
        "    print(key, data)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'742866' {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Bacchan'}\n",
            "b'897889' {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Sehwag'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQs2EH6OU74d",
        "outputId": "82306a29-77a5-4541-f757-7ed4405fbff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "for key, data in tEmpDept2.scan():\n",
        "    print(key, data)\n",
        "kxn.close()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'123980' {b'Dept:DeptID': b'40', b'Dept:DeptName': b'Production', b'Dept:Location': b'Bombay', b'Dept:ManagerID': b'349870', b'Emp:Comm': b'0.02', b'Emp:DoJ': b'2004-10-09', b'Emp:FirstName': b'Mahender', b'Emp:LastName': b'Dhoni', b'Emp:Role': b'Clerk', b'Emp:Salary': b'9000'}\n",
            "b'223112' {b'Dept:DeptID': b'30', b'Dept:DeptName': b'Accounts', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'822134', b'Emp:Comm': b'0.04', b'Emp:DoJ': b'2001-11-19', b'Emp:FirstName': b'Sania', b'Emp:LastName': b'Mirza', b'Emp:Role': b'Cus_Rep', b'Emp:Salary': b'25000'}\n",
            "b'239456' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.07', b'Emp:DoJ': b'2004-01-03', b'Emp:FirstName': b'Shahrukh', b'Emp:LastName': b'Khan', b'Emp:Role': b'Manager', b'Emp:Salary': b'30000'}\n",
            "b'299034' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.11', b'Emp:DoJ': b'2002-10-10', b'Emp:FirstName': b'Rekha', b'Emp:LastName': b'Ganesan', b'Emp:Role': b'Director', b'Emp:Salary': b'60000'}\n",
            "b'349870' {b'Dept:DeptID': b'40', b'Dept:DeptName': b'Production', b'Dept:Location': b'Bombay', b'Dept:ManagerID': b'349870', b'Emp:Comm': b'0.06', b'Emp:DoJ': b'2005-05-04', b'Emp:FirstName': b'Rani', b'Emp:LastName': b'Mukherjee', b'Emp:Role': b'Manager', b'Emp:Salary': b'25000'}\n",
            "b'546223' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.09', b'Emp:DoJ': b'2005-12-04', b'Emp:FirstName': b'Narayan', b'Emp:LastName': b'Karthikeyan', b'Emp:Role': b'Secretary', b'Emp:Salary': b'40000'}\n",
            "b'742866' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.1', b'Emp:DoJ': b'2003-03-10', b'Emp:FirstName': b'Amitabh', b'Emp:LastName': b'Bacchan', b'Emp:Role': b'Executive', b'Emp:Salary': b'50000'}\n",
            "b'822134' {b'Dept:DeptID': b'30', b'Dept:DeptName': b'Accounts', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'822134', b'Emp:Comm': b'0.08', b'Emp:DoJ': b'2000-06-04', b'Emp:FirstName': b'Rahul', b'Emp:LastName': b'Dravid', b'Emp:Role': b'Sr Manager', b'Emp:Salary': b'40000'}\n",
            "b'865477' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.02', b'Emp:DoJ': b'2002-04-04', b'Emp:FirstName': b'Madhuri', b'Emp:LastName': b'Dikshit', b'Emp:Role': b'Clerk', b'Emp:Salary': b'10000'}\n",
            "b'897889' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.05', b'Emp:DoJ': b'2005-01-02', b'Emp:FirstName': b'Virender', b'Emp:LastName': b'Sehwag', b'Emp:Role': b'Cus_Rep', b'Emp:Salary': b'15000'}\n",
            "b'989007' {b'Dept:DeptID': b'40', b'Dept:DeptName': b'Production', b'Dept:Location': b'Bombay', b'Dept:ManagerID': b'349870', b'Emp:Comm': b'0.03', b'Emp:DoJ': b'2002-01-01', b'Emp:FirstName': b'Sourav', b'Emp:LastName': b'Ganguly', b'Emp:Role': b'Cus_Rep', b'Emp:Salary': b'20000'}\n",
            "b'997445' {b'Dept:DeptID': b'30', b'Dept:DeptName': b'Accounts', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'822134', b'Emp:Comm': b'0.02', b'Emp:DoJ': b'2001-07-01', b'Emp:FirstName': b'Jagmohan', b'Emp:LastName': b'Dalmia', b'Emp:Role': b'Clerk', b'Emp:Salary': b'12000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km2Qhg60VEtX",
        "outputId": "29459ead-2a3c-42b4-f9e5-50cad5a1b3c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "for key, data in tEmpDept2.scan(row_start=b'230000', row_stop=b'870000'):\n",
        "    print(key, data)\n",
        "kxn.close()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'239456' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.07', b'Emp:DoJ': b'2004-01-03', b'Emp:FirstName': b'Shahrukh', b'Emp:LastName': b'Khan', b'Emp:Role': b'Manager', b'Emp:Salary': b'30000'}\n",
            "b'299034' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.11', b'Emp:DoJ': b'2002-10-10', b'Emp:FirstName': b'Rekha', b'Emp:LastName': b'Ganesan', b'Emp:Role': b'Director', b'Emp:Salary': b'60000'}\n",
            "b'349870' {b'Dept:DeptID': b'40', b'Dept:DeptName': b'Production', b'Dept:Location': b'Bombay', b'Dept:ManagerID': b'349870', b'Emp:Comm': b'0.06', b'Emp:DoJ': b'2005-05-04', b'Emp:FirstName': b'Rani', b'Emp:LastName': b'Mukherjee', b'Emp:Role': b'Manager', b'Emp:Salary': b'25000'}\n",
            "b'546223' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.09', b'Emp:DoJ': b'2005-12-04', b'Emp:FirstName': b'Narayan', b'Emp:LastName': b'Karthikeyan', b'Emp:Role': b'Secretary', b'Emp:Salary': b'40000'}\n",
            "b'742866' {b'Dept:DeptID': b'10', b'Dept:DeptName': b'Corporate', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'299034', b'Emp:Comm': b'0.1', b'Emp:DoJ': b'2003-03-10', b'Emp:FirstName': b'Amitabh', b'Emp:LastName': b'Bacchan', b'Emp:Role': b'Executive', b'Emp:Salary': b'50000'}\n",
            "b'822134' {b'Dept:DeptID': b'30', b'Dept:DeptName': b'Accounts', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'822134', b'Emp:Comm': b'0.08', b'Emp:DoJ': b'2000-06-04', b'Emp:FirstName': b'Rahul', b'Emp:LastName': b'Dravid', b'Emp:Role': b'Sr Manager', b'Emp:Salary': b'40000'}\n",
            "b'865477' {b'Dept:DeptID': b'20', b'Dept:DeptName': b'Sales', b'Dept:Location': b'Calcutta', b'Dept:ManagerID': b'239456', b'Emp:Comm': b'0.02', b'Emp:DoJ': b'2002-04-04', b'Emp:FirstName': b'Madhuri', b'Emp:LastName': b'Dikshit', b'Emp:Role': b'Clerk', b'Emp:Salary': b'10000'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXl-QezFVXSS",
        "outputId": "1b11c82e-ded4-471e-b104-271d4d098be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#kxn = happybase.Connection('localhost',9090,autoconnect=False)\n",
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "for key, data in tEmpDept2.scan(row_start=b'230000', row_stop=b'870000',columns=[b'Emp:LastName',b'Dept:Location']):\n",
        "    print(key, data)\n",
        "kxn.close()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'239456' {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Khan'}\n",
            "b'299034' {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Ganesan'}\n",
            "b'349870' {b'Dept:Location': b'Bombay', b'Emp:LastName': b'Mukherjee'}\n",
            "b'546223' {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Karthikeyan'}\n",
            "b'742866' {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Bacchan'}\n",
            "b'822134' {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Dravid'}\n",
            "b'865477' {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Dikshit'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#kxn = happybase.Connection('localhost',9090,autoconnect=False)\n",
        "kxn.open()\n",
        "tEmpDept2 = kxn.table('EmpDept2')\n",
        "for key, data in tEmpDept2.scan(row_start=b'230000', row_stop=b'870000',columns=[b'Emp:LastName',b'Dept:Location']):\n",
        "    print(key.decode('utf-8'), data)\n",
        "    for k,v in data.items():\n",
        "        print(k.decode('utf-8'),v.decode('utf-8'))\n",
        "kxn.close()"
      ],
      "metadata": {
        "id": "5VtqHG287_zO",
        "outputId": "c3201043-c3bb-4ad1-8bce-56e2b532112e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "239456 {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Khan'}\n",
            "Dept:Location Calcutta\n",
            "Emp:LastName Khan\n",
            "299034 {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Ganesan'}\n",
            "Dept:Location Calcutta\n",
            "Emp:LastName Ganesan\n",
            "349870 {b'Dept:Location': b'Bombay', b'Emp:LastName': b'Mukherjee'}\n",
            "Dept:Location Bombay\n",
            "Emp:LastName Mukherjee\n",
            "546223 {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Karthikeyan'}\n",
            "Dept:Location Calcutta\n",
            "Emp:LastName Karthikeyan\n",
            "742866 {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Bacchan'}\n",
            "Dept:Location Calcutta\n",
            "Emp:LastName Bacchan\n",
            "822134 {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Dravid'}\n",
            "Dept:Location Calcutta\n",
            "Emp:LastName Dravid\n",
            "865477 {b'Dept:Location': b'Calcutta', b'Emp:LastName': b'Dikshit'}\n",
            "Dept:Location Calcutta\n",
            "Emp:LastName Dikshit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6PP3akdlVKJ"
      },
      "source": [
        "#Stop HBase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p28XBoitAWTL",
        "outputId": "8858b820-f9b1-4946-f09c-e8f623b74ca5"
      },
      "source": [
        "!stop-hbase.sh\n",
        "!date"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopping hbase................\n",
            "SLF4J: Class path contains multiple SLF4J bindings.\n",
            "SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.4.0/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: Found binding in [jar:file:/content/hbase-2.6.0/lib/client-facing-thirdparty/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
            "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
            "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
            "Fri Jun 14 01:43:48 PM UTC 2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "print('signed off at  ',datetime.now(pytz.timezone('Asia/Kolkata')))"
      ],
      "metadata": {
        "id": "CrVuIM4TKCxH",
        "outputId": "9505f077-3b1e-4c05-93d3-ae3e017cf258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "signed off at   2024-06-14 19:13:53.545532+05:30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chronobooks <br>\n",
        "Two science fiction novels by Prithwis Mukerjee. A dystopian Earth. A technocratic society managed by artificial intelligence. Escape and epiphany on Mars. Can man and machine, carbon and silicon explore and escape into other dimensions of existence? An Indic perspective rooted in Advaita Vedanta and the Divine Feminine.  [More information](http://bit.ly/chronobooks) <br>\n",
        "![alt text](https://github.com/Praxis-QR/RDWH/raw/main/images/CTCYFooter-1.png)"
      ],
      "metadata": {
        "id": "8t-oMZM6KIpD"
      }
    }
  ]
}