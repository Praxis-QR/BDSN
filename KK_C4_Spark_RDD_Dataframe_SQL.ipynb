{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W2QOKe1NRHn4",
        "sRvq6-Cd5cwS",
        "-tVgOWHgpc0W",
        "_2HO3H2WkkDC",
        "G9QnYzQBmqC6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praxis-QR/BDSN/blob/main/KK_C4_Spark_RDD_Dataframe_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj93_nN2wPV1"
      },
      "source": [
        "![alt text](https://github.com/Praxis-QR/RDWH/raw/main/images/YantraJaalBanner.png)<br>\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "[Prithwis Mukerjee](http://www.linkedin.com/in/prithwis)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Data Analysis | Supermarket | Netflix | IMDB\n",
        "An alternative approach is available at https://github.com/Praxis-QR/BDSN/blob/main/KK_C2A_Spark_WordCount_Alternative.ipynb"
      ],
      "metadata": {
        "id": "Y-gYEr36F06t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "print('ॐ श्री सरस्वत्यै नमः',datetime.now(pytz.timezone('Asia/Calcutta')))\n",
        "\n",
        "!python --version\n",
        "!lsb_release -a\n",
        "\n",
        "#check which version of MongoDB  is available\n",
        "#!apt-cache policy mongodb\n",
        "\n",
        "#check which versions of software are available\n",
        "#!pip3 index versions pyspark\n",
        "#!pip3 index versions pyngrok"
      ],
      "metadata": {
        "id": "dHceCHdUqREm",
        "outputId": "b5775d29-5fa3-48c0-e8b6-5d4fb7812cf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ॐ श्री सरस्वत्यै नमः 2024-01-19 05:44:23.953587+05:30\n",
            "Python 3.10.12\n",
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 22.04.3 LTS\n",
            "Release:\t22.04\n",
            "Codename:\tjammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download, Check Data\n",
        "Sourced from https://github.com/search?q=owner%3Ajigsawlabs-student%20pyspark&type=repositories"
      ],
      "metadata": {
        "id": "poH3PbIicLHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#!wget -O netflix.csv \"https://raw.githubusercontent.com/jigsawlabs-student/pyspark-cluster-lab/main/netflix_titles.csv\"\n",
        "!wget -q \"https://github.com/Praxis-QR/BDSN/raw/main/Data2/NetFlix.csv\"\n",
        "#df = pd.read_csv('s3://jigsaw-labs-student/supermarket_sales.csv')\n",
        "df1 = pd.read_csv('NetFlix.csv')\n",
        "\n",
        "!wget -q 'https://github.com/Praxis-QR/BDSN/raw/main/Data2/SuperMarketSales.csv'\n",
        "df2 = pd.read_csv('SuperMarketSales.csv')\n",
        "\n",
        "#df2 = pd.read_csv(\"s3://jigsaw-labs-student/imdb_movies.csv\")\n",
        "!wget -q 'https://github.com/Praxis-QR/BDSN/raw/main/Data2/IMDBmovies.csv'\n",
        "df3 = pd.read_csv('IMDBmovies.csv')\n",
        "df1.shape, df2.shape, df3.shape"
      ],
      "metadata": {
        "id": "Pqv2qe7ucS1i",
        "outputId": "07838b3b-39d7-4b27-d822-b8bb3c2f23bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7787, 12), (1000, 17), (2000, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSOI8k9qicmu"
      },
      "source": [
        "# Spark Installation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Installation - Skip unless specifically required"
      ],
      "metadata": {
        "id": "GguG29b062na"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIjVxBSKIDsW"
      },
      "source": [
        "#!apt update > /dev/null\n",
        "#!apt install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAgQ-0fGhxWI"
      },
      "source": [
        "# Get latest and correct version of Spark\n",
        "#\n",
        "# if the current version of Spark is not used, there may be errors\n",
        "# check here for current versions http://apache.osuosl.org/spark\n",
        "#\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.2.2/spark-2.2.2-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "#!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "#!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "#!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "#!pip install -q findspark\n",
        "#!pip install -q pyspark\n",
        "\n",
        "#import os\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ClRT_Ari2Ys"
      },
      "source": [
        "# findspark route is no more required\n",
        "#import findspark\n",
        "#findspark.init()\n",
        "#\n",
        "#findspark.init('/content/spark-2.2.2-bin-hadoop2.7')\n",
        "#\n",
        "# https://stackoverflow.com/questions/42223498/findspark-init-indexerror-list-index-out-of-range-error\n",
        "#\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple pip install"
      ],
      "metadata": {
        "id": "vI0evwwr7CpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install pyspark\n",
        "!pip3 -q install pyngrok\n",
        "#!pip3 -q install s3fs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KtBzIPRpF-W",
        "outputId": "0d0ab8c8-d0bf-42c8-8cf8-64b6750a74c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, to_timestamp\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "#from pyspark.sql.types import StructType,StructField, StringType, DateType, TimestampType, IntegerType,DoubleType, FloatType,NumericType, DecimalType\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "zshFo_neioPt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from pyspark.sql import SparkSession\n",
        "#\n",
        "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# note UI port switched from default 4040 to 4050 to avoid clash with ngrok\n",
        "#\n",
        "spark39 = SparkSession.builder.master(\"local[*]\").config('spark.ui.port', '4050').getOrCreate()\n",
        "#\n",
        "sc = spark39.sparkContext\n",
        "#\n",
        "# The spark Console UI is available in the link that will be displayed in this cell.\n",
        "# If you do not wish to use the Console, you may skip the Tunnel part\n",
        "sc"
      ],
      "metadata": {
        "id": "JA26cSUf7Maq",
        "outputId": "84ad666a-a6f0-44c0-aea4-cc3f5bf905f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cedadc892f87:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2QOKe1NRHn4"
      },
      "source": [
        "##Tunnel with ngrok <br>\n",
        "This part required ONLY if you need to access Spark Console<br>\n",
        "To get you ngrok token please visit https://dashboard.ngrok.com/login <br>\n",
        "and create a free account"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG1ZwJ8yehyw"
      },
      "source": [
        "###Loading ngrok token from GDrive<br>\n",
        "this part may be skipped if ngrok can be placed directly in one of the two next steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7uFjFW9RU9L",
        "outputId": "db5df8ef-bde8-44c3-88aa-6c19b1f6d7b3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#\n",
        "#!ls /content/drive/'My Drive'/Praxis/WebCredentials/\n",
        "#\n",
        "!cp /content/drive/'My Drive'/Praxis/WebCredentials/ngrokToken.py ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXb94HvLSSBb"
      },
      "source": [
        "# credential file for Prithwis Mukerjee\n",
        "# this file needs to be uploaded into the VM\n",
        "\n",
        "from ngrokToken import ngrokToken\n",
        "\n",
        "#for the sake of privacy\n",
        "#the following credentials need to be stored in a text file called ngrokToken.py\n",
        "#in the format given below\n",
        "#in the Colab VM\n",
        "\n",
        "#otherwise, the values can be directly placed here\n",
        "\n",
        "#ngrokToken = 'xxxxxxxxxxxxxxx'   # uncomment this line and place your own credentials here\n",
        "#token is available at https://dashboard.ngrok.com/get-started/setup\n",
        "\n",
        "\n",
        "#print(ngrokToken)\n",
        "ngrokTokenCmd = 'ngrok authtoken '+ngrokToken"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuCKEn_dsfkm"
      },
      "source": [
        "###ngrok - python wrapper <br>\n",
        "this code adapted from https://towardsdatascience.com/quickly-share-ml-webapps-from-google-colab-using-ngrok-for-free-ae899ca2661a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sWPM5QvtS4B"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx0ssMRt36k4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae054fd-426b-4634-ad3c-b3f3d0295235"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "# you may place the token directly here\n",
        "#!ngrok authtoken xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
        "get_ipython().system_raw(ngrokTokenCmd)\n",
        "# Open a HTTP tunnel on the default port 80\n",
        "#public_url = ngrok.connect(port = '4050')\n",
        "public_url = ngrok.connect(4050)\n",
        "#This is where the Spark Console UI will be visible\n",
        "public_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://0dd1-34-118-242-101.ngrok-free.app\" -> \"http://localhost:4050\">"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL_D5j-oOn2V"
      },
      "source": [
        "The next four cells are not required. Mainly used to check on the status of the Tunnel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Ne_XhzuDGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244bc68a-e027-4559-db99-08f3eeb1015c"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"tunnels\":[{\"name\":\"http-4050-7b6b18f4-59ba-42fd-8269-5f6dd6af7423\",\"ID\":\"f95df2af638ea911dcbe2d0913061c4d\",\"uri\":\"/api/tunnels/http-4050-7b6b18f4-59ba-42fd-8269-5f6dd6af7423\",\"public_url\":\"https://0dd1-34-118-242-101.ngrok-free.app\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}}],\"uri\":\"/api/tunnels\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO66r_yEftBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c990bf6-963d-41df-d856-dc092666ae8f"
      },
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://0dd1-34-118-242-101.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7tQPpQ8q09V",
        "outputId": "f0e9a5df-2c68-4352-af7b-49b454f4ecdc"
      },
      "source": [
        "!ps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    PID TTY          TIME CMD\n",
            "      1 ?        00:00:01 node\n",
            "     17 ?        00:00:00 tail\n",
            "     55 ?        00:00:01 jupyter-noteboo\n",
            "     56 ?        00:00:00 dap_multiplexer\n",
            "     66 ?        00:00:03 python3\n",
            "     86 ?        00:00:00 python3\n",
            "   1074 ?        00:00:09 java\n",
            "   1171 ?        00:00:00 bash\n",
            "   1172 ?        00:00:00 drive\n",
            "   1173 ?        00:00:00 grep\n",
            "   1220 ?        00:00:00 fusermount <defunct>\n",
            "   1246 ?        00:00:00 drive\n",
            "   1299 ?        00:00:00 bash\n",
            "   1300 ?        00:00:00 tail\n",
            "   1301 ?        00:00:00 python3\n",
            "   1345 ?        00:00:00 ngrok\n",
            "   1361 ?        00:00:00 ps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvkMV2GxrI3L"
      },
      "source": [
        "!kill -9 1219"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKbXwFgI8WW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HffGBK-_i0uj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-d-O9qyi5wG",
        "outputId": "583d4a80-f374-4165-8ba5-025d0edf132c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7787, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cArJSIybjg_r",
        "outputId": "eeea04b4-c850-4a12-edfa-659ace603e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l2bIHHPraDj",
        "outputId": "03379d9e-66b1-43d4-e034-cd24617b231d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df.to_csv('SuperMarketSales.csv', sep=',', index=False, encoding='utf-8')\n",
        "#df2.to_csv('IMDBmovies.csv', sep=',', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "C8RVZStSkSoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supermarket Sales Data"
      ],
      "metadata": {
        "id": "8XeYwvtfho38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head SuperMarketSales.csv"
      ],
      "metadata": {
        "id": "UZhtI7jal5YS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e2e56a-4725-460b-a8c8-b16557122b1b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invoice ID,Branch,City,Customer type,Gender,Product line,Unit price,Quantity,Tax 5%,Total,Date,Time,Payment,cogs,gross margin percentage,gross income,Rating\n",
            "750-67-8428,A,Yangon,Member,Female,Health and beauty,74.69,7,26.1415,548.9715,1/5/2019,13:08,Ewallet,522.83,4.761904762,26.1415,9.1\n",
            "226-31-3081,C,Naypyitaw,Normal,Female,Electronic accessories,15.28,5,3.82,80.22,3/8/2019,10:29,Cash,76.4,4.761904762,3.82,9.6\n",
            "631-41-3108,A,Yangon,Normal,Male,Home and lifestyle,46.33,7,16.2155,340.5255,3/3/2019,13:23,Credit card,324.31,4.761904762,16.2155,7.4\n",
            "123-19-1176,A,Yangon,Member,Male,Health and beauty,58.22,8,23.288,489.048,1/27/2019,20:33,Ewallet,465.76,4.761904762,23.288,8.4\n",
            "373-73-7910,A,Yangon,Normal,Male,Sports and travel,86.31,7,30.2085,634.3785,2/8/2019,10:37,Ewallet,604.17,4.761904762,30.2085,5.3\n",
            "699-14-3026,C,Naypyitaw,Normal,Male,Electronic accessories,85.39,7,29.8865,627.6165,3/25/2019,18:30,Ewallet,597.73,4.761904762,29.8865,4.1\n",
            "355-53-5943,A,Yangon,Member,Female,Electronic accessories,68.84,6,20.652,433.692,2/25/2019,14:36,Ewallet,413.04,4.761904762,20.652,5.8\n",
            "315-22-5665,C,Naypyitaw,Normal,Female,Home and lifestyle,73.56,10,36.78,772.38,2/24/2019,11:38,Ewallet,735.6,4.761904762,36.78,8.0\n",
            "665-32-9167,A,Yangon,Member,Female,Health and beauty,36.26,2,3.626,76.146,1/10/2019,17:15,Credit card,72.52,4.761904762,3.626,7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDD Operations"
      ],
      "metadata": {
        "id": "7mva4hYshAVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read CSV into RDD"
      ],
      "metadata": {
        "id": "PpSjwcjWlK1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_RDD1 = sc.textFile(\"SuperMarketSales.csv\")\n",
        "sales_RDD2 = spark39.read.text(\"SuperMarketSales.csv\").rdd\n",
        "# for an explanation of the alternate way to read text files\n",
        "# see this https://stackoverflow.com/questions/52665353/reading-a-text-file-in-spark/52669632#52669632"
      ],
      "metadata": {
        "id": "qZM6jTgIinAN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_RDD1.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oZD35n9jFXF",
        "outputId": "59c2b55b-8bb7-4cf5-e5f9-eb4d67b8148b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Invoice ID,Branch,City,Customer type,Gender,Product line,Unit price,Quantity,Tax 5%,Total,Date,Time,Payment,cogs,gross margin percentage,gross income,Rating',\n",
              " '750-67-8428,A,Yangon,Member,Female,Health and beauty,74.69,7,26.1415,548.9715,1/5/2019,13:08,Ewallet,522.83,4.761904762,26.1415,9.1',\n",
              " '226-31-3081,C,Naypyitaw,Normal,Female,Electronic accessories,15.28,5,3.82,80.22,3/8/2019,10:29,Cash,76.4,4.761904762,3.82,9.6',\n",
              " '631-41-3108,A,Yangon,Normal,Male,Home and lifestyle,46.33,7,16.2155,340.5255,3/3/2019,13:23,Credit card,324.31,4.761904762,16.2155,7.4',\n",
              " '123-19-1176,A,Yangon,Member,Male,Health and beauty,58.22,8,23.288,489.048,1/27/2019,20:33,Ewallet,465.76,4.761904762,23.288,8.4']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_RDD2.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgRuT9P0jmrW",
        "outputId": "a16a03a7-c2c0-4d4e-f68d-66120d2c694d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='Invoice ID,Branch,City,Customer type,Gender,Product line,Unit price,Quantity,Tax 5%,Total,Date,Time,Payment,cogs,gross margin percentage,gross income,Rating'),\n",
              " Row(value='750-67-8428,A,Yangon,Member,Female,Health and beauty,74.69,7,26.1415,548.9715,1/5/2019,13:08,Ewallet,522.83,4.761904762,26.1415,9.1'),\n",
              " Row(value='226-31-3081,C,Naypyitaw,Normal,Female,Electronic accessories,15.28,5,3.82,80.22,3/8/2019,10:29,Cash,76.4,4.761904762,3.82,9.6'),\n",
              " Row(value='631-41-3108,A,Yangon,Normal,Male,Home and lifestyle,46.33,7,16.2155,340.5255,3/3/2019,13:23,Credit card,324.31,4.761904762,16.2155,7.4'),\n",
              " Row(value='123-19-1176,A,Yangon,Member,Male,Health and beauty,58.22,8,23.288,489.048,1/27/2019,20:33,Ewallet,465.76,4.761904762,23.288,8.4')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed '1d' SuperMarketSales.csv > SuperMarketSales_NoHeader.csv\n",
        "sales_RDD1 = sc.textFile(\"SuperMarketSales_NoHeader.csv\")\n",
        "sales_RDD2 = spark39.read.text(\"SuperMarketSales_NoHeader.csv\").rdd\n",
        "sales_RDD1.take(5)\n",
        "#sales_RDD2.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kChJmqcbkLqL",
        "outputId": "2c40b218-d603-4767-9f4f-1aaf22ed040b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['750-67-8428,A,Yangon,Member,Female,Health and beauty,74.69,7,26.1415,548.9715,1/5/2019,13:08,Ewallet,522.83,4.761904762,26.1415,9.1',\n",
              " '226-31-3081,C,Naypyitaw,Normal,Female,Electronic accessories,15.28,5,3.82,80.22,3/8/2019,10:29,Cash,76.4,4.761904762,3.82,9.6',\n",
              " '631-41-3108,A,Yangon,Normal,Male,Home and lifestyle,46.33,7,16.2155,340.5255,3/3/2019,13:23,Credit card,324.31,4.761904762,16.2155,7.4',\n",
              " '123-19-1176,A,Yangon,Member,Male,Health and beauty,58.22,8,23.288,489.048,1/27/2019,20:33,Ewallet,465.76,4.761904762,23.288,8.4',\n",
              " '373-73-7910,A,Yangon,Normal,Male,Sports and travel,86.31,7,30.2085,634.3785,2/8/2019,10:37,Ewallet,604.17,4.761904762,30.2085,5.3']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales_RDD2.take(5)"
      ],
      "metadata": {
        "id": "StCoKYLS8DRB",
        "outputId": "6d889ce0-eea7-4180-e508-65bb4e6e2956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='750-67-8428,A,Yangon,Member,Female,Health and beauty,74.69,7,26.1415,548.9715,1/5/2019,13:08,Ewallet,522.83,4.761904762,26.1415,9.1'),\n",
              " Row(value='226-31-3081,C,Naypyitaw,Normal,Female,Electronic accessories,15.28,5,3.82,80.22,3/8/2019,10:29,Cash,76.4,4.761904762,3.82,9.6'),\n",
              " Row(value='631-41-3108,A,Yangon,Normal,Male,Home and lifestyle,46.33,7,16.2155,340.5255,3/3/2019,13:23,Credit card,324.31,4.761904762,16.2155,7.4'),\n",
              " Row(value='123-19-1176,A,Yangon,Member,Male,Health and beauty,58.22,8,23.288,489.048,1/27/2019,20:33,Ewallet,465.76,4.761904762,23.288,8.4'),\n",
              " Row(value='373-73-7910,A,Yangon,Normal,Male,Sports and travel,86.31,7,30.2085,634.3785,2/8/2019,10:37,Ewallet,604.17,4.761904762,30.2085,5.3')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analysis with RDD"
      ],
      "metadata": {
        "id": "70L1yVjFHmiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_RDD1.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuLB_bdpH1jA",
        "outputId": "21107830-b3dc-4855-9d73-7312ce0fd849"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter Data"
      ],
      "metadata": {
        "id": "x4T1AaK2U16R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x)\n",
        "sales_wallet = sales_RDD1.filter(lambda x: \"Ewallet\" in x)\n",
        "sales_cash = sales_RDD1.filter(lambda x: \"Cash\" in x)\n",
        "sales_card = sales_RDD1.filter(lambda x: \"Credit card\" in x)\n",
        "#\n",
        "wallet_count = sales_wallet.count()\n",
        "cash_count = sales_cash.count()\n",
        "card_count = sales_card.count()\n",
        "print(wallet_count, cash_count, card_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfPe0qulIKrT",
        "outputId": "44941f06-d7c1-48c6-97f4-5a8aafe849c8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "345 344 311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Aggregate"
      ],
      "metadata": {
        "id": "yaYd9j4RU8Ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def x4m1(text):\n",
        "    elements = text.split(\",\")\n",
        "    return(int(elements[7]))\n"
      ],
      "metadata": {
        "id": "Lj16h_igVDO8"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_wallet_Qty = sales_wallet.map(lambda x: x4m1(x))\n",
        "sales_wallet_Qty.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaNIZo5KWD_i",
        "outputId": "89b7b7e4-7cb8-4b66-cb7a-0728d69ec6b7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 8, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wallet_total_Qty = sales_wallet_Qty.reduce(lambda x,y : x+y)\n",
        "print('wallet total quantity = ',wallet_total_Qty, '\\nwallet average quantity =', wallet_total_Qty/wallet_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn12QeWdXE-7",
        "outputId": "fbb53d77-052d-474d-bb51-019f872da5c1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wallet total quantity =  1892 \n",
            "wallet average quantity = 5.484057971014493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accumulators"
      ],
      "metadata": {
        "id": "YmmDtntbfWiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wallet_data = sales_wallet_Qty.aggregate(\n",
        "    (0,0), # the initial value\n",
        "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
        "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
        "    )\n",
        "\n",
        "print(wallet_data[0],wallet_data[1], '\\nWallet Quantity Average = ', wallet_data[0]/wallet_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCI145B9fqJg",
        "outputId": "1289fc45-9869-4cea-905a-ab2c73b42fe4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1892 345 \n",
            "Wallet Quantity Average =  5.484057971014493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Two aggregations"
      ],
      "metadata": {
        "id": "TWDgDNdpgraX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def x4m2(text):\n",
        "    elements = text.split(\",\")\n",
        "    return(int(elements[7]), float(elements[15]))"
      ],
      "metadata": {
        "id": "FKj5CVUJaBAO"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_wallet_Qty_Inc = sales_wallet.map(lambda x: x4m2(x))\n",
        "sales_wallet_Qty_Inc.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpyErMW1aJLO",
        "outputId": "5dbcfc7d-95a3-4745-9a3b-bb5e872d84ab"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7, 26.1415), (8, 23.288), (7, 30.2085)]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wallet_data = sales_wallet_Qty_Inc.aggregate(\n",
        "    (0,0,0), # the initial value\n",
        "    (lambda acc, value: (acc[0] + value[0], acc[1] + value[1], acc[2] + 1)), # combine value with acc\n",
        "    (lambda acc1, acc2, acc3 : (acc1[0] + acc2[0], acc2[1] + acc2[1], acc3[1] + acc3[1])) # combine accumulators\n",
        "    )\n",
        "\n",
        "print(wallet_data[0],wallet_data[2], '\\nWallet Quantity Average = ', wallet_data[0]/wallet_data[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "WPhYIjSEkFid",
        "outputId": "19d763c7-7c8f-4960-b9d5-9facf87cada1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "<lambda>() missing 1 required positional argument: 'acc3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-6da9e54ddc6d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m wallet_data = sales_wallet_Qty_Inc.aggregate(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# the initial value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# combine value with acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0macc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc3\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0macc1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# combine accumulators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, zeroValue, seqOp, combOp)\u001b[0m\n\u001b[1;32m   2104\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m     def treeAggregate(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/util.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             raise PySparkRuntimeError(\n",
            "\u001b[0;31mTypeError\u001b[0m: <lambda>() missing 1 required positional argument: 'acc3'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wallet_total_Qty_Inc = sales_wallet_Qty_Inc.reduce(lambda x,y : x[0]+y[0])\n",
        "print(wallet_total_Qty_Inc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N-YQn9UgaYVt",
        "outputId": "c960ff23-ffcc-493b-a1f5-2568c01da5af"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 49.0 failed 1 times, most recent failure: Lost task 1.0 in stage 49.0 (TID 61) (cedadc892f87 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-53-e3792c18740f>\", line 1, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1046)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-53-e3792c18740f>\", line 1, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1046)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-e3792c18740f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwallet_total_Qty_Inc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msales_wallet_Qty_Inc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwallet_total_Qty_Inc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1922\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 49.0 failed 1 times, most recent failure: Lost task 1.0 in stage 49.0 (TID 61) (cedadc892f87 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-53-e3792c18740f>\", line 1, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1046)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-53-e3792c18740f>\", line 1, in <lambda>\nTypeError: 'int' object is not subscriptable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1046)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert RDD to Spark DataFrame"
      ],
      "metadata": {
        "id": "bzNLlQGSl2XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data on , and convert string to other datatypes\n",
        "#\n",
        "sales_RDD1_split1 = sales_RDD1.map(lambda l: l.split(\",\"))\n",
        "#sales_RDD1_split1.take(1)\n",
        "sales_RDD1_split2 = sales_RDD1_split1.map(lambda l: [l[0],l[1],l[2],l[3],l[4],l[5],float(l[6]),int(l[7]),float(l[8]),float(l[9]),l[10],l[11],l[12],float(l[13]),float(l[14]),float(l[15]),float(l[16])])\n",
        "#sales_RDD1_split2.take(1)"
      ],
      "metadata": {
        "id": "d3Ncb2dl1Q9q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to dataframe\n",
        "# Date, Time does not convert correctly\n",
        "#\n",
        "colSchema = StructType([\n",
        "    StructField('Invoice ID', StringType(), True),\n",
        "    StructField('Branch', StringType(), True),\n",
        "    StructField('City', StringType(), True),\n",
        "    StructField('Customer Type', StringType(), True),\n",
        "    StructField('Gender', StringType(), True),\n",
        "    StructField('Product Line', StringType(), True),\n",
        "    StructField('Unit Price', FloatType(), True),\n",
        "    StructField('Quantity', IntegerType(), True),\n",
        "    StructField('Tax 5%', DoubleType(), True),\n",
        "    StructField('Total', DoubleType(), True),\n",
        "    StructField('Date', StringType(), True),\n",
        "    StructField('Time', StringType(), True),\n",
        "    StructField('Payment', StringType(), True),\n",
        "    StructField('cogs', DoubleType(), True),\n",
        "    StructField('gross margin percentage', DoubleType(), True),\n",
        "    StructField('gross income', DoubleType(), True),\n",
        "    StructField('Rating', DoubleType(), True)\n",
        "])\n",
        "\n",
        "sales_df1 = spark39.createDataFrame(data = sales_RDD1_split2, schema = colSchema)\n",
        "\n",
        "#sales_df1.printSchema()\n",
        "#sales_df1.show(5)"
      ],
      "metadata": {
        "id": "aY9BFmvroc_l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Date, Time datatype\n",
        "#\n",
        "sales_df1 = sales_df1.withColumn('Date',to_date(col('Date'), \"M/d/y\"))\n",
        "sales_df1 = sales_df1.withColumn('Time',to_timestamp(col('Time'), \"HH:mm\"))\n",
        "\n",
        "sales_df1.printSchema()\n",
        "sales_df1.show(5)"
      ],
      "metadata": {
        "id": "Pcw0mGh8g5xY",
        "outputId": "1208940e-ee69-4a59-ddcf-6cc4b693c4d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Invoice ID: string (nullable = true)\n",
            " |-- Branch: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Customer Type: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Product Line: string (nullable = true)\n",
            " |-- Unit Price: float (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Tax 5%: double (nullable = true)\n",
            " |-- Total: double (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Time: timestamp (nullable = true)\n",
            " |-- Payment: string (nullable = true)\n",
            " |-- cogs: double (nullable = true)\n",
            " |-- gross margin percentage: double (nullable = true)\n",
            " |-- gross income: double (nullable = true)\n",
            " |-- Rating: double (nullable = true)\n",
            "\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "| Invoice ID|Branch|     City|Customer Type|Gender|        Product Line|Unit Price|Quantity| Tax 5%|   Total|      Date|               Time|    Payment|  cogs|gross margin percentage|gross income|Rating|\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "|750-67-8428|     A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|548.9715|2019-01-05|1970-01-01 13:08:00|    Ewallet|522.83|            4.761904762|     26.1415|   9.1|\n",
            "|226-31-3081|     C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|   80.22|2019-03-08|1970-01-01 10:29:00|       Cash|  76.4|            4.761904762|        3.82|   9.6|\n",
            "|631-41-3108|     A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|340.5255|2019-03-03|1970-01-01 13:23:00|Credit card|324.31|            4.761904762|     16.2155|   7.4|\n",
            "|123-19-1176|     A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 489.048|2019-01-27|1970-01-01 20:33:00|    Ewallet|465.76|            4.761904762|      23.288|   8.4|\n",
            "|373-73-7910|     A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|634.3785|2019-02-08|1970-01-01 10:37:00|    Ewallet|604.17|            4.761904762|     30.2085|   5.3|\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataframe Operations"
      ],
      "metadata": {
        "id": "pFfrwyqBhULk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read CSV into Spark Dataframe"
      ],
      "metadata": {
        "id": "1LGAoj2Yoxva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df2 = spark39.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"./SuperMarketSales.csv\")\n",
        "#sales_df2.printSchema()\n",
        "#sales_df2.show(5)"
      ],
      "metadata": {
        "id": "hHD1IOKfo0im"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df2 = sales_df2.withColumn('Date',to_date(col('Date'), \"M/d/y\"))\n",
        "sales_df2.printSchema()\n",
        "sales_df2.show(5)"
      ],
      "metadata": {
        "id": "LbinD7CNnX9A",
        "outputId": "ad4b260e-2c0e-4f58-b4f8-68fdac3a7eba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Invoice ID: string (nullable = true)\n",
            " |-- Branch: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Customer type: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Product line: string (nullable = true)\n",
            " |-- Unit price: double (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Tax 5%: double (nullable = true)\n",
            " |-- Total: double (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Time: timestamp (nullable = true)\n",
            " |-- Payment: string (nullable = true)\n",
            " |-- cogs: double (nullable = true)\n",
            " |-- gross margin percentage: double (nullable = true)\n",
            " |-- gross income: double (nullable = true)\n",
            " |-- Rating: double (nullable = true)\n",
            "\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "| Invoice ID|Branch|     City|Customer type|Gender|        Product line|Unit price|Quantity| Tax 5%|   Total|      Date|               Time|    Payment|  cogs|gross margin percentage|gross income|Rating|\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "|750-67-8428|     A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|548.9715|2019-01-05|2024-01-19 13:08:00|    Ewallet|522.83|            4.761904762|     26.1415|   9.1|\n",
            "|226-31-3081|     C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|   80.22|2019-03-08|2024-01-19 10:29:00|       Cash|  76.4|            4.761904762|        3.82|   9.6|\n",
            "|631-41-3108|     A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|340.5255|2019-03-03|2024-01-19 13:23:00|Credit card|324.31|            4.761904762|     16.2155|   7.4|\n",
            "|123-19-1176|     A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 489.048|2019-01-27|2024-01-19 20:33:00|    Ewallet|465.76|            4.761904762|      23.288|   8.4|\n",
            "|373-73-7910|     A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|634.3785|2019-02-08|2024-01-19 10:37:00|    Ewallet|604.17|            4.761904762|     30.2085|   5.3|\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL Table Operations"
      ],
      "metadata": {
        "id": "PufIPWYghgKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataframe to Table"
      ],
      "metadata": {
        "id": "gjSUFvSgsIAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df2.createOrReplaceTempView(\"tSales\")\n",
        "spark39.sql(\"describe tSales\").show()"
      ],
      "metadata": {
        "id": "QtGXv1vdsUv7",
        "outputId": "0258e553-921a-4f83-d08f-9cc12240430a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------+-------+\n",
            "|            col_name|data_type|comment|\n",
            "+--------------------+---------+-------+\n",
            "|          Invoice ID|   string|   NULL|\n",
            "|              Branch|   string|   NULL|\n",
            "|                City|   string|   NULL|\n",
            "|       Customer type|   string|   NULL|\n",
            "|              Gender|   string|   NULL|\n",
            "|        Product line|   string|   NULL|\n",
            "|          Unit price|   double|   NULL|\n",
            "|            Quantity|      int|   NULL|\n",
            "|              Tax 5%|   double|   NULL|\n",
            "|               Total|   double|   NULL|\n",
            "|                Date|     date|   NULL|\n",
            "|                Time|timestamp|   NULL|\n",
            "|             Payment|   string|   NULL|\n",
            "|                cogs|   double|   NULL|\n",
            "|gross margin perc...|   double|   NULL|\n",
            "|        gross income|   double|   NULL|\n",
            "|              Rating|   double|   NULL|\n",
            "+--------------------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark39.sql(\"show tables\").show()\n",
        "spark39.sql(\"select * from tsales\").show()"
      ],
      "metadata": {
        "id": "kBCxzrIrssZA",
        "outputId": "08caf702-e8da-40ec-9a16-65d2f77c8635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|         |   tsales|       true|\n",
            "+---------+---------+-----------+\n",
            "\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "| Invoice ID|Branch|     City|Customer type|Gender|        Product line|Unit price|Quantity| Tax 5%|   Total|      Date|               Time|    Payment|  cogs|gross margin percentage|gross income|Rating|\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "|750-67-8428|     A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|548.9715|2019-01-05|2024-01-19 13:08:00|    Ewallet|522.83|            4.761904762|     26.1415|   9.1|\n",
            "|226-31-3081|     C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|   80.22|2019-03-08|2024-01-19 10:29:00|       Cash|  76.4|            4.761904762|        3.82|   9.6|\n",
            "|631-41-3108|     A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|340.5255|2019-03-03|2024-01-19 13:23:00|Credit card|324.31|            4.761904762|     16.2155|   7.4|\n",
            "|123-19-1176|     A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 489.048|2019-01-27|2024-01-19 20:33:00|    Ewallet|465.76|            4.761904762|      23.288|   8.4|\n",
            "|373-73-7910|     A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|634.3785|2019-02-08|2024-01-19 10:37:00|    Ewallet|604.17|            4.761904762|     30.2085|   5.3|\n",
            "|699-14-3026|     C|Naypyitaw|       Normal|  Male|Electronic access...|     85.39|       7|29.8865|627.6165|2019-03-25|2024-01-19 18:30:00|    Ewallet|597.73|            4.761904762|     29.8865|   4.1|\n",
            "|355-53-5943|     A|   Yangon|       Member|Female|Electronic access...|     68.84|       6| 20.652| 433.692|2019-02-25|2024-01-19 14:36:00|    Ewallet|413.04|            4.761904762|      20.652|   5.8|\n",
            "|315-22-5665|     C|Naypyitaw|       Normal|Female|  Home and lifestyle|     73.56|      10|  36.78|  772.38|2019-02-24|2024-01-19 11:38:00|    Ewallet| 735.6|            4.761904762|       36.78|   8.0|\n",
            "|665-32-9167|     A|   Yangon|       Member|Female|   Health and beauty|     36.26|       2|  3.626|  76.146|2019-01-10|2024-01-19 17:15:00|Credit card| 72.52|            4.761904762|       3.626|   7.2|\n",
            "|692-92-5582|     B| Mandalay|       Member|Female|  Food and beverages|     54.84|       3|  8.226| 172.746|2019-02-20|2024-01-19 13:27:00|Credit card|164.52|            4.761904762|       8.226|   5.9|\n",
            "|351-62-0822|     B| Mandalay|       Member|Female| Fashion accessories|     14.48|       4|  2.896|  60.816|2019-02-06|2024-01-19 18:07:00|    Ewallet| 57.92|            4.761904762|       2.896|   4.5|\n",
            "|529-56-3974|     B| Mandalay|       Member|  Male|Electronic access...|     25.51|       4|  5.102| 107.142|2019-03-09|2024-01-19 17:03:00|       Cash|102.04|            4.761904762|       5.102|   6.8|\n",
            "|365-64-0515|     A|   Yangon|       Normal|Female|Electronic access...|     46.95|       5|11.7375|246.4875|2019-02-12|2024-01-19 10:25:00|    Ewallet|234.75|            4.761904762|     11.7375|   7.1|\n",
            "|252-56-2699|     A|   Yangon|       Normal|  Male|  Food and beverages|     43.19|      10| 21.595| 453.495|2019-02-07|2024-01-19 16:48:00|    Ewallet| 431.9|            4.761904762|      21.595|   8.2|\n",
            "|829-34-3910|     A|   Yangon|       Normal|Female|   Health and beauty|     71.38|      10|  35.69|  749.49|2019-03-29|2024-01-19 19:21:00|       Cash| 713.8|            4.761904762|       35.69|   5.7|\n",
            "|299-46-1805|     B| Mandalay|       Member|Female|   Sports and travel|     93.72|       6| 28.116| 590.436|2019-01-15|2024-01-19 16:19:00|       Cash|562.32|            4.761904762|      28.116|   4.5|\n",
            "|656-95-9349|     A|   Yangon|       Member|Female|   Health and beauty|     68.93|       7|24.1255|506.6355|2019-03-11|2024-01-19 11:03:00|Credit card|482.51|            4.761904762|     24.1255|   4.6|\n",
            "|765-26-6951|     A|   Yangon|       Normal|  Male|   Sports and travel|     72.61|       6| 21.783| 457.443|2019-01-01|2024-01-19 10:39:00|Credit card|435.66|            4.761904762|      21.783|   6.9|\n",
            "|329-62-1586|     A|   Yangon|       Normal|  Male|  Food and beverages|     54.67|       3| 8.2005|172.2105|2019-01-21|2024-01-19 18:00:00|Credit card|164.01|            4.761904762|      8.2005|   8.6|\n",
            "|319-50-3348|     B| Mandalay|       Normal|Female|  Home and lifestyle|      40.3|       2|   4.03|   84.63|2019-03-11|2024-01-19 15:30:00|    Ewallet|  80.6|            4.761904762|        4.03|   4.4|\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql_drop_table = \"\"\"drop table if exists praxisdb.sales_T\"\"\"\n",
        "\n",
        "sql_drop_database = \"\"\"drop database if exists praxisdb cascade\"\"\"\n",
        "\n",
        "sql_create_database = \"\"\"create database if not exists praxisdb location '/content/praxisdb/'\"\"\"\n",
        "\n",
        "sql_create_table = \"\"\"create table if not exists praxisdb.sales_T using parquet as select  * from tSales\"\"\"\n",
        "\n",
        "spark39.sql(sql_drop_database)\n",
        "\n",
        "spark39.sql(sql_create_database)\n",
        "\n",
        "spark39.sql(sql_drop_table)\n",
        "\n",
        "spark39.sql(sql_create_table)"
      ],
      "metadata": {
        "id": "TYIPqzuItXXO",
        "outputId": "ed2e7efc-e85e-4fdc-8691-1c45700440b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark39.sql(\"use praxisdb\")\n",
        "spark39.sql(\"show tables\").show()"
      ],
      "metadata": {
        "id": "4clewVjTszG6",
        "outputId": "84fdf242-64db-4930-c7fa-486dc0c8941a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "| praxisdb|  sales_t|      false|\n",
            "|         |   tsales|       true|\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark39.sql(\"select * from praxisdb.sales_t\").show()"
      ],
      "metadata": {
        "id": "awQOz6NpuvzR",
        "outputId": "6ee320dd-9e56-4394-bb47-77e9bea3780a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "| Invoice ID|Branch|     City|Customer type|Gender|        Product line|Unit price|Quantity| Tax 5%|   Total|      Date|               Time|    Payment|  cogs|gross margin percentage|gross income|Rating|\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "|750-67-8428|     A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|548.9715|2019-01-05|2024-01-19 13:08:00|    Ewallet|522.83|            4.761904762|     26.1415|   9.1|\n",
            "|226-31-3081|     C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|   80.22|2019-03-08|2024-01-19 10:29:00|       Cash|  76.4|            4.761904762|        3.82|   9.6|\n",
            "|631-41-3108|     A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|340.5255|2019-03-03|2024-01-19 13:23:00|Credit card|324.31|            4.761904762|     16.2155|   7.4|\n",
            "|123-19-1176|     A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 489.048|2019-01-27|2024-01-19 20:33:00|    Ewallet|465.76|            4.761904762|      23.288|   8.4|\n",
            "|373-73-7910|     A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|634.3785|2019-02-08|2024-01-19 10:37:00|    Ewallet|604.17|            4.761904762|     30.2085|   5.3|\n",
            "|699-14-3026|     C|Naypyitaw|       Normal|  Male|Electronic access...|     85.39|       7|29.8865|627.6165|2019-03-25|2024-01-19 18:30:00|    Ewallet|597.73|            4.761904762|     29.8865|   4.1|\n",
            "|355-53-5943|     A|   Yangon|       Member|Female|Electronic access...|     68.84|       6| 20.652| 433.692|2019-02-25|2024-01-19 14:36:00|    Ewallet|413.04|            4.761904762|      20.652|   5.8|\n",
            "|315-22-5665|     C|Naypyitaw|       Normal|Female|  Home and lifestyle|     73.56|      10|  36.78|  772.38|2019-02-24|2024-01-19 11:38:00|    Ewallet| 735.6|            4.761904762|       36.78|   8.0|\n",
            "|665-32-9167|     A|   Yangon|       Member|Female|   Health and beauty|     36.26|       2|  3.626|  76.146|2019-01-10|2024-01-19 17:15:00|Credit card| 72.52|            4.761904762|       3.626|   7.2|\n",
            "|692-92-5582|     B| Mandalay|       Member|Female|  Food and beverages|     54.84|       3|  8.226| 172.746|2019-02-20|2024-01-19 13:27:00|Credit card|164.52|            4.761904762|       8.226|   5.9|\n",
            "|351-62-0822|     B| Mandalay|       Member|Female| Fashion accessories|     14.48|       4|  2.896|  60.816|2019-02-06|2024-01-19 18:07:00|    Ewallet| 57.92|            4.761904762|       2.896|   4.5|\n",
            "|529-56-3974|     B| Mandalay|       Member|  Male|Electronic access...|     25.51|       4|  5.102| 107.142|2019-03-09|2024-01-19 17:03:00|       Cash|102.04|            4.761904762|       5.102|   6.8|\n",
            "|365-64-0515|     A|   Yangon|       Normal|Female|Electronic access...|     46.95|       5|11.7375|246.4875|2019-02-12|2024-01-19 10:25:00|    Ewallet|234.75|            4.761904762|     11.7375|   7.1|\n",
            "|252-56-2699|     A|   Yangon|       Normal|  Male|  Food and beverages|     43.19|      10| 21.595| 453.495|2019-02-07|2024-01-19 16:48:00|    Ewallet| 431.9|            4.761904762|      21.595|   8.2|\n",
            "|829-34-3910|     A|   Yangon|       Normal|Female|   Health and beauty|     71.38|      10|  35.69|  749.49|2019-03-29|2024-01-19 19:21:00|       Cash| 713.8|            4.761904762|       35.69|   5.7|\n",
            "|299-46-1805|     B| Mandalay|       Member|Female|   Sports and travel|     93.72|       6| 28.116| 590.436|2019-01-15|2024-01-19 16:19:00|       Cash|562.32|            4.761904762|      28.116|   4.5|\n",
            "|656-95-9349|     A|   Yangon|       Member|Female|   Health and beauty|     68.93|       7|24.1255|506.6355|2019-03-11|2024-01-19 11:03:00|Credit card|482.51|            4.761904762|     24.1255|   4.6|\n",
            "|765-26-6951|     A|   Yangon|       Normal|  Male|   Sports and travel|     72.61|       6| 21.783| 457.443|2019-01-01|2024-01-19 10:39:00|Credit card|435.66|            4.761904762|      21.783|   6.9|\n",
            "|329-62-1586|     A|   Yangon|       Normal|  Male|  Food and beverages|     54.67|       3| 8.2005|172.2105|2019-01-21|2024-01-19 18:00:00|Credit card|164.01|            4.761904762|      8.2005|   8.6|\n",
            "|319-50-3348|     B| Mandalay|       Normal|Female|  Home and lifestyle|      40.3|       2|   4.03|   84.63|2019-03-11|2024-01-19 15:30:00|    Ewallet|  80.6|            4.761904762|        4.03|   4.4|\n",
            "+-----------+------+---------+-------------+------+--------------------+----------+--------+-------+--------+----------+-------------------+-----------+------+-----------------------+------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table Data Analysis"
      ],
      "metadata": {
        "id": "rVE9KiBLFNqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE backtick ` used to enclose column names with spaces inside!\n",
        "spark39.sql(\"select payment, count(*), sum(quantity), avg(quantity), sum(`gross income`), avg(`gross income`) from praxisdb.sales_t group by payment \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKRZR8rCFZTt",
        "outputId": "76511589-3aec-45df-ddb4-6a7309b4e89d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+-------------+-----------------+-----------------+------------------+\n",
            "|    payment|count(1)|sum(quantity)|    avg(quantity)|sum(gross income)| avg(gross income)|\n",
            "+-----------+--------+-------------+-----------------+-----------------+------------------+\n",
            "|    Ewallet|     345|         1892|5.484057971014493|5237.767000000001|15.181933333333335|\n",
            "|       Cash|     344|         1896|5.511627906976744|5343.170000000006|15.532470930232577|\n",
            "|Credit card|     311|         1722|5.536977491961415|4798.432000000001| 15.42904180064309|\n",
            "+-----------+--------+-------------+-----------------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark39.sql(\"select `gross income` from praxisdb.sales_t\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95EkBGJcGpmF",
        "outputId": "7dfcfd38-0a99-494b-94cf-6a274f81423f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|gross income|\n",
            "+------------+\n",
            "|     26.1415|\n",
            "|        3.82|\n",
            "|     16.2155|\n",
            "|      23.288|\n",
            "|     30.2085|\n",
            "|     29.8865|\n",
            "|      20.652|\n",
            "|       36.78|\n",
            "|       3.626|\n",
            "|       8.226|\n",
            "|       2.896|\n",
            "|       5.102|\n",
            "|     11.7375|\n",
            "|      21.595|\n",
            "|       35.69|\n",
            "|      28.116|\n",
            "|     24.1255|\n",
            "|      21.783|\n",
            "|      8.2005|\n",
            "|        4.03|\n",
            "+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKTZezaqMK_0"
      },
      "source": [
        "#Chronobooks <br>\n",
        "![alt text](https://1.bp.blogspot.com/-lTiYBkU2qbU/X1er__fvnkI/AAAAAAAAjtE/GhDR3OEGJr4NG43fZPodrQD5kbxtnKebgCLcBGAsYHQ/s600/Footer2020-600x200.png)<hr>\n",
        "Chronotantra and Chronoyantra are two science fiction novels that explore the collapse of human civilisation on Earth and then its rebirth and reincarnation both on Earth as well as on the distant worlds of Mars, Titan and Enceladus. But is it the human civilisation that is being reborn? Or is it some other sentience that is revealing itself.\n",
        "If you have an interest in AI and found this material useful, you may consider buying these novels, in paperback or kindle, from [http://bit.ly/chronobooks](http://bit.ly/chronobooks)"
      ]
    }
  ]
}