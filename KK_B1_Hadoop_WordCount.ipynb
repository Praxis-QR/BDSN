{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KK B1 Hadoop WordCount",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praxis-QR/BDSN/blob/main/KK_B1_Hadoop_WordCount.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOvxIBi-IJ8G"
      },
      "source": [
        "![alt text](https://github.com/Praxis-QR/RDWH/raw/main/images/YantraJaalBanner.png)<br>\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "[Prithwis Mukerjee](http://www.linkedin.com/in/prithwis)<br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "print('Tested',datetime.now(pytz.timezone('Asia/Calcutta')))"
      ],
      "metadata": {
        "id": "usCcWgYGelUW",
        "outputId": "ca542f05-51ae-4ce4-ad40-bdae1217f42a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tested 2024-03-12 05:46:29.967760+05:30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwjvirExIQAM"
      },
      "source": [
        "#Hadoop\n",
        "This Notebook has all the codes required to install Hadoop in the Colab VM and execute the a WordCount program using the streaming API <br>\n",
        "The mapper.py and reducer.py programs are available in the authors G-Drive / Github and are downloaded as required<br>\n",
        "[100 Interview questions on Hadoop](https://data-flair.training/blogs/top-100-hadoop-interview-questions-and-answers/)\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK5y3g-ySDmZ"
      },
      "source": [
        "##Acknowledgements\n",
        "Hadoop Installation from [Anjaly Sam's Github Repository](https://github.com/anjalysam/Hadoop) <br>\n",
        "\n",
        "To get the concept behind map-reduce see [this notebook](https://github.com/Praxis-QR/BDSN/blob/main/Basic_WordCount_Concept.ipynb) <br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9bT9M1yvyXG"
      },
      "source": [
        "# 1 Download, Install Hadoop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXFZuorwF25e"
      },
      "source": [
        "# The default JVM available at /usr/lib/jvm/java-11-openjdk-amd64/  works for Hadoop\n",
        "# But gives errors with Hive https://stackoverflow.com/questions/54037773/hive-exception-class-jdk-internal-loader-classloadersappclassloader-cannot\n",
        "# Hence this JVM needs to be installed\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bijZAdD_cBMK",
        "outputId": "598516d8-156a-41eb-82ea-f9b146f910ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# If there is an error in this cell, it is very likely that the version of hadoop has changed\n",
        "# Download the latest version of Hadoop and change the version numbers accordingly\n",
        "#wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
        "#!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
        "#!wget  https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
        "!wget  https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n",
        "# Unzip it\n",
        "# the tar command with the -x flag to extract, -z to uncompress, -v for verbose output, and -f to specify that we’re extracting from a file\n",
        "#!tar -xzf hadoop-3.3.2.tar.gz\n",
        "!tar -xzf hadoop-3.3.5.tar.gz\n",
        "#copy  hadoop file to user/local\n",
        "#!mv  hadoop-3.3.2/ /usr/local/\n",
        "!mv  hadoop-3.3.5/ /usr/local/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-12 00:17:07--  https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 706533213 (674M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.5.tar.gz’\n",
            "\n",
            "hadoop-3.3.5.tar.gz 100%[===================>] 673.80M  28.0MB/s    in 25s     \n",
            "\n",
            "2024-03-12 00:17:32 (27.2 MB/s) - ‘hadoop-3.3.5.tar.gz’ saved [706533213/706533213]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pjQecX-LetK",
        "outputId": "56b23516-d31d-48a4-cd80-6a50095ad447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /usr/local"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin    cuda\tcuda-12.2  games\t       hadoop-3.3.5  lib    licensing  sbin   src\n",
            "colab  cuda-12\tetc\t   _gcs_config_ops.so  include\t     lib64  man        share\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh6Dqbbrwqpe"
      },
      "source": [
        "# 2 Set Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OUc19ZtcBG5"
      },
      "source": [
        "#To find the default Java path\n",
        "#!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n",
        "#!ls /usr/lib/jvm/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ez4T7Gs3RAn"
      },
      "source": [
        "#To set java path, go to /usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh then\n",
        "#. . . export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ . . .\n",
        "#we have used a simpler alternative route using os.environ - it works\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"   # default is changed\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\"\n",
        "# make sure that the version number is as downloaded\n",
        "#os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.0/\"\n",
        "#os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.2/\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.5/\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKOGAmCVhXZ4",
        "outputId": "73b1b370-f450-4a75-c660-15933b7216e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!echo $PATH"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDFgpWGLhdhl",
        "outputId": "fc57f666-f3a2-46f1-9632-870d740445a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Add Hadoop BIN to PATH\n",
        "# Get the current_path from output of previous command\n",
        "#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'\n",
        "#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.2/bin/'\n",
        "#os.environ[\"PATH\"] = new_path\n",
        "#!echo $PATH\n",
        "\n",
        "current_path = os.getenv('PATH')\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.0/bin/'\n",
        "#new_path = current_path+':/usr/local/hadoop-3.3.2/bin/'\n",
        "new_path = current_path+':/usr/local/hadoop-3.3.5/bin/'\n",
        "os.environ[\"PATH\"] = new_path\n",
        "!echo $PATH"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/usr/local/hadoop-3.3.5/bin/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj00rPPZyEWZ"
      },
      "source": [
        "# 3 Test Hadoop Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhf-zK7NcBDF"
      },
      "source": [
        "#Running Hadoop - Test RUN, not doing anything at all\n",
        "#!/usr/local/hadoop-3.3.0/bin/hadoop\n",
        "# UNCOMMENT the following line if you want to make sure that Hadoop is alive!\n",
        "#!hadoop"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n3I6iqjGod-"
      },
      "source": [
        "# Testing Hadoop with PI generating sample program, should calculate value of pi = 3.14157500000000000000\n",
        "# pi example\n",
        "#Uncomment the following line if  you want to test Hadoop with pi example\n",
        "# Final output should be : Estimated value of Pi is 3.14157500000000000000\n",
        "#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar pi 16 100000\n",
        "#!hadoop jar /usr/local/hadoop-3.3.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.2.jar pi 16 100000\n",
        "#!hadoop jar /usr/local/hadoop-3.3.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar pi 16 100000"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC9WSzgMprwr"
      },
      "source": [
        "# 4 Run WordCount with Hadoop\n",
        "Instead of using Java for Map and Reduce methods, we use the streaming API of Hadoop and two simple python programs as mapper.py and reducer.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxloM8fzqORx"
      },
      "source": [
        "# get mapper.py reducer.py from G_drive\n",
        "#!gdown https://drive.google.com/uc?id=1VTzQ18cWAj6L29ncW6sABy-ITmDCcv5r\n",
        "#!gdown https://drive.google.com/uc?id=1Or8Cbf9AsFMHStjMzDw3pXCd6TZ0dqxJ\n",
        "\n",
        "#get mapper.py reducer.py from this git repository\n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/mapper.py\n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/reducer.py"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRWPIMj9qpZK"
      },
      "source": [
        "# to see the codes, uncomment the following lines\n",
        "#!cat mapper.py\n",
        "#print(\"\\n----------------------    see above for mapper, see below for reducer\")\n",
        "#!cat reducer.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MQ_4eaxqlCW"
      },
      "source": [
        "# python codes are made executable\n",
        "!chmod u+rwx /content/mapper.py\n",
        "!chmod u+rwx /content/reducer.py"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yfxo4CJrCtJ"
      },
      "source": [
        "# get a simple txt file as data for word count\n",
        "# or you can upload your own\n",
        "#!gdown https://drive.google.com/uc?id=1R5W0UVH2S3JjPxerqyX4ue5y6tMt0Wkk\n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/Chronotantra.txt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W38-6u0KrSrp",
        "outputId": "14eacccc-016d-495a-f6f6-b15c3d0ca058"
      },
      "source": [
        "# locate the streaming jar file\n",
        "!find / -name 'hadoop-streaming*.jar'"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘/proc/62/task/62/net’: Invalid argument\n",
            "find: ‘/proc/62/net’: Invalid argument\n",
            "/usr/local/hadoop-3.3.5/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar\n",
            "/usr/local/hadoop-3.3.5/share/hadoop/tools/sources/hadoop-streaming-3.3.5-sources.jar\n",
            "/usr/local/hadoop-3.3.5/share/hadoop/tools/sources/hadoop-streaming-3.3.5-test-sources.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h60r05EPk-xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e016bf59-39fe-48fb-abf5-bef1f6539a1b"
      },
      "source": [
        "# remove output directories\n",
        "!rm -r wc_out\n",
        "!rm -r wc2_out"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'wc_out': No such file or directory\n",
            "rm: cannot remove 'wc2_out': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH-I6ow7rl9k",
        "outputId": "05a2e57e-f87f-4e23-b64e-85e2e737f238"
      },
      "source": [
        "# execute the streaming jar with proper parameters\n",
        "# four parameters are input file, output directory, the mapper progra, the reducer program\n",
        "#\n",
        "#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar -input /content/hobbit.txt -output /content/wc_out -file /content/mapper.py  -file /content/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'\n",
        "#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar -input /content/Chronotantra.txt -output /content/wc_out -file /content/mapper.py  -file /content/reducer.py  -mapper 'python mapper.py'  -reducer 'python reducer.py'\n",
        "#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar -input /content/Chronotantra.txt -output /content/wc_out  -mapper 'python mapper.py'  -reducer 'python reducer.py'\n",
        "#!hadoop jar /usr/local/hadoop-3.3.2/share/hadoop/tools/lib/hadoop-streaming-3.3.2.jar -input /content/Chronotantra.txt -output /content/wc_out  -mapper 'python mapper.py'  -reducer 'python reducer.py'\n",
        "!hadoop jar /usr/local/hadoop-3.3.5/share/hadoop/tools/lib/hadoop-streaming-3.3.5.jar -input /content/Chronotantra.txt -output /content/wc_out  -mapper 'python mapper.py'  -reducer 'python reducer.py'"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-03-12 00:22:16,934 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-03-12 00:22:17,039 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-03-12 00:22:17,039 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-03-12 00:22:17,078 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-12 00:22:17,335 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-03-12 00:22:17,355 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-03-12 00:22:17,540 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1108542370_0001\n",
            "2024-03-12 00:22:17,540 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-03-12 00:22:17,805 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-03-12 00:22:17,807 INFO mapreduce.Job: Running job: job_local1108542370_0001\n",
            "2024-03-12 00:22:17,815 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-03-12 00:22:17,817 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-03-12 00:22:17,823 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-12 00:22:17,826 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-12 00:22:17,880 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-03-12 00:22:17,885 INFO mapred.LocalJobRunner: Starting task: attempt_local1108542370_0001_m_000000_0\n",
            "2024-03-12 00:22:17,919 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-12 00:22:17,920 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-12 00:22:17,951 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-12 00:22:17,967 INFO mapred.MapTask: Processing split: file:/content/Chronotantra.txt:0+353890\n",
            "2024-03-12 00:22:17,984 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-03-12 00:22:18,062 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-03-12 00:22:18,062 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-03-12 00:22:18,062 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-03-12 00:22:18,062 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-03-12 00:22:18,062 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-03-12 00:22:18,065 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-03-12 00:22:18,072 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n",
            "2024-03-12 00:22:18,077 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-03-12 00:22:18,080 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-03-12 00:22:18,080 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-03-12 00:22:18,081 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-03-12 00:22:18,082 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-03-12 00:22:18,082 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-03-12 00:22:18,086 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-03-12 00:22:18,087 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-03-12 00:22:18,087 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-03-12 00:22:18,088 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-03-12 00:22:18,088 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-03-12 00:22:18,089 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-03-12 00:22:18,124 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-12 00:22:18,124 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-12 00:22:18,126 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-12 00:22:18,813 INFO mapreduce.Job: Job job_local1108542370_0001 running in uber mode : false\n",
            "2024-03-12 00:22:18,814 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2024-03-12 00:22:20,368 INFO streaming.PipeMapRed: Records R/W=971/1\n",
            "2024-03-12 00:22:20,385 INFO streaming.PipeMapRed: R/W/S=1000/727/0 in:500=1000/2 [rec/s] out:363=727/2 [rec/s]\n",
            "2024-03-12 00:22:20,756 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-12 00:22:20,757 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-12 00:22:20,762 INFO mapred.LocalJobRunner: \n",
            "2024-03-12 00:22:20,763 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-03-12 00:22:20,763 INFO mapred.MapTask: Spilling map output\n",
            "2024-03-12 00:22:20,763 INFO mapred.MapTask: bufstart = 0; bufend = 261471; bufvoid = 104857600\n",
            "2024-03-12 00:22:20,763 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26100800(104403200); length = 113597/6553600\n",
            "2024-03-12 00:22:20,907 INFO mapred.MapTask: Finished spill 0\n",
            "2024-03-12 00:22:20,926 INFO mapred.Task: Task:attempt_local1108542370_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-03-12 00:22:20,928 INFO mapred.LocalJobRunner: Records R/W=971/1\n",
            "2024-03-12 00:22:20,929 INFO mapred.Task: Task 'attempt_local1108542370_0001_m_000000_0' done.\n",
            "2024-03-12 00:22:20,942 INFO mapred.Task: Final Counters for attempt_local1108542370_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=495294\n",
            "\t\tFILE: Number of bytes written=1101435\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2647\n",
            "\t\tMap output records=28400\n",
            "\t\tMap output bytes=261471\n",
            "\t\tMap output materialized bytes=318277\n",
            "\t\tInput split bytes=82\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=28400\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=204996608\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=353890\n",
            "2024-03-12 00:22:20,942 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108542370_0001_m_000000_0\n",
            "2024-03-12 00:22:20,943 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-03-12 00:22:20,963 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-03-12 00:22:20,966 INFO mapred.LocalJobRunner: Starting task: attempt_local1108542370_0001_r_000000_0\n",
            "2024-03-12 00:22:20,976 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-03-12 00:22:20,976 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-03-12 00:22:20,976 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-03-12 00:22:20,980 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1afe6ca\n",
            "2024-03-12 00:22:20,983 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-03-12 00:22:21,009 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2117966208, maxSingleShuffleLimit=529491552, mergeThreshold=1397857792, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-03-12 00:22:21,015 INFO reduce.EventFetcher: attempt_local1108542370_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-03-12 00:22:21,059 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1108542370_0001_m_000000_0 decomp: 318273 len: 318277 to MEMORY\n",
            "2024-03-12 00:22:21,065 INFO reduce.InMemoryMapOutput: Read 318273 bytes from map-output for attempt_local1108542370_0001_m_000000_0\n",
            "2024-03-12 00:22:21,071 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 318273, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->318273\n",
            "2024-03-12 00:22:21,073 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-03-12 00:22:21,074 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-12 00:22:21,074 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-03-12 00:22:21,083 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-12 00:22:21,083 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318269 bytes\n",
            "2024-03-12 00:22:21,119 INFO reduce.MergeManagerImpl: Merged 1 segments, 318273 bytes to disk to satisfy reduce memory limit\n",
            "2024-03-12 00:22:21,120 INFO reduce.MergeManagerImpl: Merging 1 files, 318277 bytes from disk\n",
            "2024-03-12 00:22:21,121 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-03-12 00:22:21,121 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-03-12 00:22:21,122 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318269 bytes\n",
            "2024-03-12 00:22:21,122 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-12 00:22:21,134 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
            "2024-03-12 00:22:21,138 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2024-03-12 00:22:21,140 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2024-03-12 00:22:21,172 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-12 00:22:21,172 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-12 00:22:21,174 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-12 00:22:21,191 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-12 00:22:21,228 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-03-12 00:22:21,285 INFO streaming.PipeMapRed: Records R/W=14053/1\n",
            "2024-03-12 00:22:21,385 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-03-12 00:22:21,386 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-03-12 00:22:21,388 INFO mapred.Task: Task:attempt_local1108542370_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-03-12 00:22:21,389 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-03-12 00:22:21,390 INFO mapred.Task: Task attempt_local1108542370_0001_r_000000_0 is allowed to commit now\n",
            "2024-03-12 00:22:21,392 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1108542370_0001_r_000000_0' to file:/content/wc_out\n",
            "2024-03-12 00:22:21,394 INFO mapred.LocalJobRunner: Records R/W=14053/1 > reduce\n",
            "2024-03-12 00:22:21,394 INFO mapred.Task: Task 'attempt_local1108542370_0001_r_000000_0' done.\n",
            "2024-03-12 00:22:21,397 INFO mapred.Task: Final Counters for attempt_local1108542370_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1131880\n",
            "\t\tFILE: Number of bytes written=1492146\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=6994\n",
            "\t\tReduce shuffle bytes=318277\n",
            "\t\tReduce input records=28400\n",
            "\t\tReduce output records=6994\n",
            "\t\tSpilled Records=28400\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=72434\n",
            "2024-03-12 00:22:21,397 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108542370_0001_r_000000_0\n",
            "2024-03-12 00:22:21,397 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-03-12 00:22:21,819 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-03-12 00:22:21,820 INFO mapreduce.Job: Job job_local1108542370_0001 completed successfully\n",
            "2024-03-12 00:22:21,832 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1627174\n",
            "\t\tFILE: Number of bytes written=2593581\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2647\n",
            "\t\tMap output records=28400\n",
            "\t\tMap output bytes=261471\n",
            "\t\tMap output materialized bytes=318277\n",
            "\t\tInput split bytes=82\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=6994\n",
            "\t\tReduce shuffle bytes=318277\n",
            "\t\tReduce input records=28400\n",
            "\t\tReduce output records=6994\n",
            "\t\tSpilled Records=56800\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=463470592\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=353890\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=72434\n",
            "2024-03-12 00:22:21,833 INFO streaming.StreamJob: Output directory: /content/wc_out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d2DTo_GsBot",
        "outputId": "f683ae8f-33c9-4e41-976f-190ffc9ba48b"
      },
      "source": [
        "# check output directory\n",
        "!ls wc_out"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91LjuJnlsKGz",
        "outputId": "3f7bd96f-c3ee-4064-bc78-84e740d01e7a"
      },
      "source": [
        "# see actual output\n",
        "#!tail wc_out/part-00000\n",
        "!head wc_out/part-00000"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t8\n",
            "10\t2\n",
            "100\t2\n",
            "1000\t1\n",
            "105\t1\n",
            "108\t2\n",
            "109\t1\n",
            "11\t1\n",
            "110\t2\n",
            "113\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbaMQ1lWpB7O"
      },
      "source": [
        "### Sorting the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld-kLgmOpLw8"
      },
      "source": [
        "#https://www.geeksforgeeks.org/sort-command-linuxunix-examples/\n",
        "!sort -nr -k 2 -t$'\\t' wc_out/part-00000 > sorted.txt"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K09MH8h4qNfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ab3ed2-c7a8-4c59-e6e3-d4b8c97ec23e"
      },
      "source": [
        "!head -30 sorted.txt\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "would\t346\n",
            "could\t247\n",
            "one\t198\n",
            "time\t156\n",
            "like\t145\n",
            "know\t144\n",
            "us\t134\n",
            "mars\t119\n",
            "back\t106\n",
            "even\t105\n",
            "world\t97\n",
            "something\t95\n",
            "see\t95\n",
            "well\t93\n",
            "hermit\t93\n",
            "two\t87\n",
            "people\t86\n",
            "course\t84\n",
            "around\t84\n",
            "way\t82\n",
            "first\t80\n",
            "really\t79\n",
            "new\t76\n",
            "little\t74\n",
            "long\t73\n",
            "still\t71\n",
            "information\t70\n",
            "ai\t67\n",
            "good\t63\n",
            "earth\t60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZN0vwE2pxHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e86e5a86-7ac2-409e-9217-5442e0c8e84a"
      },
      "source": [
        "!tail -30 sorted.txt"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2150\t1\n",
            "214\t1\n",
            "206\t1\n",
            "205\t1\n",
            "2019\t1\n",
            "2018\t1\n",
            "2007\t1\n",
            "20062007\t1\n",
            "2000\t1\n",
            "1999\t1\n",
            "1970s\t1\n",
            "1956\t1\n",
            "187\t1\n",
            "186\t1\n",
            "17866\t1\n",
            "156\t1\n",
            "155\t1\n",
            "15\t1\n",
            "150\t1\n",
            "1493\t1\n",
            "133\t1\n",
            "132\t1\n",
            "12th\t1\n",
            "12700\t1\n",
            "115\t1\n",
            "113\t1\n",
            "11\t1\n",
            "109\t1\n",
            "105\t1\n",
            "1000\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "print('signed off at  ',datetime.now(pytz.timezone('Asia/Kolkata')))"
      ],
      "metadata": {
        "id": "kO8mrX8_0BS-",
        "outputId": "851d1593-08e9-44d4-e4b8-b93c992912ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "signed off at   2024-03-12 05:54:02.246298+05:30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEShQrGRJio3"
      },
      "source": [
        "#Chronobooks <br>\n",
        "![alt text](https://1.bp.blogspot.com/-lTiYBkU2qbU/X1er__fvnkI/AAAAAAAAjtE/GhDR3OEGJr4NG43fZPodrQD5kbxtnKebgCLcBGAsYHQ/s600/Footer2020-600x200.png)<hr>\n",
        "Chronotantra and Chronoyantra are two science fiction novels that explore the collapse of human civilisation on Earth and then its rebirth and reincarnation both on Earth as well as on the distant worlds of Mars, Titan and Enceladus. But is it the human civilisation that is being reborn? Or is it some other sentience that is revealing itself.\n",
        "If you have an interest in AI and found this material useful, you may consider buying these novels, in paperback or kindle, from [http://bit.ly/chronobooks](http://bit.ly/chronobooks)"
      ]
    }
  ]
}