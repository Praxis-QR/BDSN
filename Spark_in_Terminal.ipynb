{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2RILIGVT/0W9fdH6UKWtY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praxis-QR/BDSN/blob/main/Spark_in_Terminal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQy1oyK1Pzbz",
        "outputId": "9f2c3d25-125f-4bb4-f5c5-4343a34ba91d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 44 kB/s \n",
            "\u001b[K     |████████████████████████████████| 199 kB 66.6 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -q pyspark\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test Data \n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/Chronotantra.txt"
      ],
      "metadata": {
        "id": "3QNgZHGYR-pB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sparksample.py\n",
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\t# create Spark context with necessary configuration\n",
        "\tsc = SparkContext(\"local\",\"PySpark Word Count Exmaple\")\n",
        "\t\n",
        "\t# read data from text file and split each line into words\n",
        "\twords = sc.textFile(\"Chronotantra.txt\").flatMap(lambda line: line.split(\" \"))\n",
        "\t\n",
        "\t# count the occurrence of each word\n",
        "\twordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a +b)\n",
        "\t\n",
        "\t# save the counts to output\n",
        "\twordCounts.saveAsTextFile(\"CT1-out\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV1UXWuZRjlv",
        "outputId": "4a6cd402-4f82-4256-bbbc-8061bf9619ad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sparksample.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls CT-out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoHtOwF1TTa9",
        "outputId": "9fe42abd-bfcd-4c81-ec5c-32dd9def861b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head CT-out/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9plNMwEBTc9V",
        "outputId": "1c4b28f8-ce92-4880-f238-1b3aa5783f81"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('', 7389)\n",
            "('Chronotantra', 2)\n",
            "('Prithwis', 5)\n",
            "('Mukerjee', 4)\n",
            "('CHRONOTANTRA', 1)\n",
            "('A', 46)\n",
            "('science', 9)\n",
            "('fiction', 4)\n",
            "('novel', 5)\n",
            "('Published', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit sparksample.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNg8uuIsW54H",
        "outputId": "6dc47e66-ea6c-4b04-e448-d819eed2a756"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/12/01 06:24:56 INFO SparkContext: Running Spark version 3.3.1\n",
            "22/12/01 06:24:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "22/12/01 06:24:57 INFO ResourceUtils: ==============================================================\n",
            "22/12/01 06:24:57 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "22/12/01 06:24:57 INFO ResourceUtils: ==============================================================\n",
            "22/12/01 06:24:57 INFO SparkContext: Submitted application: PySpark Word Count Exmaple\n",
            "22/12/01 06:24:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "22/12/01 06:24:57 INFO ResourceProfile: Limiting resource is cpu\n",
            "22/12/01 06:24:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "22/12/01 06:24:57 INFO SecurityManager: Changing view acls to: root\n",
            "22/12/01 06:24:57 INFO SecurityManager: Changing modify acls to: root\n",
            "22/12/01 06:24:57 INFO SecurityManager: Changing view acls groups to: \n",
            "22/12/01 06:24:57 INFO SecurityManager: Changing modify acls groups to: \n",
            "22/12/01 06:24:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "22/12/01 06:24:57 INFO Utils: Successfully started service 'sparkDriver' on port 44225.\n",
            "22/12/01 06:24:57 INFO SparkEnv: Registering MapOutputTracker\n",
            "22/12/01 06:24:58 INFO SparkEnv: Registering BlockManagerMaster\n",
            "22/12/01 06:24:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "22/12/01 06:24:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "22/12/01 06:24:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "22/12/01 06:24:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-51ad4f69-3cb6-45ed-b008-18d62dc6899b\n",
            "22/12/01 06:24:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "22/12/01 06:24:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "22/12/01 06:24:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "22/12/01 06:24:59 INFO Executor: Starting executor ID driver on host 5cd1a221a622\n",
            "22/12/01 06:24:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "22/12/01 06:24:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38527.\n",
            "22/12/01 06:24:59 INFO NettyBlockTransferService: Server created on 5cd1a221a622:38527\n",
            "22/12/01 06:24:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "22/12/01 06:24:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5cd1a221a622, 38527, None)\n",
            "22/12/01 06:24:59 INFO BlockManagerMasterEndpoint: Registering block manager 5cd1a221a622:38527 with 434.4 MiB RAM, BlockManagerId(driver, 5cd1a221a622, 38527, None)\n",
            "22/12/01 06:24:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5cd1a221a622, 38527, None)\n",
            "22/12/01 06:24:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5cd1a221a622, 38527, None)\n",
            "22/12/01 06:25:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.4 KiB, free 434.2 MiB)\n",
            "22/12/01 06:25:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.5 KiB, free 434.2 MiB)\n",
            "22/12/01 06:25:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5cd1a221a622:38527 (size: 32.5 KiB, free: 434.4 MiB)\n",
            "22/12/01 06:25:00 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "22/12/01 06:25:01 INFO FileInputFormat: Total input files to process : 1\n",
            "22/12/01 06:25:01 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "22/12/01 06:25:01 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "22/12/01 06:25:01 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "22/12/01 06:25:01 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "22/12/01 06:25:01 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "22/12/01 06:25:01 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/sparksample.py:12) as input to shuffle 0\n",
            "22/12/01 06:25:01 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "22/12/01 06:25:01 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:83)\n",
            "22/12/01 06:25:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "22/12/01 06:25:01 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "22/12/01 06:25:01 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/sparksample.py:12), which has no missing parents\n",
            "22/12/01 06:25:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)\n",
            "22/12/01 06:25:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 434.1 MiB)\n",
            "22/12/01 06:25:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5cd1a221a622:38527 (size: 7.5 KiB, free: 434.4 MiB)\n",
            "22/12/01 06:25:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
            "22/12/01 06:25:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/sparksample.py:12) (first 15 tasks are for partitions Vector(0))\n",
            "22/12/01 06:25:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "22/12/01 06:25:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (5cd1a221a622, executor driver, partition 0, PROCESS_LOCAL, 4488 bytes) taskResourceAssignments Map()\n",
            "22/12/01 06:25:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "22/12/01 06:25:01 INFO HadoopRDD: Input split: file:/content/Chronotantra.txt:0+353890\n",
            "22/12/01 06:25:04 INFO PythonRunner: Times: total = 1482, boot = 989, init = 100, finish = 393\n",
            "22/12/01 06:25:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1662 bytes result sent to driver\n",
            "22/12/01 06:25:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2799 ms on 5cd1a221a622 (executor driver) (1/1)\n",
            "22/12/01 06:25:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "22/12/01 06:25:04 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 46257\n",
            "22/12/01 06:25:04 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/sparksample.py:12) finished in 3.170 s\n",
            "22/12/01 06:25:04 INFO DAGScheduler: looking for newly runnable stages\n",
            "22/12/01 06:25:04 INFO DAGScheduler: running: Set()\n",
            "22/12/01 06:25:04 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "22/12/01 06:25:04 INFO DAGScheduler: failed: Set()\n",
            "22/12/01 06:25:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "22/12/01 06:25:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 108.2 KiB, free 434.0 MiB)\n",
            "22/12/01 06:25:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 40.8 KiB, free 434.0 MiB)\n",
            "22/12/01 06:25:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5cd1a221a622:38527 (size: 40.8 KiB, free: 434.3 MiB)\n",
            "22/12/01 06:25:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
            "22/12/01 06:25:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "22/12/01 06:25:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "22/12/01 06:25:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (5cd1a221a622, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "22/12/01 06:25:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "22/12/01 06:25:05 INFO ShuffleBlockFetcherIterator: Getting 1 (99.6 KiB) non-empty blocks including 1 (99.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "22/12/01 06:25:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 43 ms\n",
            "22/12/01 06:25:05 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "22/12/01 06:25:05 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "22/12/01 06:25:05 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "22/12/01 06:25:05 INFO PythonRunner: Times: total = 192, boot = -1488, init = 1550, finish = 130\n",
            "22/12/01 06:25:05 INFO FileOutputCommitter: Saved output of task 'attempt_202212010625012622062904257423704_0008_m_000000_0' to file:/content/CT1-out/_temporary/0/task_202212010625012622062904257423704_0008_m_000000\n",
            "22/12/01 06:25:05 INFO SparkHadoopMapRedUtil: attempt_202212010625012622062904257423704_0008_m_000000_0: Committed. Elapsed time: 5 ms.\n",
            "22/12/01 06:25:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1960 bytes result sent to driver\n",
            "22/12/01 06:25:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 690 ms on 5cd1a221a622 (executor driver) (1/1)\n",
            "22/12/01 06:25:05 INFO DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:83) finished in 0.773 s\n",
            "22/12/01 06:25:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "22/12/01 06:25:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "22/12/01 06:25:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "22/12/01 06:25:05 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 4.123878 s\n",
            "22/12/01 06:25:05 INFO SparkHadoopWriter: Start to commit write Job job_202212010625012622062904257423704_0008.\n",
            "22/12/01 06:25:05 INFO SparkHadoopWriter: Write Job job_202212010625012622062904257423704_0008 committed. Elapsed time: 45 ms.\n",
            "22/12/01 06:25:05 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "22/12/01 06:25:05 INFO SparkUI: Stopped Spark web UI at http://5cd1a221a622:4040\n",
            "22/12/01 06:25:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "22/12/01 06:25:05 INFO MemoryStore: MemoryStore cleared\n",
            "22/12/01 06:25:05 INFO BlockManager: BlockManager stopped\n",
            "22/12/01 06:25:05 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "22/12/01 06:25:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "22/12/01 06:25:05 INFO SparkContext: Successfully stopped SparkContext\n",
            "22/12/01 06:25:05 INFO ShutdownHookManager: Shutdown hook called\n",
            "22/12/01 06:25:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-a07ae867-cdb2-4f61-9604-b3be8884ea36/pyspark-b04bc832-1ff0-4f39-90d8-1a62cf557f86\n",
            "22/12/01 06:25:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-a07ae867-cdb2-4f61-9604-b3be8884ea36\n",
            "22/12/01 06:25:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-fabdfeb7-1ca8-49bf-9034-bd617b3e84ea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq_yd1OqV9bd",
        "outputId": "3c8f7297-a8a7-4a64-a406-12cbd58ef5d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.15 (default, Oct 12 2022, 19:14:39) \n",
            "[GCC 7.5.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "22/12/01 06:20:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.1\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.8.15 (default, Oct 12 2022 19:14:39)\n",
            "Spark context Web UI available at http://5cd1a221a622:4040\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1669875636893).\n",
            "SparkSession available as 'spark'.\n",
            ">>> quit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Terminal"
      ],
      "metadata": {
        "id": "FH80fASvQHV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import JSON\n",
        "from google.colab import output\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "\n",
        "def shell(command):\n",
        "  if command.startswith('cd'):\n",
        "    path = command.strip().split(maxsplit=1)[1]\n",
        "    os.chdir(path)\n",
        "    return JSON([''])\n",
        "  return JSON([getoutput(command)])\n",
        "output.register_callback('shell', shell)"
      ],
      "metadata": {
        "id": "i0TSdNdYQSGM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Shell (need to be run)\n",
        "%%html\n",
        "<div id=term_demo></div>\n",
        "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
        "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
        "<script>\n",
        "  $('#term_demo').terminal(async function(command) {\n",
        "      if (command !== '') {\n",
        "          try {\n",
        "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
        "              let out = res.data['application/json'][0]\n",
        "              this.echo(new String(out))\n",
        "          } catch(e) {\n",
        "              this.error(new String(e));\n",
        "          }\n",
        "      } else {\n",
        "          this.echo('');\n",
        "      }\n",
        "  }, {\n",
        "      greetings: 'Welcome to Colab Shell',\n",
        "      name: 'colab_demo',\n",
        "      height: 800,\n",
        "      prompt: 'colab > '\n",
        "  });"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "qX30Ya2VQXgn",
        "outputId": "3afbf3c2-3837-4125-c81c-18c05f8ee591"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div id=term_demo></div>\n",
              "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
              "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
              "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
              "<script>\n",
              "  $('#term_demo').terminal(async function(command) {\n",
              "      if (command !== '') {\n",
              "          try {\n",
              "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
              "              let out = res.data['application/json'][0]\n",
              "              this.echo(new String(out))\n",
              "          } catch(e) {\n",
              "              this.error(new String(e));\n",
              "          }\n",
              "      } else {\n",
              "          this.echo('');\n",
              "      }\n",
              "  }, {\n",
              "      greetings: 'Welcome to Colab Shell',\n",
              "      name: 'colab_demo',\n",
              "      height: 800,\n",
              "      prompt: 'colab > '\n",
              "  });\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}